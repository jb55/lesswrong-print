<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_jp" class="content clear"><div class="md">
        
  <div><p><strong>Followup to</strong>:&#xA0; <a href="/lw/jk/burdensome_details/">Burdensome Details</a>, <a href="/lw/jn/how_much_evidence_does_it_take/">How Much Evidence?</a></p>
<p>The more complex an explanation is, the more evidence you need just
to find it in belief-space.&#xA0; (In Traditional Rationality this is often
phrased <a href="/lw/jo/einsteins_arrogance/">misleadingly</a>,
as "The more complex a proposition is, the more evidence is required to
argue for it.")&#xA0; How can we measure the complexity of an explanation? 
How can we determine how much evidence is required?</p>
<p>Occam's Razor is often phrased as "The simplest explanation that
fits the facts."&#xA0; Robert Heinlein replied that the simplest explanation
is "The lady down the street is a witch; she did it."</p>
<p>One observes that the length of an English sentence is not a good
way to measure "complexity".&#xA0; And "fitting" the facts by merely <em>failing to prohibit</em> them is insufficient.</p><a id="more"></a><p>Why, exactly, is the length of an English sentence a poor measure of
complexity?&#xA0; Because when you speak a sentence aloud, you are using <em>labels</em>
for concepts that the listener shares - the receiver has already stored
the complexity in them.&#xA0; Suppose we abbreviated Heinlein's whole
sentence as "Tldtsiawsdi!" so that the entire explanation can be
conveyed in one word; better yet, we'll give it a short arbitrary label
like "Fnord!"&#xA0; Does this reduce the complexity?&#xA0; No, because you have
to tell the listener in advance that "Tldtsiawsdi!" stands for "The
lady down the street is a witch; she did it."&#xA0; "Witch", itself, is a
label for some extraordinary assertions - just because we all know what
it means doesn't mean the concept is simple.</p>
<p>An enormous bolt of electricity comes out of the sky and hits
something, and the Norse tribesfolk say, "Maybe a really powerful agent
was angry and threw a lightning bolt."<em>&#xA0;</em>The human brain is the most complex artifact in the known universe.&#xA0; If <em>anger</em>
seems simple, it's because we don't see all the neural circuitry that's
implementing the emotion.&#xA0; (Imagine trying to explain why <em>Saturday Night Live</em>
is funny, to an alien species with no sense of humor.&#xA0; But don't feel
superior; you yourself have no sense of fnord.)&#xA0; The complexity of
anger, and indeed the complexity of intelligence, was glossed over by
the humans who hypothesized Thor the thunder-agent.</p>
<p><em>To a human,</em> Maxwell's Equations take much longer to explain
than Thor.&#xA0; Humans don't have a built-in vocabulary for calculus the
way we have a built-in vocabulary for anger.&#xA0; You've got to explain
your language, and the language behind the language, and the very
concept of mathematics, before you can start on electricity.</p>
<p>And yet it seems that there should be some sense in which Maxwell's Equations are <em>simpler</em> than a human brain, or Thor the thunder-agent.</p>
<p>There is:&#xA0; It's <em>enormously</em> easier (as it turns out) to write
a computer program that simulates Maxwell's Equations, compared to a
computer program that simulates an intelligent emotional mind like Thor.</p> <p>The
formalism of Solomonoff Induction measures the "complexity of a
description" by the length of the shortest computer program which
produces that description as an output.&#xA0; To talk about the "shortest
computer program" that does something, you need to specify a space of
computer programs, which requires a language and interpreter. 
Solomonoff Induction uses Turing machines, or rather, bitstrings that
specify Turing machines.&#xA0; &#xA0;What if you don't like Turing machines? 
Then there's only a constant complexity penalty to design your own
Universal Turing Machine that interprets whatever code you give it in
whatever programming language you like.&#xA0; Different inductive formalisms
are penalized by a worst-case constant factor relative to each other,
corresponding to the size of a universal interpreter for that formalism.<br> </p>
<p>In the better (IMHO) versions of Solomonoff Induction, the computer
program does not produce a deterministic prediction, but assigns
probabilities to strings.&#xA0; For example, we could write a program to
explain a fair coin by writing a program that assigns equal
probabilities to all 2^N strings of length N.&#xA0; This is Solomonoff
Induction's approach to <em>fitting</em> the observed data.&#xA0; The higher the probability a program assigns to the observed data, the better that program <em>fits</em>
the data.&#xA0; And probabilities must sum to 1, so for a program to better
"fit" one possibility, it must steal probability mass from some other
possibility which will then "fit" much more poorly.&#xA0; There is no
superfair coin that assigns 100% probability to heads and 100%
probability to tails.</p>
<p>How do we trade off the fit to the data, against the complexity of the program?&#xA0; If you ignore complexity penalties, and think <em>only</em>
about fit, then you will always prefer programs that claim to
deterministically predict the data, assign it 100% probability.&#xA0; If the
coin shows "HTTHHT", then the program which claims that the coin was
fixed to show "HTTHHT" fits the observed data 64 times better than the
program which claims the coin is fair.&#xA0; Conversely, if you ignore fit,
and consider <em>only</em> complexity, then the "fair coin" hypothesis
will always seem simpler than any other hypothesis.&#xA0; Even if the coin
turns up "HTHHTHHHTHHHHTHHHHHT..."&#xA0; Indeed, the fair coin <em>is</em>
simpler and it fits this data exactly as well as it fits any other
string of 20 coinflips - no more, no less - but we see another
hypothesis, seeming not too complicated, that fits the data much better.</p>
<p>If you let a program store one more binary bit of information, it
will be able to cut down a space of possibilities by half, and hence
assign twice as much probability to all the points in the remaining
space.&#xA0; This suggests that one bit of program complexity should cost <em>at least</em>
a "factor of two gain" in the fit.&#xA0; If you try to design a computer
program that explicitly stores an outcome like "HTTHHT", the six bits
that you lose in complexity must destroy all plausibility gained by a
64-fold improvement in fit.&#xA0; Otherwise, you will sooner or later decide
that all fair coins are fixed.</p>
<p>Unless your program is being smart, and <em>compressing</em> the data, it should do no good just to move one bit from the data into the program description.</p>
<p>The way Solomonoff induction works to predict sequences is that you
sum up over all allowed computer programs - if any program is allowed,
Solomonoff induction becomes uncomputable - with each program having a
prior probability of (1/2) to the power of its code length in bits, and
each program is further weighted by its fit to all data observed so
far.&#xA0; This gives you a weighted mixture of experts that can predict
future bits.</p>
<p>The Minimum Message Length formalism is nearly equivalent to
Solomonoff induction.&#xA0; You send a string describing a code, and then
you send a string describing the data in that code.&#xA0; Whichever
explanation leads to the shortest <em>total</em> message is the best. 
If you think of the set of allowable codes as a space of computer
programs, and the code description language as a universal machine,
then Minimum Message Length is nearly equivalent to Solomonoff
induction.&#xA0; (Nearly, because it chooses the <em>shortest</em> program, rather than summing up over all programs.)</p>
<p>This lets us see clearly the problem with using "The lady down the
street is a witch; she did it" to explain the pattern in the sequence
"0101010101".&#xA0; If you're sending a message to a friend, trying to
describe the sequence you observed, you would have to say:&#xA0; "The lady
down the street is a witch; she made the sequence come out
0101010101."&#xA0; Your accusation of witchcraft wouldn't let you <em>shorten</em> the rest of the message; you would still have to describe, in full detail, the data which her witchery caused.</p>
<p>Witchcraft may fit our observations in the sense of qualitatively <em>permitting</em> them; but this is because witchcraft permits <em>everything</em>, like saying "<a href="/lw/is/fake_causality/">Phlogiston!</a>"&#xA0; So, even after you say "witch", you still have to describe all the observed data in full detail.&#xA0; You have not <em>compressed the total length of the message describing your observations</em> by transmitting the message about witchcraft; you have simply added a useless prologue, increasing the total length.</p>
<p>The real sneakiness was concealed in the word "it" of "A witch did it".&#xA0; A witch did <em>what?</em></p>
<p>Of course, thanks to <a href="/lw/il/hindsight_bias/">hindsight bias</a> and <a href="/lw/j7/anchoring_and_adjustment/">anchoring</a> and <a href="/lw/ip/fake_explanations/">fake explanations</a> and <a href="/lw/is/fake_causality/">fake causality</a> and <a href="/lw/iw/positive_bias_look_into_the_dark/">positive bias</a> and <a href="/lw/he/knowing_about_biases_can_hurt_people/">motivated cognition</a>, it may seem all too obvious that if a woman is a witch, of <em>course</em> she would make the coin come up 0101010101.&#xA0; But of this I have already spoken.</p></div></div></div>
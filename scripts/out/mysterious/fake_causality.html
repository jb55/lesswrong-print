<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_is" class="content clear"><div class="md">
        
  <div><p><strong>Followup to</strong>:&#xA0; <a href="/lw/ip/fake_explanations/">Fake Explanations</a>, <a href="/lw/iq/guessing_the_teachers_password/">Guessing the Teacher's Password</a></p>
<p>Phlogiston was the 18 century's answer to the Elemental Fire of the Greek alchemists.&#xA0; Ignite wood, and let it burn.&#xA0; What is the orangey-bright "fire" stuff?&#xA0; Why does the wood transform into ash?&#xA0; To both questions, the 18th-century chemists answered, "phlogiston".</p>
<p>...and that was it, you see, that was their answer:&#xA0; "Phlogiston."</p>
<p>Phlogiston escaped from burning substances as visible fire.&#xA0; As the phlogiston escaped, the burning substances lost phlogiston and so became ash, the "true material".&#xA0; Flames in enclosed containers went out because the air became saturated with phlogiston, and so could not hold any more.&#xA0; Charcoal left little residue upon burning because it was nearly pure phlogiston.</p>
<p>Of course, one didn't use phlogiston theory to <em>predict</em> the outcome of a chemical transformation.&#xA0; You looked at the result first, then you used phlogiston theory to <em>explain</em> it.&#xA0; It's not that phlogiston theorists predicted a flame would extinguish in a closed container; rather they lit a flame in a container, watched it go out, and then said, "The air must have become saturated with phlogiston."&#xA0; You couldn't even use phlogiston theory to <a href="/lw/if/your_strength_as_a_rationalist/">say what you ought <em>not</em> to see</a>; it could explain everything.</p>
<p>This was an earlier age of science.&#xA0; For a long time, no one realized there was a problem.&#xA0; <a href="/lw/ip/fake_explanations/">Fake explanations</a> don't <em>feel</em> fake.&#xA0; That's what makes them dangerous.</p>
<p><a id="more"></a></p>
<p>Modern research suggests that humans think about cause and effect using something like the directed acyclic graphs (DAGs) of Bayes nets.&#xA0; Because it rained, the sidewalk is wet; because the sidewalk is wet, it is slippery:</p>
<p>[Rain] -&gt; [Sidewalk wet] -&gt; [Sidewalk slippery]</p>
<p>From this we can infer - or, in a Bayes net, rigorously calculate in probabilities - that when the sidewalk is slippery, it probably rained; but if we already know that the sidewalk is wet, learning that the sidewalk is slippery tells us nothing more about whether it rained.</p>
<p>Why is fire hot and bright when it burns?</p>
<p>["Phlogiston"] -&gt; [Fire hot and bright]</p>
<p>It <em>feels</em> like an explanation.&#xA0; It's <em>represented</em> using the same cognitive data format.&#xA0; But the human mind does not automatically detect when a cause has an unconstraining arrow to its effect. Worse, thanks to <a href="/lw/il/hindsight_bias/">hindsight bias</a>, it may feel like the cause <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/">constrains</a> the effect, when it was merely<em>&#xA0;</em><a href="/lw/ii/conservation_of_expected_evidence/">fitted</a> to the effect.</p>
<p>Interestingly, <a href="http://books.google.com/books?id=k9VsqN24pNYC&amp;dq=&amp;pg=PP1&amp;ots=WR9UGWdOdd&amp;sig=w_Mrax-y4VVwZy5SQGySphNsKMc&amp;prev=http://www.google.com/search%3Fhl%3Den%26safe%3Doff%26q%3Dpearl%2Bintelligent%2Bsystems%26btnG%3DSearch&amp;sa=X&amp;oi=print&amp;ct=title#PPA143,M1">our modern understanding of probabilistic reasoning about causality</a> can describe precisely what the phlogiston theorists were doing wrong.&#xA0; One of the primary inspirations for Bayesian networks was noticing the problem of double-counting evidence if inference resonates between an effect and a cause.&#xA0; For example, let's say that I get a bit of unreliable information that the sidewalk is wet.&#xA0; This should make me think it's more likely to be raining.&#xA0; But, if it's more likely to be raining, doesn't that make it more likely that the sidewalk is wet?&#xA0; And wouldn't <em>that</em> make it more likely that the sidewalk is slippery?&#xA0; But if the sidewalk is slippery, it's probably wet; and then I should again raise my probability that it's raining...</p>
<p>Judea Pearl uses the metaphor of an algorithm for counting soldiers in a line.&#xA0; Suppose you're in the line, and you see two soldiers next to you, one in front and one in back.&#xA0; That's three soldiers.&#xA0; So you ask the soldier next to you, "How many soldiers do <em>you</em> see?"&#xA0; He looks around and says, "Three".&#xA0; So that's a total of six soldiers.&#xA0; This, obviously, is <em>not</em> how to do it.</p>
<p>A smarter way is to ask the soldier in front of you, "How many soldiers forward of you?" and the soldier in back, "How many soldiers backward of you?"&#xA0; The question "How many soldiers forward?" can be passed on as a message without confusion.&#xA0; If I'm at the front of the line, I pass the message "1 soldier forward", for myself.&#xA0; The person directly in back of me gets the message "1 soldier forward", and passes on the message "2 soldiers forward" to the soldier behind him.&#xA0; At the same time, each soldier is also getting the message "N soldiers backward" from the soldier behind them, and passing it on as "N+1 soldiers backward" to the soldier in front of them.&#xA0; How many soldiers in total?&#xA0; Add the two numbers you receive, plus one for yourself: that is the total number of soldiers in line.</p>
<p>The key idea is that every soldier must <em>separately</em> track the two messages, the forward-message and backward-message, and add them together only at the end.&#xA0; You never add any soldiers from the backward-message you receive to the forward-message you pass back.&#xA0; Indeed, the total number of soldiers is never passed as a message - no one ever says it aloud.</p>
<p>An analogous principle operates in rigorous probabilistic reasoning about causality.&#xA0; If you learn something about whether it's raining, from some source <em>other</em> than observing the sidewalk to be wet, this will send a forward-message from [rain] to [sidewalk wet] and raise our expectation of the sidewalk being wet.&#xA0; If you observe the sidewalk to be wet, this sends a backward-message to our belief that it is raining, and this message propagates from [rain] to all neighboring nodes <em>except</em> the [sidewalk wet] node.&#xA0; We count each piece of evidence exactly once; no update message ever "bounces" back and forth.&#xA0; The exact algorithm may be found in Judea Pearl's classic "<a href="http://books.google.com/books?id=k9VsqN24pNYC&amp;dq=&amp;pg=PP1&amp;ots=WR9UGWdOdd&amp;sig=w_Mrax-y4VVwZy5SQGySphNsKMc&amp;prev=http://www.google.com/search%3Fhl%3Den%26safe%3Doff%26q%3Dpearl%2Bintelligent%2Bsystems%26btnG%3DSearch&amp;sa=X&amp;oi=print&amp;ct=title#PPA143,M1">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</a>".</p>
<p>So what went wrong in phlogiston theory?&#xA0; When we observe that fire is hot, the [fire] node can send a backward-evidence to the ["phlogiston"] node, leading us to update our beliefs about phlogiston.&#xA0; But if so, we can't count this as a successful forward-prediction of phlogiston theory.&#xA0; The message should go in only one direction, and not bounce back.</p>
<p>Alas, human beings do not use a rigorous algorithm for updating belief networks.&#xA0; We learn about parent nodes from observing children, and predict child nodes from beliefs about parents.&#xA0; But we don't keep rigorously separate books for the backward-message and forward-message.&#xA0; We just remember that phlogiston is hot, which <em>causes</em> fire to be hot.&#xA0; So it seems like phlogiston theory predicts the hotness of fire.&#xA0; Or, worse, it just feels like <em>phlogiston makes the fire hot.</em></p>
<p>Until you notice that no <em>advance</em> predictions are being made, the non-constraining causal node is not labeled "fake".&#xA0; It's represented the same way as any other node in your belief network.&#xA0; It feels like a fact, like all the other facts you know:&#xA0; <em>Phlogiston makes the fire hot.</em></p>
<p>A properly designed AI would notice the problem instantly.&#xA0; This wouldn't even require special-purpose code, just correct bookkeeping of the belief network.&#xA0; (Sadly, we humans can't rewrite our own code, the way a properly designed AI could.)</p>
<p>Speaking of "<a href="/lw/im/hindsight_devalues_science/">hindsight bias</a>" is just the nontechnical way of saying that humans do not rigorously separate forward and backward messages, allowing forward messages to be contaminated by backward ones.</p>
<p>Those who long ago went down the path of phlogiston were not trying to be fools.&#xA0; No scientist deliberately wants to get stuck in a blind alley.&#xA0; Are there any fake explanations in <em>your</em> mind?&#xA0; &#xA0;If there are, I guarantee they're not labeled "fake explanation", so polling your thoughts for the "fake" keyword will not turn them up.</p>
<p>Thanks to <a href="/lw/im/hindsight_devalues_science/">hindsight bias</a>, it's also not enough to check how well your theory "predicts" facts you already know.&#xA0; You've got to predict for tomorrow, not yesterday.&#xA0; It's the only way a messy human mind can be guaranteed of sending a pure forward message.</p></div></div></div>
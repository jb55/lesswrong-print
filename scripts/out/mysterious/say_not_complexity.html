<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_ix" class="content clear"><div class="md">
        
  <div><p>Once upon a time...</p>
<p>This is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice.&#xA0; I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn't know yet if he could learn to think about AI.</p>
<p>I had asked Marcello to say how he thought an AI might discover how to solve a Rubik's Cube.&#xA0; Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them.&#xA0; How would an AI <em>invent for itself</em> the concept of an "operator", or "macro", which is the key to solving the Rubik's Cube?</p>
<p>At some point in this discussion, Marcello said:&#xA0; "Well, I think the AI needs complexity to do X, and complexity to do Y -"</p>
<p>And I said, "Don't say '<em>complexity'.</em>"</p>
<p>Marcello said, "Why not?"</p>
<p><a id="more"></a></p>
<p>I said, "Complexity should never be a goal in itself.&#xA0; You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder."&#xA0; (I was thinking of all the people whom I had heard advocating that the Internet would "wake up" and become an AI when it became "sufficiently complex".)</p>
<p>And Marcello said, "But there's got to be <em>some</em> amount of complexity that does it."</p>
<p>I closed my eyes briefly, and tried to think of how to explain it all in words.&#xA0; To me, saying 'complexity' simply <em>felt</em> like the wrong move in the AI dance.&#xA0; No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion.&#xA0; We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience...</p>
<p>I said, "Did you read <a href="http://yudkowsky.net/bayes/technical.html">A Technical Explanation of Technical Explanation</a>?"</p>
<p>"Yes," said Marcello.</p>
<p>"Okay," I said, "saying 'complexity' doesn't concentrate your probability mass."</p>
<p>"Oh," Marcello said, "like '<a href="/lw/iv/the_futility_of_emergence/">emergence</a>'.&#xA0; Huh.&#xA0; So... now I've got to think about how X might actually happen..."</p>
<p>That was when I thought to myself, "<em>Maybe <strong>this</strong> one is teachable.</em>"</p>
<p>Complexity is not a useless concept.&#xA0; It has mathematical definitions attached to it, such as Kolmogorov complexity, and Vapnik-Chervonenkis complexity.&#xA0; Even on an intuitive level, complexity is often worth thinking about - you have to judge the complexity of a hypothesis and decide if it's "too complicated" given the supporting evidence, or look at a design and try to make it simpler.</p>
<p>But concepts are not useful or useless of themselves.&#xA0; Only <em>usages</em> are correct or incorrect.&#xA0; In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing.&#xA0; It is an extremely common misstep, at least in my field.&#xA0; You can join a discussion on Artificial General Intelligence and watch people doing the same thing, left and right, over and over again - constantly skipping over things they don't understand, without realizing that's what they're doing.</p>
<p>In an eyeblink it happens: putting a <a href="/lw/is/fake_causality/">non-controlling causal node</a> behind something mysterious, a causal node that <a href="/lw/ip/fake_explanations/">feels like an explanation</a> but isn't.&#xA0; The mistake takes place below the level of words.&#xA0; It requires no special character flaw; it is how human beings think <a href="/lw/iq/guessing_the_teachers_password/">by default</a>, since the ancient times.</p>
<p>What you must avoid is <em>skipping over the mysterious part;</em> you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts - "complexity", for example.&#xA0; But the essential mistake is that <em>skip-over,</em> regardless of what causal node goes behind it.&#xA0; The skip-over is not a thought, but a microthought.&#xA0; You have to pay close attention to catch yourself at it.&#xA0; And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning.&#xA0; You have to <em>feel</em> which parts of your map are still blank, and more importantly, pay attention to that feeling.</p>
<p>I suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness.&#xA0; You'll get more kudos for a seemingly complete model that includes some "<a href="/lw/iv/the_futility_of_emergence/">emergent phenomena</a>", versus an explicitly incomplete map where the label says "I got no clue how this part works" or "then a miracle occurs".&#xA0; A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?&#xA0; And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important.&#xA0; That's the price you sometimes pay, for entering into terra incognita and trying to solve problems <em>incrementally.</em>&#xA0; But that makes it even <em>more</em> important to <em>know</em> when you aren't finished yet.&#xA0; Mostly, people don't dare to enter terra incognita at all, for the deadly fear of wasting their time.<em>&#xA0;</em></p>
<p>And if you're working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to <a href="/lw/id/you_can_face_reality/">admit to yourself</a> that you don't know how to build an AI yet, and your current life-plans will come crashing down in ruins around your ears.&#xA0; But perhaps I am <a href="/lw/hz/correspondence_bias/">over-explaining</a>, since skip-over happens <a href="/lw/iq/guessing_the_teachers_password/">by default</a> in humans; if you're looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.</p>
<p>Marcello and I developed a convention in our AI work: when we ran into something we didn't understand, which was often, we would say "magic" - as in, "X magically does Y" - to remind ourselves that <em>here was an unsolved problem, a gap in our understanding.</em>&#xA0; It is far better to say "magic", than "complexity" or "emergence"; the latter <a href="/lw/iq/guessing_the_teachers_password/">words</a> create an illusion of understanding.&#xA0; Wiser to say "magic", and leave yourself a placeholder, a reminder of work you will have to do later.</p></div></div></div>

# Hindsight Bias

*Hindsight bias* is when people who know the answer vastly
overestimate its *predictability* or *obviousness,* compared to the
estimates of subjects who must guess without advance knowledge. 
Hindsight bias is sometimes called the
*I-knew-it-all-along effect*.

Fischhoff and Beyth (1975) presented students with historical
accounts of unfamiliar incidents, such as a conflict between the
Gurkhas and the British in 1814.  Given the account as background
knowledge, five groups of students were asked what they would have
predicted as the probability for each of four outcomes: British
victory, Gurkha victory, stalemate with a peace settlement, or
stalemate with no peace settlement.  Four experimental groups were
respectively told that these four outcomes were the historical
outcome.  The fifth, control group was not told any historical
outcome.  In every case, a group told an outcome assigned
substantially higher probability to that outcome, than did any
other group or the control group.

Hindsight bias matters in legal cases, where a judge or jury must
determine whether a defendant was legally negligent in failing to
foresee a hazard (Sanchiro 2003). In an experiment based on an
actual legal case, Kamin and Rachlinski (1995) asked two groups to
estimate the probability of flood damage caused by blockage of a
city-owned drawbridge. The control group was told only the
background information known to the city when it decided not to
hire a bridge watcher. The experimental group was given this
information, plus the fact that a flood had actually occurred.
Instructions stated the city was negligent if the foreseeable
probability of flooding was greater than 10%. 76% of the control
group concluded the flood was so unlikely that no precautions were
necessary; 57% of the experimental group concluded the flood was so
likely that failure to take precautions was legally negligent. A
third experimental group was told the outcome andalso explicitly
instructed to avoid hindsight bias, which made no difference: 56%
concluded the city was legally negligent.

Viewing history through the lens of hindsight, we vastly
underestimate the cost of effective safety precautions.  In 1986,
the *Challenger* exploded for reasons traced to an O-ring losing
flexibility at low temperature.  There were warning signs of a
problem with the O-rings.  But preventing the *Challenger* disaster
would have required, not attending to the problem with the O-rings,
but attending to *every* warning sign which seemed as severe as the
O-ring problem, *without benefit of hindsight*.  It could have been
done, but it would have required a *general policy* much more
expensive than just fixing the O-Rings.

Shortly after September 11th 2001, I thought to myself,
*and now someone will turn up minor intelligence warnings of something-or-other, and then the hindsight will begin.* 
Yes, I'm sure they had some minor warnings of an al Qaeda plot, but
they probably also had minor warnings of mafia activity, nuclear
material for sale, and an invasion from Mars.

Because we don't see the cost of a general policy, we learn overly
specific lessons.  After September 11th, the FAA prohibited
box-cutters on airplanes - as if the problem had been the failure
to take *this particular* "obvious" precaution.  We don't learn the
general lesson:
*the cost of effective caution is very high because you must attend to problems that are not as obvious now as past problems seem in hindsight.*

The test of a model is how much probability it assigns to the
observed outcome.  Hindsight bias systematically distorts this
test; we think our model assigned much more probability than it
actually did.  Instructing the jury doesn't help.  You have to
[write down your predictions in advance](http://www.overcomingbias.com/2007/08/conservation-of.html). 
Or as Fischhoff (1982) put it:

> When we attempt to understand past events, we implicitly test the
> hypotheses or rules we use both to interpret and to anticipate the
> world around us. If, in hindsight, we systematically underestimate
> the surprises that the past held and holds for us, we are
> subjecting those hypotheses to inordinately weak tests and,
> presumably, finding little reason to change them.


* * * * *

Fischhoff, B. 1982. For those condemned to study the past:
Heuristics and biases in hindsight. In Kahneman et. al. 1982:
332â351.

Fischhoff, B., and Beyth, R. 1975. I knew it would happen:
Remembered probabilities of once-future things. Organizational
Behavior and Human Performance, 13: 1-16.

Kamin, K. and Rachlinski, J. 1995.
[Ex Post â  Ex Ante: Determining Liability in Hindsight](http://www.jstor.org/view/01477307/ap050075/05a00120/0).
Law and Human Behavior, 19(1): 89-104.

Sanchiro, C. 2003. Finding Error. Mich. St. L. Rev. 1189.

<h1 id="making-beliefs-pay-rent-in-anticipated-experiences"
>Making Beliefs Pay Rent (in Anticipated Experiences)</h1
><p
>Thus begins the ancient parable:</p
><p
><em
  >If a tree falls in a forest and no one hears it, does it make a sound? One says, &quot;Yes it does, for it makes vibrations in the air.&quot; Another says, &quot;No it does not, for there is no auditory processing in any brain.&quot;</em
  ></p
><p
>Suppose that, after the tree falls, the two walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other? Though the two argue, one saying &quot;No,&quot; and the other saying &quot;Yes,&quot; they do not anticipate any different experiences.  The two think they have different models of the world, but they have no difference with respect to what they expect will <em
  >happen to</em
  > them.</p
><p
>It's tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly. We don't see the atoms underlying the brick, but the atoms are in fact there. There is a floor beneath your feet, but you don't <em
  >experience</em
  > the floor directly; you see the light <em
  >reflected</em
  > from the floor, or rather, you see what your retina and visual cortex have processed of that light. To infer the floor from seeing the floor is to step back into the unseen causes of experience. It may seem like a very short and direct step, but it is still a step.</p
><p
>You stand on top of a tall building, next to a grandfather clock with an hour, minute, and ticking second hand. In your hand is a bowling ball, and you drop it off the roof. On which tick of the clock will you hear the crash of the bowling ball hitting the ground?</p
><p
>To answer precisely, you must use beliefs like <em
  >Earth's gravity is 9.8 meters per second per second,</em
  > and <em
  >This building is around 120 meters tall.</em
  > These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional. It probably does not exaggerate much to describe these two beliefs as sentences made out of words. But these two beliefs have an inferential <em
  >consequence</em
  > that is a direct sensory anticipation - if the clock's second hand is on the 12 numeral when you drop the ball, you anticipate seeing it on the 1 numeral when you hear the crash five seconds later. To anticipate sensory experiences as precisely as possible, we must process beliefs that are not anticipations of sensory experience.</p
><p
>It is a great strength of <em
  >Homo sapiens</em
  > that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.</p
><p
>The same brain that builds a network of inferred causes behind sensory experience, can also build a network of causes that is not connected to sensory experience, or poorly connected. Alchemists believed that phlogiston caused fire - we could oversimply their minds by drawing a little node labeled &quot;Phlogiston&quot;, and an arrow from this node to their sensory experience of a crackling campfire - but this belief yielded no advance predictions; the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance. Or suppose your postmodern English professor teaches you that the famous writer Wulky Wilkinsen is actually a &quot;post-utopian&quot;. What does this mean you should expect from his books? Nothing. The belief, if you can call it that, doesn't connect to sensory experience at all. But you had better remember the propositional assertion that &quot;Wulky Wilkinsen&quot; has the &quot;post-utopian&quot; attribute, so you can regurgitate it on the upcoming quiz. Likewise if &quot;post-utopians&quot; show &quot;colonial alienation&quot;; if the quiz asks whether Wulky Wilkinsen shows colonial alienation, you'd better answer yes. The beliefs are connected to each other, though still not connected to any anticipated experience.</p
><p
>We can build up whole networks of beliefs that are connected only to each other - call these &quot;floating&quot; beliefs. It is a uniquely human flaw among animal species, a perversion of <em
  >Homo sapiens's</em
  > ability to build more general and flexible belief networks.</p
><p
>The rationalist virtue of <em
  >empiricism</em
  > consists of constantly asking which experiences our beliefs predict - or better yet, prohibit.  Do you believe that phlogiston is the cause of fire?  Then what do you expect to see happen, because of that? Do you believe that Wulky Wilkinsen is a post-utopian? Then what do you expect to see because of that? No, not &quot;colonial alienation&quot;; <em
  >what experience will happen to you?</em
  > Do you believe that if a tree falls in the forest, and no one hears it, it still makes a sound? Then what experience must therefore befall you?</p
><p
>It is even better to ask: what experience <em
  >must not</em
  > happen to you?  Do you believe that <em
  >elan vital</em
  > explains the mysterious aliveness of living beings?  Then what does this belief <em
  >not</em
  > allow to happen - what would definitely falsify this belief? A null answer means that your belief does not <em
  >constrain</em
  > experience; it permits <em
  >anything</em
  > to happen to you.  It floats.</p
><p
>When you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can't find the difference of anticipation, you're probably arguing about labels in your belief network - or even worse, floating beliefs, barnacles on your network. If you don't know what experiences are implied by Wulky Wilkinsen being a post-utopian, you can go on arguing forever. (You can also publish papers forever.)</p
><p
>Above all, don't ask what to believe - ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it.</p
><h1 id="belief-in-belief"
>Belief in Belief</h1
><p
>Carl Sagan once told a <a href="http://www.godlessgeeks.com/LINKS/Dragon.htm"
  >parable</a
  > of a man who comes to us and claims: &quot;There is a dragon in my garage.&quot; Fascinating! We reply that we wish to see this dragon - let us set out at once for the garage! &quot;But wait,&quot; the claimant says to us, &quot;it is an <em
  >invisible</em
  > dragon.&quot;</p
><p
>Now as Sagan points out, this doesn't make the hypothesis unfalsifiable. Perhaps we go to the claimant's garage, and although we see no dragon, we hear heavy breathing from no visible source; footprints mysteriously appear on the ground; and instruments show that something in the garage is consuming oxygen and breathing out carbon dioxide.</p
><p
>But now suppose that we say to the claimant, &quot;Okay, we'll visit the garage and see if we can hear heavy breathing,&quot; and the claimant quickly says no, it's an <em
  >inaudible</em
  > dragon. We propose to measure carbon dioxide in the air, and the claimant says the dragon does not breathe. We propose to toss a bag of flour into the air to see if it outlines an invisible dragon, and the claimant immediately says, &quot;The dragon is permeable to flour.&quot;</p
><p
>Carl Sagan used this parable to illustrate the classic moral that poor hypotheses need to do fast footwork to avoid falsification. But I tell this parable to make a different point: The claimant must have an accurate model of the situation <em
  >somewhere</em
  > in his mind, because he can anticipate, in advance, <em
  >exactly which experimental results he'll need to excuse.</em
  ></p
><p
>Some philosophers have been much confused by such scenarios, asking, &quot;Does the claimant <em
  >really</em
  > believe there's a dragon present, or not?&quot; As if the human brain only had enough disk space to represent one belief at a time! Real minds are more tangled than that. As discussed in yesterday's post, there are different types of belief; <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/"
  >not all beliefs are direct anticipations</a
  >. The claimant clearly does not <em
  >anticipate</em
  > seeing anything unusual upon opening the garage door; otherwise he wouldn't make advance excuses. It may also be that the claimant's pool of propositional beliefs contains <em
  >There is a dragon in my garage.</em
  > It may seem, to a rationalist, that these two beliefs should collide and conflict even though they are of different types. Yet it is a physical fact that you can write &quot;The sky is green!&quot; next to a picture of a blue sky without the paper bursting into flames.</p
><p
>The rationalist virtue of empiricism is supposed to prevent us from this class of mistake. We're supposed to constantly ask our beliefs which experiences they predict, make them pay rent in anticipation. But the dragon-claimant's problem runs deeper, and cannot be cured with such simple advice. It's not exactly <em
  >difficult</em
  > to connect belief in a dragon to anticipated experience of the garage. If you believe there's a dragon in your garage, then you can expect to open up the door and see a dragon. If you don't see a dragon, then that means there's no dragon in your garage. This is pretty straightforward. You can even try it with your own garage.</p
><p
>No, this invisibility business is a symptom of something much worse.</p
><p
>Depending on how your childhood went, you may remember a time period when you first began to doubt Santa Claus's existence, but you still believed that you were <em
  >supposed</em
  > to believe in Santa Claus, so you tried to deny the doubts. As Daniel Dennett observes, where it is difficult to believe a thing, it is often much easier to believe that you <em
  >ought</em
  > to believe it. What does it mean to believe that the <a href="/lw/gt/a_fable_of_science_and_politics/"
  >Ultimate Cosmic Sky</a
  > is both perfectly blue and perfectly green? The statement is confusing; it's not even clear what it would <em
  >mean</em
  > to believe it - what exactly would <em
  >be</em
  > believed, if you believed. You can much more easily believe that it is <em
  >proper,</em
  > that it is <em
  >good</em
  > and <em
  >virtuous</em
  > and <em
  >beneficial,</em
  > to believe that the Ultimate Cosmic Sky is both perfectly blue and perfectly green.  Dennett calls this &quot;belief in belief&quot;.</p
><p
>And here things become complicated, as human minds are wont to do - I think even Dennett oversimplifies how this psychology works in practice. For one thing, if you believe in belief, you cannot admit to yourself that you only believe in belief, because it is virtuous to <em
  >believe,</em
  > not to believe in belief, and so if you only believe in belief, instead of believing, you are not virtuous. Nobody will <em
  >admit</em
  > to themselves, &quot;I don't believe the Ultimate Cosmic Sky is blue and green, but I believe I ought to believe it&quot; - not unless they are unusually capable of acknowledging their own lack of virtue. People don't believe in belief in belief, they just believe in belief.</p
><p
>(Those who find this confusing may find it helpful to study mathematical logic, which trains one to make very sharp distinctions between the proposition P, a proof of P, and a proof that P is provable.  There are similarly sharp distinctions between P, wanting P, believing P, wanting to believe P, and believing that you believe P.)</p
><p
>There's different kinds of belief in belief. You may believe in belief explicitly; you may recite in your deliberate stream of consciousness the verbal sentence &quot;It is virtuous to believe that the Ultimate Cosmic Sky is perfectly blue and perfectly green.&quot; (While also believing that you believe this, unless you are unusually capable of acknowledging your own lack of virtue.) But there's also less explicit forms of belief in belief. Maybe the dragon-claimant fears the public ridicule that he imagines will result if he publicly confesses he was wrong (although, in fact, a rationalist would congratulate him, and others are more likely to ridicule him if he goes on claiming there's a dragon in his garage). Maybe the dragon-claimant flinches away from the prospect of admitting to himself that there is no dragon, because it conflicts with his self-image as the glorious discoverer of the dragon, who saw in his garage what all others had failed to see.</p
><p
>If all our thoughts were deliberate verbal sentences like philosophers manipulate, the human mind would be a great deal easier for humans to understand. Fleeting mental images, unspoken flinches, desires acted upon without acknowledgement - these account for as much of ourselves as words.</p
><p
>While I disagree with Dennett on some details and complications, I still think that Dennett's notion of <em
  >belief in belief</em
  > is the key insight necessary to understand the dragon-claimant. But we need a wider concept of <em
  >belief,</em
  > not limited to verbal sentences. &quot;Belief&quot; should include unspoken anticipation-controllers.  &quot;Belief in belief&quot; should include unspoken cognitive-behavior-guiders. It is not psychologically realistic to say &quot;The dragon-claimant does not believe there is a dragon in his garage; he believes it is beneficial to believe there is a dragon in his garage.&quot;  But it is realistic to say the dragon-claimant <em
  >anticipates as if</em
  > there is no dragon in his garage, and <em
  >makes excuses as if</em
  > he believed in the belief.</p
><p
>You can possess an ordinary mental picture of your garage, with no dragons in it, which correctly predicts your experiences on opening the door, and never once think the verbal phrase <em
  >There is no dragon in my garage.</em
  > I even bet it's happened to you - that when you open your garage door or bedroom door or whatever, and expect to see no dragons, no such verbal phrase runs through your mind.</p
><p
>And to flinch away from giving up your belief in the dragon - or flinch away from giving up your <em
  >self-image</em
  > as a person who believes in the dragon - it is not necessary to explicitly think <em
  >I want to believe there's a dragon in my garage.</em
  > It is only necessary to flinch away from the prospect of admitting you don't believe.</p
><p
>To correctly anticipate, in advance, which experimental results shall need to be excused, the dragon-claimant must (a) possess an accurate anticipation-controlling model somewhere in his mind, and (b) act cognitively to protect either (b1) his free-floating propositional belief in the dragon or (b2) his self-image of believing in the dragon.</p
><p
>If someone believes in their belief in the dragon, and also believes in the dragon, the problem is much less severe.  They will be willing to stick their neck out on experimental predictions, and perhaps even agree to give up the belief if the experimental prediction is wrong - although belief in belief can still interfere with this, if the belief itself is not absolutely confident.  When someone makes up excuses <em
  >in advance,</em
  >it would seem to require that belief, and belief in belief, have become unsynchronized. # Bayesian Judo</p
><p
>You can have some fun with people whose <a href="/lw/i4/belief_in_belief/"
  >anticipations get out of sync with what they believe they believe</a
  >.</p
><p
>I was once at a dinner party, trying to explain to a man what I did for a living, when he said: &quot;I don't believe Artificial Intelligence is possible because only God can make a soul.&quot;</p
><p
>At this point I must have been divinely inspired, because I instantly responded: &quot;You mean if I can make an Artificial Intelligence, it proves your religion is false?&quot;</p
><p
>He said, &quot;What?&quot;</p
><p
>I said, &quot;Well, if your religion predicts that I can't possibly make an Artificial Intelligence, then, if I make an Artificial Intelligence, it means your religion is false. Either your religion allows that it might be possible for me to build an AI; or, if I build an AI, that disproves your religion.&quot;</p
><p
>There was a pause, as the one realized he had just made his hypothesis vulnerable to falsification, and then he said, &quot;Well, I didn't mean that you couldn't make an intelligence, just that it couldn't be emotional in the same way we are.&quot;</p
><p
>I said, &quot;So if I make an Artificial Intelligence that, without being deliberately preprogrammed with any sort of script, starts talking about an emotional life that sounds like ours, <em
  >that</em
  > means your religion is wrong.&quot;</p
><p
>He said, &quot;Well, um, I guess we may have to agree to disagree on this.&quot;</p
><p
>I said: &quot;No, we can't, actually. There's a theorem of rationality called Aumann's Agreement Theorem which shows that no two rationalists can agree to disagree. If two people disagree with each other, at least one of them must be doing something wrong.&quot;</p
><p
>We went back and forth on this briefly. Finally, he said, &quot;Well, I guess I was really trying to say that I don't think you can make something eternal.&quot;</p
><p
>I said, &quot;Well, I don't think so either! I'm glad we were able to reach agreement on this, as Aumann's Agreement Theorem requires.&quot;  I stretched out my hand, and he shook it, and then he wandered away.</p
><p
>A woman who had stood nearby, listening to the conversation, said to me gravely, &quot;That was beautiful.&quot;</p
><p
>&quot;Thank you very much,&quot; I said.</p
><h1 id="professing-and-cheering"
>Professing and Cheering</h1
><p
>I once attended a panel on the topic, &quot;Are science and religion compatible?&quot; One of the women on the panel, a pagan, held forth interminably upon how she believed that the Earth had been created when a giant primordial cow was born into the primordial abyss, who licked a primordial god into existence, whose descendants killed a primordial giant and used its corpse to create the Earth, etc. The tale was long, and detailed, and more absurd than the Earth being supported on the back of a giant turtle. And the speaker clearly knew enough science to know this.</p
><p
>I still find myself struggling for words to describe what I saw as this woman spoke. She spoke with... pride? Self-satisfaction? A deliberate flaunting of herself?</p
><p
>The woman went on describing her creation myth for what seemed like forever, but was probably only five minutes. That strange pride/satisfaction/flaunting clearly had something to do with her <em
  >knowing</em
  > that her beliefs were scientifically outrageous. And it wasn't that she hated science; as a panelist she professed that religion and science were compatible. She even talked about how it was quite understandable that the Vikings talked about a primordial abyss, given the land in which they lived - explained away her own religion! - and yet nonetheless insisted this was what she &quot;believed&quot;, said with peculiar satisfaction.</p
><p
>I'm not sure that Daniel Dennett's concept of &quot;<a href="/lw/i4/belief_in_belief/"
  >belief in belief</a
  >&quot; stretches to cover this event. It was weirder than that. She didn't recite her creation myth with the fanatical faith of someone who needs to reassure herself. She didn't act like she expected us, the audience, to be convinced - or like she needed our belief to validate her.</p
><p
>Dennett, in addition to suggesting belief in belief, has also suggested that much of what is called &quot;religious belief&quot; should really be studied as &quot;religious profession&quot;. Suppose an alien anthropologist studied a group of postmodernist English students who all seemingly <em
  >believed</em
  > that Wulky Wilkensen was a post-utopian author. The appropriate question may not be &quot;Why do the students all believe this strange belief?&quot; but &quot;Why do they all write this strange sentence on quizzes?&quot; Even if a sentence is essentially meaningless, you can still know when you are supposed to chant the response aloud.</p
><p
>I think Dennett may be slightly too cynical in suggesting that religious profession is <em
  >just</em
  > saying the belief aloud - most people are honest enough that, if they say a religious statement aloud, they will also feel obligated to say the verbal sentence into their own stream of consciousness.</p
><p
>But even the concept of &quot;religious profession&quot; doesn't seem to cover the pagan woman's claim to believe in the primordial cow. If you had to profess a religious belief to satisfy a priest, or satisfy a co-religionist - heck, to satisfy your own self-image as a religious person - you would have to <em
  >pretend</em
  >to believe <em
  >much more convincingly</em
  > than this woman was doing. As she recited her tale of the primordial cow, with that same strange flaunting pride, she wasn't even <em
  >trying</em
  > to be persuasive - wasn't even trying to convince us that she took her own religion seriously. I think that's the part that so took me aback. I know people who believe they believe ridiculous things, but when they profess them, they'll spend much more effort to convince themselves that they take their beliefs seriously.</p
><p
>It finally occurred to me that this woman wasn't trying to convince us or even convince herself. Her recitation of the creation story wasn't <em
  >about</em
  > the creation of the world at all. Rather, by launching into a five-minute diatribe about the primordial cow, she was <em
  >cheering</em
  > <em
  >for paganism,</em
  > like holding up a banner at a football game. A banner saying &quot;<a href="/lw/gt/a_fable_of_science_and_politics/"
  >GO BLUES</a
  >&quot; isn't a statement of fact, or an attempt to persuade; it doesn't have to be convincing - it's a cheer.</p
><p
>That strange flaunting pride... it was like she was marching naked in a gay pride parade. (Incidentally, I'd have no objection if she <em
  >had</em
  > marched naked in a gay pride parade. Lesbianism is not something that <a href="/lw/hp/feeling_rational/"
  >truth can destroy</a
  >.) It wasn't just a cheer, like marching, but an outrageous cheer, like marching naked - believing that she couldn't be arrested or criticized, because she was doing it for her pride parade.</p
><p
>That's why it mattered to her that what she was saying was beyond ridiculous. If she'd tried to make it sound more plausible, it would have been like putting on clothes.</p
><h1 id="belief-as-attire"
>Belief as Attire</h1
><p
>I have so far distinguished between belief as <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/"
  >anticipation-controller</a
  >, <a href="/lw/i4/belief_in_belief/"
  >belief in belief</a
  >, <a href="/lw/i6/professing_and_cheering/"
  >professing and cheering</a
  >.  Of these, we might call anticipation-controlling beliefs &quot;proper beliefs&quot; and the other forms &quot;improper belief&quot;.  A proper belief can be wrong or irrational, e.g., someone who genuinely anticipates that prayer will cure her sick baby, but the other forms are arguably &quot;not belief at all&quot;.</p
><p
>Yet another form of improper belief is belief as group-identification - as a way of belonging.  Robin Hanson uses the excellent <a href="http://www.overcomingbias.com/2007/08/professing-and-.html#comment-78160476"
  >metaphor</a
  > of wearing unusual clothing, a group uniform like a priest's vestments or a Jewish skullcap, and so I will call this &quot;belief as attire&quot;.</p
><p
>In terms of <a href="/lw/i0/are_your_enemies_innately_evil/"
  >humanly realistic psychology</a
  >, the Muslims who flew planes into the World Trade Center undoubtedly saw themselves as heroes defending truth, justice, and the Islamic Way from hideous alien monsters a la the movie <a href="http://www.imdb.com/title/tt0116629/"
  >Independence Day</a
  >.  Only a very inexperienced nerd, the sort of nerd who has no idea how non-nerds see the world, would say this out loud in an Alabama bar.  It is not an American thing to say.  The American thing to say is that the terrorists &quot;hate our freedom&quot; and that flying a plane into a building is a &quot;cowardly act&quot;.  You cannot say the phrases &quot;heroic self-sacrifice&quot; and &quot;suicide bomber&quot; in the same sentence, even for the sake of accurately describing how the Enemy sees the world.   The very <em
  >concept</em
  > of the courage and altruism of a suicide bomber is Enemy attire - you can tell, because the Enemy talks about it.  The cowardice and sociopathy of a suicide bomber is American attire.  There are no quote marks you can use to talk about how the Enemy sees the world; it would be like dressing up as a Nazi for Halloween.</p
><p
>Belief-as-attire may help explain how people can be <em
  >passionate</em
  > about improper beliefs.  Mere <a href="/lw/i4/belief_in_belief/"
  >belief in belief</a
  >, or <a href="/lw/i6/professing_and_cheering/"
  >religious professing</a
  >, would have some trouble creating genuine, deep, powerful emotional effects.  Or so I suspect; I confess I'm not an expert here.  But my impression is this:  People who've stopped anticipating-as-if their religion is true, will go to great lengths to <em
  >convince</em
  > themselves they are passionate, and this desperation can be mistaken for passion.  But it's not the same fire they had as a child.</p
><p
>On the other hand, it is very easy for a human being to genuinely, passionately, gut-level belong to a group, to cheer for<a href="/lw/gt/a_fable_of_science_and_politics/"
  >their favorite sports team</a
  >.  (This is the foundation on which rests the swindle of &quot;Republicans vs. Democrats&quot; and analogous <a href="/lw/hu/the_third_alternative/"
  >false dilemmas</a
  > in other countries, but that's a topic for another post.)  Identifying with a tribe is a very strong emotional force.  People will die for it.  And once you get people to identify with a tribe, the beliefs which are attire of that tribe will be spoken with the full passion of belonging to that tribe.</p
><h1 id="focus-your-uncertainty"
>Focus Your Uncertainty</h1
><p
>Will bond yields go up, or down, or remain the same? If you're a TV pundit and your job is to explain the outcome after the fact, then there's no reason to worry. No matter <em
  >which</em
  > of the three possibilities comes true, you'll be able to explain why the outcome perfectly fits your pet market theory . There's no reason to think of these three possibilities as somehow <em
  >opposed</em
  > to one another, as <em
  >exclusive,</em
  > because you'll get full marks for punditry no matter which outcome occurs.</p
><p
>But wait! Suppose you're a <em
  >novice</em
  > TV pundit, and you aren't experienced enough to make up plausible explanations on the spot. You need to prepare remarks in advance for tomorrow's broadcast, and you have limited time to prepare. In this case, it would be helpful to know <em
  >which</em
  > outcome will actually occur - whether bond yields will go up, down, or remain the same - because then you would only need to prepare <em
  >one</em
  > set of excuses.</p
><p
>Alas, no one can possibly foresee the future. What are you to do? You certainly can't use &quot;probabilities&quot;. We all <a href="/lw/i2/two_more_things_to_unlearn_from_school/"
  >know from school</a
  > that &quot;probabilities&quot; are little numbers that appear next to a word problem, and there aren't any little numbers here. Worse, you <em
  >feel</em
  > uncertain. You don't remember <em
  >feeling</em
  > uncertain while you were manipulating the little numbers in word problems. <em
  >College classes teaching math</em
  >are nice clean places, therefore <em
  >math itself</em
  > can't apply to life situations that aren't nice and clean.  You wouldn't want to inappropriately <a href="http://www.aft.org/pubs-reports/american_educator/issues/summer07/Crit_Thinking.pdf"
  >transfer thinking skills from one context to another</a
  >.  Clearly, this is not a matter for &quot;probabilities&quot;.</p
><p
>Nonetheless, you only have 100 minutes to prepare your excuses. You can't spend the entire 100 minutes on &quot;up&quot;, and also spend all 100 minutes on &quot;down&quot;, and also spend all 100 minutes on &quot;same&quot;. You've got to prioritize somehow.</p
><p
>If you needed to justify your time expenditure to a review committee, you would have to spend equal time on each possibility.  Since there are no little numbers written down, you'd have no documentation to justify spending different amounts of time. You can hear the reviewers now: <em
  >And why, Mr. Finkledinger, did you spend exactly 42 minutes on excuse #3? Why not 41 minutes, or 43? Admit it - you're not being objective! You're playing subjective favorites!</em
  ></p
><p
>But, you realize with a small flash of relief, there's no review committee to scold you. This is good, because there's a major Federal Reserve announcement tomorrow, and it seems unlikely that bond prices will remain the same. You don't want to spend 33 precious minutes on an excuse you don't anticipate needing.</p
><p
>Your mind keeps drifting to the explanations you use on television, of why each event plausibly fits your market theory. But it rapidly becomes clear that plausibility can't help you here - all three events are plausible. Fittability to your pet market theory doesn't tell you how to divide your time. There's an uncrossable gap between your 100 minutes of time, which are conserved; versus your ability to explain how an outcome fits your theory, which is unlimited.</p
><p
>And yet... even in your uncertain state of mind, it seems that you <em
  >anticipate</em
  > the three events differently; that you <em
  >expect</em
  > to need some excuses more than others. And - this is the fascinating part - when you think of something that makes it seem <em
  >more</em
  > likely that bond prices will go up, then you feel <em
  >less</em
  > likely to need an excuse for bond prices going down or remaining the same.</p
><p
>It even seems like there's a relation between how much you anticipate each of the three outcomes, and how much time you want to spend preparing each excuse. Of course the relation can't actually be quantified. You have 100 minutes to prepare your speech, but there isn't 100 of anything to divide up in this anticipation business. (Although you do work out that, <em
  >if</em
  > some particular outcome occurs, then your utility function is logarithmic in time spent preparing the excuse.)</p
><p
>Still... your mind keeps coming back to the idea that anticipation is limited, unlike excusability, but like time to prepare excuses. Maybe anticipation should be treated as a <em
  >conserved resource,</em
  > like money. Your first impulse is to try to get more anticipation, but you soon realize that, even if you get more anticiptaion, you won't have any more time to prepare your excuses. No, your only course is to <em
  >allocate</em
  > your <em
  >limited supply</em
  > of anticipation as best you can.</p
><p
>You're pretty sure you weren't taught anything like that in your statistics courses. They didn't tell you what to do when you <em
  >felt</em
  > so terribly uncertain. They didn't tell you what to do when there were no little numbers handed to you.  Why, even if you tried to use numbers, you might end up using any sort of numbers at all - there's no hint what kind of math to use, if you should be using math!  Maybe you'd end up using <em
  >pairs</em
  > of numbers, right and left numbers, which you'd call DS for Dexter-Sinister... or who knows what else?  (Though you do have only 100 minutes to spend preparing excuses.)</p
><p
>If only there were an art of <em
  >focusing your uncertainty</em
  > - of <em
  >squeezing</em
  > as much anticipation as possible into whichever outcome will <em
  >actually happen!</em
  ></p
><p
>But what could we call an art like that?  And what would the rules be like?</p
><h1 id="the-virtue-of-narrowness"
>The Virtue of Narrowness</h1
><blockquote
><p
  >What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.</p
  ></blockquote
><p
>Within their own professions, people grasp the importance of narrowness; a car mechanic knows the difference between a carburetor and a radiator, and would not think of them both as &quot;car parts&quot;.  A hunter-gatherer knows the difference between a lion and a panther.  A janitor does not wipe the floor with window cleaner, even if the bottles look similar to one who has not mastered the art.</p
><p
>Outside their own professions, people often commit the misstep of trying to broaden a word as widely as possible, to cover as much territory as possible.  Is it not more glorious, more wise, more impressive, to talk about <em
  >all</em
  > the apples in the world?  How much loftier it must be to <em
  >explain human thought in general,</em
  > without being distracted by smaller questions, such as how humans invent techniques for solving a Rubik's Cube.  Indeed, it scarcely seems necessary to consider <em
  >specific</em
  > questions at all; isn't a general theory a worthy enough accomplishment on its own?</p
><p
>It is the way of the curious to lift up one pebble from among a million pebbles on the shore, and see something new about it, something interesting, something different. You call these pebbles &quot;diamonds&quot;, and ask what might be special about them - what inner qualities they might have in common, beyond the glitter you first noticed. And then someone else comes along and says: &quot;Why not call <em
  >this</em
  > pebble a diamond too? And this one, and this one?&quot; They are enthusiastic, and they mean well. For it seems undemocratic and exclusionary and elitist and unholistic to call some pebbles &quot;diamonds&quot;, and others not. It seems... <em
  >narrow-minded...</em
  > if you'll pardon the phrase. Hardly <em
  >open,</em
  > hardly <em
  >embracing,</em
  > hardly <em
  >communal.</em
  ></p
><p
>You might think it poetic, to give one word many meanings, and thereby spread shades of connotation all around. But even poets, if they are good poets, must learn to see the world precisely. It is not enough to compare love to a flower. Hot jealous unconsummated love is not the same as the love of a couple married for decades. If you need a flower to symbolize jealous love, you must go into the garden, and look, and make subtle distinctions - find a flower with a heady scent, and a bright color, and thorns. Even if your intent is to shade meanings and cast connotations, you must keep precise track of exactly which meanings you shade and connote.</p
><p
>It is a necessary part of the rationalist's art - or even the poet's art! - to focus narrowly on unusual pebbles which possess some special quality. And look at the details which those pebbles - and those pebbles alone! - share among each other.  This is not a sin.</p
><p
>It is perfectly all right for modern evolutionary biologists to explain <em
  >just</em
  > the patterns of living creatures, and not the &quot;evolution&quot; of stars or the &quot;evolution&quot; of technology.  Alas, some unfortunate souls use the same word &quot;evolution&quot; to cover the naturally selected patterns of replicating life, <em
  >and</em
  > the strictly accidental structure of stars, <em
  >and</em
  > the intelligently configured structure of technology.  And as we all know, if people use the same word, it must all be the same thing.  You should automatically generalize anything you think you know about biological evolution to technology.  Anyone who tells you otherwise must be a mere pointless pedant.  It couldn't possibly be that your abysmal ignorance of modern evolutionary theory is so total that you can't tell the difference between a carburetor and a radiator.  That's unthinkable.  No, the <em
  >other</em
  > guy - you know, the one who's studied the math - is just too dumb to see the connections.</p
><p
>And what could be more virtuous than seeing connections?  Surely the wisest of all human beings are the New Age gurus who say &quot;Everything is connected to everything else.&quot;  If you ever say this aloud, you should pause, so that everyone can absorb the sheer shock of this Deep Wisdom.</p
><p
>There is a trivial mapping between a graph and its complement.  A fully connected graph, with an edge between every two vertices, conveys the same amount of information as a graph with no edges at all.  The important graphs are the ones where some things are <em
  >not</em
  > connected to some other things.</p
><p
>When the unenlightened ones try to be profound, they draw endless verbal comparisons between this topic, and that topic, which is like this, which is like that; until their graph is fully connected and also totally useless. The remedy is specific knowledge and in-depth study. When you understand things in detail, you can see how they are <em
  >not</em
  > alike, and start enthusiastically subtracting edges <em
  >off</em
  > your graph.</p
><p
>Likewise, the important categories are the ones that do not contain everything in the universe.  Good hypotheses can only explain some possible outcomes, and not others.</p
><p
>It was perfectly all right for Isaac Newton to explain <em
  >just</em
  > gravity, <em
  >just</em
  > the way things fall down - and how planets orbit the Sun, and how the Moon generates the tides - but <em
  >not</em
  > the role of money in human society or how the heart pumps blood. Sneering at narrowness is rather reminiscent of ancient Greeks who thought that going out and actually <em
  >looking</em
  > at things was manual labor, and manual labor was for slaves.</p
><p
>As Plato put it (in <em
  >The Republic, Book VII</em
  >):</p
><blockquote
><p
  >&quot;If anyone should throw back his head and learn something by staring at the varied patterns on a ceiling, apparently you would think that he was contemplating with his reason, when he was only staring with his eyes... I cannot but believe that no study makes the soul look on high except that which is concerned with real being and the unseen. Whether he gape and stare upwards, or shut his mouth and stare downwards, if it be things of the senses that he tries to learn something about, I declare he never could learn, for none of these things admit of knowledge: I say his soul is looking down, not up, even if he is floating on his back on land or on sea!&quot;</p
  ></blockquote
><p
>Many today make a similar mistake, and think that narrow concepts are as lowly and unlofty and unphilosophical as, say, going out and looking at things - an endeavor only suited to the underclass.  But rationalists - and also poets - need narrow words to express precise thoughts; they need categories which include only some things, and exclude others. There's nothing wrong with focusing your mind, narrowing your categories, excluding possibilities, and sharpening your propositions. Really, there isn't! If you make your words too broad, you end up with something that isn't true and doesn't even make good poetry.</p
><p
><em
  >And DON'T EVEN GET ME STARTED on people who think Wikipedia is an &quot;Artificial Intelligence&quot;, the invention of LSD was a &quot;Singularity&quot; or that corporations are &quot;superintelligent&quot;!</em
  ></p
><h1 id="your-strength-as-a-rationalist"
>Your Strength as a Rationalist</h1
><p
>(The following happened to me in an IRC chatroom, long enough ago that I was still hanging around in IRC chatrooms.  Time has fuzzed the memory and my report may be imprecise.)</p
><p
>So there I was, in an IRC chatroom, when someone reports that a friend of his needs medical advice.  His friend says that he's been having sudden chest pains, so he called an ambulance, and the ambulance showed up, but the paramedics told him it was nothing, and left, and now the chest pains are getting worse.  What should his friend do?</p
><p
>I was confused by this story.  I remembered reading about homeless people in New York who would call ambulances just to be taken someplace warm, and how the paramedics always had to take them to the emergency room, even on the 27th iteration.  Because if they didn't, the ambulance company could be sued for lots and lots of money.  Likewise, emergency rooms are legally obligated to treat anyone, regardless of ability to pay.  (And the hospital absorbs the costs, which are enormous, so hospitals are closing their emergency rooms...  It makes you wonder what's the point of having economists if we're just going to ignore them.)  So I didn't quite understand how the described events could have happened.  <em
  >Anyone</em
  > reporting sudden chest pains should have been hauled off by an ambulance instantly.</p
><p
>And this is where I fell down as a rationalist.  I remembered several occasions where my doctor would completely fail to panic at the report of symptoms that seemed, to me, very alarming.  And the Medical Establishment was always right.  Every single time.  I had chest pains myself, at one point, and the doctor patiently explained to me that I was describing chest muscle pain, not a heart attack.  So I said into the IRC channel, &quot;Well, if the paramedics told your friend it was nothing, it must <em
  >really be</em
  > nothing - they'd have hauled him off if there was the tiniest chance of serious trouble.&quot;</p
><p
>Thus I managed to explain the story within my existing model, though the fit still felt a little forced...</p
><p
>Later on, the fellow comes back into the IRC chatroom and says his friend made the whole thing up.  Evidently this was not one of his more reliable friends.</p
><p
>I should have realized, perhaps, that an unknown acquaintance of an acquaintance in an IRC channel might be <a href="http://www.overcomingbias.com/2007/08/truth-bias.html"
  >less reliable</a
  > than a published journal article.  Alas, belief is easier than disbelief; <a href="http://www.wjh.harvard.edu/~dtg/Gilbert%20et%20al%20(EVERYTHING%20YOU%20READ).pdf"
  >we believe instinctively, but disbelief requires a conscious effort</a
  >.</p
><p
>So instead, by dint of mighty straining, I forced my model of reality to explain an anomaly that <em
  >never actually happened.</em
  >  And I <em
  >knew</em
  > how embarrassing this was.  I <em
  >knew</em
  > that the usefulness of a model is not what it can explain, but what it can't.  A hypothesis that forbids nothing, permits everything, and thereby fails to <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/"
  >constrain anticipation</a
  >.</p
><p
>Your strength as a rationalist is your ability to be more confused by fiction than by reality.  If you are equally good at explaining any outcome, you have zero knowledge.</p
><p
>We are all weak, from time to time; the sad part is that I <em
  >could</em
  > have been stronger.  I had all the information I needed to arrive at the correct answer, I even <em
  >noticed</em
  > the problem, and then I ignored it.  My feeling of confusion was a Clue, and I threw my Clue away.</p
><p
>I should have paid more attention to that sensation of <em
  >still feels a little forced.</em
  > It's one of the most important feelings a truthseeker can have, a part of your strength as a rationalist.  It is a design flaw in human cognition that this sensation manifests as a quiet strain in the back of your mind, instead of a wailing alarm siren and a glowing neon sign reading &quot;EITHER YOUR MODEL IS FALSE OR THIS STORY IS WRONG.&quot;</p
><h1 id="absence-of-evidence-is-evidence-of-absence"
>Absence of Evidence is Evidence of Absence</h1
><p
>From Robyn Dawes's <em
  >Rational Choice in an Uncertain World</em
  >:</p
><blockquote
><p
  >Post-hoc fitting of evidence to hypothesis was involved in a most grievous chapter in United States history: the internment of Japanese-Americans at the beginning of the Second World War.  When California governor Earl Warren testified before a congressional hearing in San Francisco on February 21, 1942, a questioner pointed out that there had been no sabotage or any other type of espionage by the Japanese-Americans up to that time.  Warren responded, &quot;I take the view that this lack [of subversive activity] is the most ominous sign in our whole situation. It convinces me more than perhaps any other factor that the sabotage we are to get, the Fifth Column activities are to get, are timed just like Pearl Harbor was timed... I believe we are just being lulled into a false sense of security.&quot;</p
  ></blockquote
><p
>Consider Warren's argument from a <a href="http://yudkowsky.net/bayes/bayes.html"
  >Bayesian perspective</a
  >.  When we see evidence, hypotheses that assigned a <em
  >higher</em
  > likelihood to that evidence, gain probability at the expense of hypotheses that assigned a <em
  >lower</em
  > likelihood to the evidence.  This is a phenomenon of <em
  >relative</em
  > likelihoods and <em
  >relative</em
  > probabilities.  You can assign a high likelihood to the evidence and still lose probability mass to some other hypothesis, if that other hypothesis assigns a likelihood that is even higher.</p
><p
>Warren seems to be arguing that, given that we see no sabotage, this <em
  >confirms</em
  > that a Fifth Column exists.  You could argue that a Fifth Column <em
  >might</em
  > delay its sabotage.  But the likelihood is still higher that the <em
  >absence</em
  > of a Fifth Column would perform an absence of sabotage.</p
><p
>Let E stand for the observation of sabotage, H1 for the hypothesis of a Japanese-American Fifth Column, and H2 for the hypothesis that no Fifth Column exists.  Whatever the likelihood that a Fifth Column would do no sabotage, the probability P(E|H1), it cannot be as large as the likelihood that no Fifth Column does no sabotage, the probability P(E|H2).  So observing a lack of sabotage increases the probability that no Fifth Column exists.</p
><p
>A lack of sabotage doesn't <em
  >prove</em
  > that no Fifth Column exists.  Absence of <em
  >proof</em
  > is not <em
  >proof</em
  > of absence.  In logic, A-&gt;B, &quot;A implies B&quot;, is not equivalent to ~A-&gt;~B, &quot;not-A implies not-B&quot;.</p
><p
>But in probability theory, absence of <em
  >evidence</em
  > is always <em
  >evidence</em
  > of absence.   If E is a binary event and P(H|E) &gt; P(H), &quot;seeing E increases the probability of H&quot;; then P(H|~E) &lt; P(H), &quot;failure to observe E decreases the probability of H&quot;.  P(H) is a weighted mix of P(H|E) and P(H|~E), and necessarily lies between the two.  If any of this sounds at all confusing, see <a href="http://yudkowsky.net/bayes/bayes.html"
  >An Intuitive Explanation of Bayesian Reasoning</a
  >.</p
><p
>Under the vast majority of real-life circumstances, a cause may not reliably produce signs of itself, but the absence of the cause is even less likely to produce the signs.  The absence of an observation may be strong evidence of absence or very weak evidence of absence, depending on how likely the cause is to produce the observation.  The absence of an observation that is only weakly permitted (even if the alternative hypothesis does not allow it at all), is very weak evidence of absence (though it is evidence nonetheless).  This is the fallacy of &quot;gaps in the fossil record&quot; - fossils form only rarely; it is futile to trumpet the absence of a weakly permitted observation when many strong positive observations have already been recorded.  But if there are <em
  >no</em
  > positive observations at all, it is time to worry; hence the Fermi Paradox.</p
><p
><a href="/lw/if/your_strength_as_a_rationalist/"
  >Your strength as a rationalist</a
  > is your ability to be more confused by fiction than by reality; if you are equally good at explaining any outcome you have zero knowledge.  The strength of a model is not what it <em
  >can</em
  > explain, but what it <em
  >can't</em
  >, for only prohibitions <a href="/lw/i4/belief_in_belief/"
  >constrain anticipation</a
  >.  If you don't notice when your model makes the evidence unlikely, you might as well have no model, and also you might as well have no evidence; no brain and no eyes.</p
><h1 id="conservation-of-expected-evidence"
>Conservation of Expected Evidence</h1
><p
>Friedrich Spee von Langenfeld, a priest who heard the confessions of condemned witches, wrote in 1631 the <em
  >Cautio Criminalis</em
  > ('prudence in criminal cases') in which he bitingly described the decision tree for condemning accused witches:  If the witch had led an evil and improper life, she was guilty; if she had led a good and proper life, this too was a proof, for witches dissemble and try to appear especially virtuous. After the woman was put in prison: if she was afraid, this proved her guilt; if she was not afraid, this proved her guilt, for witches characteristically pretend innocence and wear a bold front. Or on hearing of a denunciation of witchcraft against her, she might seek flight or remain; if she ran, that proved her guilt; if she remained, the devil had detained her so she could not get away.</p
><p
>Spee acted as confessor to many witches; he was thus in a position to observe <em
  >every</em
  > branch of the accusation tree, that no matter <em
  >what</em
  > the accused witch said or did, it was held a proof against her.  In any individual case, you would only hear one branch of the dilemma.  It is for this reason that scientists write down their experimental predictions in advance.</p
><p
>But <em
  >you can't have it both ways</em
  > - as a matter of probability theory, not mere fairness.  The rule that &quot;<a href="/lw/ih/absence_of_evidence_is_evidence_of_absence/"
  >absence of evidence <em
    >is</em
    > evidence of absence</a
  >&quot; is a special case of a more general law, which I would name Conservation of Expected Evidence:  The <em
  >expectation</em
  > of the posterior probability, after viewing the evidence, must equal the prior probability.</p
><blockquote
><p
  ><strong
    >P(H) = P(H)</strong
    ><em
    >*<br
       />P(H) = P(H,E) + P(H,~E)</em
    ><strong
    >*<br
       />P(H) = P(H|E)*P(E) + P(H|~E)*P(~E)</strong
    ></p
  ></blockquote
><p
><em
  >Therefore,</em
  > for every expectation of evidence, there is an equal and opposite expectation of counterevidence.</p
><p
>If you expect a strong probability of seeing weak evidence in one direction, it must be balanced by a weak expectation of seeing strong evidence in the other direction.  If you're very confident in your theory, and therefore anticipate seeing an outcome that matches your hypothesis, this can only provide a very small increment to your belief (it is already close to 1); but the unexpected failure of your prediction would (and must) deal your confidence a huge blow.  On <em
  >average,</em
  > you must expect to be <em
  >exactly</em
  > as confident as when you started out.  Equivalently, the mere <em
  >expectation</em
  > of encountering evidence - before you've actually seen it - should not shift your prior beliefs.  (Again, if this is not intuitively obvious, see <a href="http://yudkowsky.net/bayes/bayes.html"
  >An Intuitive Explanation of Bayesian Reasoning</a
  >.)</p
><p
>So if you <a href="/lw/ih/absence_of_evidence_is_evidence_of_absence/"
  >claim</a
  > that &quot;no sabotage&quot; is evidence <em
  >for</em
  >the existence of a Japanese-American Fifth Column, you must conversely hold that seeing sabotage would argue <em
  >against</em
  > a Fifth Column.  If you claim that &quot;a good and proper life&quot; is evidence that a woman is a witch, then an evil and improper life must be evidence that she is not a witch.  If you <a href="/lw/i8/religions_claim_to_be_nondisprovable/"
  >argue</a
  > that God, to test humanity's faith, refuses to reveal His existence, then the miracles described in the Bible must argue against the existence of God.</p
><p
>Doesn't quite sound right, does it?  Pay attention to that feeling of <em
  >this seems a little forced,</em
  > that <a href="/lw/if/your_strength_as_a_rationalist/"
  >quiet strain in the back of your mind</a
  >.  It's important.</p
><p
>For a true Bayesian, it is impossible to seek evidence that <em
  >confirms</em
  > a theory.  There is no possible plan you can devise, no clever strategy, no cunning device, by which you can legitimately expect your confidence in a fixed proposition to be higher (on <em
  >average</em
  >) than before.  You can only ever seek evidence to <em
  >test</em
  > a theory, not to confirm it.</p
><p
>This realization can take quite a load off your mind.  You need not worry about how to interpret every possible experimental result to confirm your theory.  You needn't bother planning how to make <em
  >any</em
  > given iota of evidence confirm your theory, because you know that for every expectation of evidence, there is an equal and oppositive expectation of counterevidence.  If you try to weaken the counterevidence of a possible &quot;abnormal&quot; observation, you can only do it by weakening the support of a &quot;normal&quot; observation, to a precisely equal and opposite degree.  It is a zero-sum game.  No matter how you connive, no matter how you argue, no matter how you strategize, you can't possibly expect the resulting game plan to shift your beliefs (on average) in a particular direction.</p
><p
>You might as well sit back and relax while you wait for the evidence to come in.</p
><p
>...human psychology is <em
  >so</em
  > screwed up.</p
><h1 id="hindsight-bias"
>Hindsight Bias</h1
><p
><em
  >Hindsight bias</em
  > is when people who know the answer vastly overestimate its <em
  >predictability</em
  > or <em
  >obviousness,</em
  > compared to the estimates of subjects who must guess without advance knowledge.  Hindsight bias is sometimes called the <em
  >I-knew-it-all-along effect</em
  >.</p
><p
>Fischhoff and Beyth (1975) presented students with historical accounts of unfamiliar incidents, such as a conflict between the Gurkhas and the British in 1814.  Given the account as background knowledge, five groups of students were asked what they would have predicted as the probability for each of four outcomes: British victory, Gurkha victory, stalemate with a peace settlement, or stalemate with no peace settlement.  Four experimental groups were respectively told that these four outcomes were the historical outcome.  The fifth, control group was not told any historical outcome.  In every case, a group told an outcome assigned substantially higher probability to that outcome, than did any other group or the control group.</p
><p
>Hindsight bias matters in legal cases, where a judge or jury must determine whether a defendant was legally negligent in failing to foresee a hazard (Sanchiro 2003). In an experiment based on an actual legal case, Kamin and Rachlinski (1995) asked two groups to estimate the probability of flood damage caused by blockage of a city-owned drawbridge. The control group was told only the background information known to the city when it decided not to hire a bridge watcher. The experimental group was given this information, plus the fact that a flood had actually occurred. Instructions stated the city was negligent if the foreseeable probability of flooding was greater than 10%. 76% of the control group concluded the flood was so unlikely that no precautions were necessary; 57% of the experimental group concluded the flood was so likely that failure to take precautions was legally negligent. A third experimental group was told the outcome andalso explicitly instructed to avoid hindsight bias, which made no difference: 56% concluded the city was legally negligent.</p
><p
>Viewing history through the lens of hindsight, we vastly underestimate the cost of effective safety precautions.  In 1986, the <em
  >Challenger</em
  > exploded for reasons traced to an O-ring losing flexibility at low temperature.  There were warning signs of a problem with the O-rings.  But preventing the <em
  >Challenger</em
  > disaster would have required, not attending to the problem with the O-rings, but attending to <em
  >every</em
  > warning sign which seemed as severe as the O-ring problem, <em
  >without benefit of hindsight</em
  >.  It could have been done, but it would have required a <em
  >general policy</em
  > much more expensive than just fixing the O-Rings.</p
><p
>Shortly after September 11th 2001, I thought to myself, <em
  >and now someone will turn up minor intelligence warnings of something-or-other, and then the hindsight will begin.</em
  >  Yes, I'm sure they had some minor warnings of an al Qaeda plot, but they probably also had minor warnings of mafia activity, nuclear material for sale, and an invasion from Mars.</p
><p
>Because we don't see the cost of a general policy, we learn overly specific lessons.  After September 11th, the FAA prohibited box-cutters on airplanes - as if the problem had been the failure to take <em
  >this particular</em
  > &quot;obvious&quot; precaution.  We don't learn the general lesson: <em
  >the cost of effective caution is very high because you must attend to problems that are not as obvious now as past problems seem in hindsight.</em
  ></p
><p
>The test of a model is how much probability it assigns to the observed outcome.  Hindsight bias systematically distorts this test; we think our model assigned much more probability than it actually did.  Instructing the jury doesn't help.  You have to <a href="http://www.overcomingbias.com/2007/08/conservation-of.html"
  >write down your predictions in advance</a
  >.  Or as Fischhoff (1982) put it:</p
><blockquote
><p
  >When we attempt to understand past events, we implicitly test the hypotheses or rules we use both to interpret and to anticipate the world around us. If, in hindsight, we systematically underestimate the surprises that the past held and holds for us, we are subjecting those hypotheses to inordinately weak tests and, presumably, finding little reason to change them.</p
  ></blockquote
><hr
 /><p
>Fischhoff, B. 1982. For those condemned to study the past: Heuristics and biases in hindsight. In Kahneman et. al. 1982: 332â351.</p
><p
>Fischhoff, B., and Beyth, R. 1975. I knew it would happen: Remembered probabilities of once-future things. Organizational Behavior and Human Performance, 13: 1-16.</p
><p
>Kamin, K. and Rachlinski, J. 1995. <a href="http://www.jstor.org/view/01477307/ap050075/05a00120/0"
  >Ex Post â  Ex Ante: Determining Liability in Hindsight</a
  >. Law and Human Behavior, 19(1): 89-104.</p
><p
>Sanchiro, C. 2003. Finding Error. Mich. St. L. Rev. 1189.</p
><h1 id="hindsight-devalues-science"
>Hindsight Devalues Science</h1
><p
>This <a href="http://csml.som.ohio-state.edu/Music829C/hindsight.bias.html"
  >excerpt</a
  > from Meyers's <em
  >Exploring Social Psychology</em
  > is worth reading in entirety.  Cullen Murphy, editor of <em
  >The Atlantic,</em
  > said that the social sciences turn up &quot;no ideas or conclusions that can't be found in [any] encyclopedia of quotations... Day after day social scientists go out into the world.  Day after day they discover that people's behavior is pretty much what you'd expect.&quot;</p
><p
>Of course, the &quot;expectation&quot; is all <a href="/lw/il/hindsight_bias/"
  >hindsight</a
  >.  (Hindsight bias:  Subjects who know the actual answer to a question assign much higher probabilities they &quot;would have&quot; guessed for that answer, compared to subjects who must guess without knowing the answer.)</p
><p
>The historian Arthur Schlesinger, Jr. dismissed scientific studies of WWII soldiers' experiences as &quot;ponderous demonstrations&quot; of common sense.  For example:</p
><ol style="list-style-type: decimal;"
><li
  >Better educated soldiers suffered more adjustment problems than less educated soldiers. (Intellectuals were less prepared for battle stresses than street-smart people.)  </li
  ><li
  >Southern soldiers coped better with the hot South Sea Island climate than Northern soldiers. (Southerners are more accustomed to hot weather.)  </li
  ><li
  >White privates were more eager to be promoted to noncommissioned officers than Black privates. (Years of oppression take a toll on achievement motivation.)  </li
  ><li
  >Southern Blacks preferred Southern to Northern White officers (because Southern officers were more experienced and skilled in interacting with Blacks).  </li
  ><li
  >As long as the fighting continued, soldiers were more eager to return home than after the war ended. (During the fighting, soldiers knew they were in mortal danger.)  </li
  ></ol
><p
>How many of these findings do you think you <em
  >could have</em
  > predicted in advance?   3 out of 5?  4 out of 5?  Are there any cases where you would have predicted the opposite - where your model <a href="/lw/ii/conservation_of_expected_evidence/"
  >takes a hit</a
  >?  Take a moment to think before continuing...</p
><p
>In this demonstration (from Paul Lazarsfeld by way of Meyers), all of the findings above are the <em
  >opposite</em
  > of what was actually found.  How many times did you think your model took a hit?  How many times did you admit you would have been wrong?  That's how good your model really was.  The measure of <a href="/lw/if/your_strength_as_a_rationalist/"
  >your strength as a rationalist</a
  > is your ability to be more confused by fiction than by reality.</p
><p
>Unless, of course, I reversed the results again.  What do you think?</p
><p
>Do your thought processes at this point, where you <em
  >really don't</em
  > know the answer, feel different from the thought processes you used to rationalize either side of the &quot;known&quot; answer?</p
><p
>Daphna Baratz exposed college students to pairs of supposed findings, one true (&quot;In prosperous times people spend a larger portion of their income than during a recession&quot;) and one the truth's opposite.  In both sides of the pair, students rated the supposed finding as what they &quot;would have predicted&quot;.  Perfectly standard hindsight bias.</p
><p
>Which leads people to think they have no need for science, because they &quot;could have predicted&quot; that.</p
><p
>(Just as you would expect, right?)</p
><p
>Hindsight will lead us to systematically undervalue the surprisingness of scientific findings, especially the discoveries we <em
  >understand</em
  > - the ones that seem real to us, the ones we can retrofit into our models of the world.  If you understand neurology or physics and read news in that topic, then you probably underestimate the surprisingness of findings in those fields too.  This unfairly devalues the contribution of the researchers; and worse, will prevent you from noticing when you are seeing evidence that<a href="/lw/ii/conservation_of_expected_evidence/"
  >doesn't fit</a
  > what you <em
  >really</em
  > would have expected.</p
><p
>We need to make a conscious effort to be shocked <em
  >enough.</em
  ></p
><h1 id="fake-explanations"
>Fake Explanations</h1
><p
>Once upon a time, there was an instructor who taught physics students.  One day she called them into her class, and showed them a wide, square plate of metal, next to a hot radiator.  The students each put their hand on the plate, and found the side next to the radiator cool, and the distant side warm.  And the instructor said, <em
  >Why do you think this happens?</em
  >  Some students guessed convection of air currents, and others guessed strange metals in the plate.  They devised many creative explanations, none stooping so low as to say &quot;I don't know&quot; or &quot;<a href="/lw/if/your_strength_as_a_rationalist/"
  >This seems impossible.</a
  >&quot;</p
><p
>And the answer was that before the students entered the room, the instructor turned the plate around.</p
><p
>Consider the student who frantically stammers, &quot;Eh, maybe because of the heat conduction and so?&quot;  I ask: is this answer a <a href="/lw/i7/belief_as_attire/"
  >proper belief</a
  >?  The words are easily enough <a href="/lw/i6/professing_and_cheering/"
  >professed</a
  > - said in a loud, emphatic voice.  But do the words actually <a href="/lw/i4/belief_in_belief/"
  >control anticipation</a
  >?</p
><p
>Ponder that innocent little phrase, &quot;because of&quot;, which comes before &quot;heat conduction&quot;.  Ponder some of the <em
  >other</em
  > things we could put after it.  We could say, for example, &quot;Because of phlogiston&quot;, or &quot;Because of magic.&quot;</p
><p
>&quot;Magic!&quot; you cry.  &quot;That's not a <em
  >scientific</em
  > explanation!&quot;  Indeed, the phrases &quot;because of heat conduction&quot; and &quot;because of magic&quot; are readily recognized as belonging to different <em
  >literary genres.</em
  >  &quot;Heat conduction&quot; is something that Spock might say on <em
  >Star Trek</em
  >, whereas &quot;magic&quot; would be said by Giles in <em
  >Buffy the Vampire Slayer</em
  >.</p
><p
>However, as Bayesians, we take no notice of literary genres.  For us, the substance of a model is the control it exerts on anticipation.  If you say &quot;heat conduction&quot;, what experience does that lead you to <em
  >anticipate?</em
  >  Under normal circumstances, it leads you to anticipate that, if you put your hand on the side of the plate near the radiator, that side will feel warmer than the opposite side.  If &quot;because of heat conduction&quot; can also explain the radiator-adjacent side feeling <em
  >cooler,</em
  > then it can explain pretty much <em
  >anything.</em
  ></p
><p
><a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/"
  >And</a
  > <a href="/lw/i4/belief_in_belief/"
  >as</a
  > <a href="/lw/i5/bayesian_judo/"
  >we</a
  > <a href="/lw/i6/professing_and_cheering/"
  >all</a
  > <a href="/lw/i7/belief_as_attire/"
  >know</a
  > <a href="/lw/i8/religions_claim_to_be_nondisprovable/"
  >by</a
  > <a href="/lw/ia/focus_your_uncertainty/"
  >this</a
  > <a href="/lw/if/your_strength_as_a_rationalist/"
  >point</a
  > (<a href="/lw/ih/absence_of_evidence_is_evidence_of_absence/"
  >I</a
  > <a href="/lw/im/hindsight_devalues_science/"
  >do</a
  > <a href="/lw/ii/conservation_of_expected_evidence/"
  >hope</a
  >), if you are equally good at explaining any outcome, you have zero knowledge.  &quot;Because of heat conduction&quot;, used in such fashion, is a disguised hypothesis of maximum entropy.  It is anticipation-isomorphic to saying &quot;magic&quot;.  It feels like an explanation, but it's not.</p
><p
>Supposed that instead of guessing, we measured the heat of the metal plate at various points and various times.  Seeing a metal plate next to the radiator, we would ordinarily expect the point temperatures to satisfy an equilibrium of the diffusion equation with respect to the boundary conditions imposed by the environment.  You might not know the exact temperature of the first point measured, but after measuring the first points - I'm not physicist enough to know how many would be required - you could take an excellent guess at the rest.</p
><p
>A true master of the art of using numbers to constrain the anticipation of material phenomena - a &quot;physicist&quot; - would take some measurements and say, &quot;This plate was in equilibrium with the environment two and a half minutes ago, turned around, and is now approaching equilibrium again.&quot;</p
><p
>The deeper error of the students is not simply that they failed to constrain anticipation.  Their deeper error is that they thought they were doing physics.  They said the phrase &quot;because of&quot;, followed by the sort of words Spock might say on <em
  >Star Trek,</em
  > and thought they thereby entered the magisterium of science.</p
><p
>Not so.  They simply moved their magic from one literary genre to another.</p
><h1 id="guessing-the-teachers-password"
>Guessing the Teachers Password</h1
><p
>When I was young, I read popular physics books such as Richard Feynman's <em
  >QED: The <a href="/lw/hs/think_like_reality/"
    >Strange</a
    > Theory of Light and Matter.</em
  >  I knew that light was waves, sound was waves, matter was waves.  I took pride in my scientific literacy, when I was nine years old.</p
><p
>When I was older, and I began to read the <em
  >Feynman Lectures on Physics,</em
  > I ran across a gem called &quot;the wave equation&quot;.  I could follow the equation's derivation, but, <a href="http://www.math.utah.edu/~pa/math/polya.html"
  >looking back</a
  >, I couldn't see its truth at a glance.  So I thought about the wave equation for three days, on and off, until I saw that it was embarrassingly obvious.  And when I finally understood, I realized that the whole time I had accepted the honest assurance of physicists that light was waves, sound was waves, matter was waves, I had not had the vaguest idea of what the word &quot;wave&quot; meant to a physicist.</p
><p
>There is an instinctive tendency to think that if a physicist says &quot;light is made of waves&quot;, and the teacher says &quot;What is light made of?&quot;, and the student says &quot;Waves!&quot;, the student has made a true statement.  That's only fair, right?  We accept &quot;waves&quot; as a correct answer from the physicist; wouldn't it be unfair to reject it from the student?  Surely, the answer &quot;Waves!&quot; is either <em
  >true</em
  > or <em
  >false,</em
  > right?* *</p
><p
>Which is one more bad habit to <a href="/lw/i2/two_more_things_to_unlearn_from_school/"
  >unlearn from school</a
  >. Words do not have intrinsic definitions. If I hear the syllables &quot;bea-ver&quot; and think of a large rodent, that is a fact about my own state of mind, not a fact about the syllables &quot;bea-ver&quot;.  The sequence of syllables &quot;made of waves&quot; (or &quot;<a href="/lw/ip/fake_explanations/"
  >because of heat conduction</a
  >&quot;) is not a <em
  >hypothesis,</em
  > it is a pattern of vibrations traveling through the air, or ink on paper.  It can <em
  >associate</em
  > to a hypothesis in someone's mind, but it is not, of itself, right or wrong.  But in school, the teacher hands you a gold star for <em
  >saying</em
  > &quot;made of waves&quot;, which must be the correct answer because the teacher heard a physicist emit the same sound-vibrations.  Since verbal behavior (spoken or written) is what gets the gold star, students begin to think that verbal behavior has a truth-value.  After all, either light is made of waves, or it isn't, right?</p
><p
>And this leads into an even worse habit.  Suppose the teacher presents you with a <a href="/lw/ip/fake_explanations/"
  >confusing problem</a
  > involving a metal plate next to a radiator; the far side feels warmer than the side next to the radiator.  The teacher asks &quot;Why?&quot;  If you say &quot;I don't know&quot;, you have <em
  >no</em
  > chance of getting a gold star - it won't even count as class participation.  But, during the current semester, this teacher has used the phrases &quot;because of heat convection&quot;, &quot;because of heat conduction&quot;, and &quot;because of radiant heat&quot;.  One of these is probably what the teacher wants.  You say, &quot;Eh, maybe because of heat conduction?&quot;</p
><p
>This is not a* <em
  >hypothesis</em
  >about<em
  >the metal plate.  This is not even a <a href="/lw/i7/belief_as_attire/"
    >proper belief</a
    >.  It is an attempt to</em
  >guess the teacher's password.*</p
><p
>Even visualizing the symbols of the diffusion equation (the math governing heat conduction) doesn't mean you've formed a hypothesis <em
  >about</em
  > the metal plate.  This is not school; we are not testing your memory to see if you can write down the diffusion equation.  This is Bayescraft; we are scoring your anticipations of experience.  If you <em
  >use</em
  > the diffusion equation, by measuring a few points with a thermometer and then trying to predict what the thermometer will say on the next measurement, then it is definitely connected to experience.  Even if the student just visualizes something <em
  >flowing,</em
  > and therefore holds a match near the cooler side of the plate to try to measure where the heat goes, then this mental image of flowing-ness connects to experience; it controls anticipation.</p
><p
>If you aren't <em
  >using</em
  > the diffusion equation - putting in numbers and getting out results that control your anticipation of particular experiences - then the connection between map and territory is severed as though by a knife.  What remains <a href="/lw/i7/belief_as_attire/"
  >is not a belief</a
  >, but a verbal behavior.<br
   />In the school system, it's all about verbal behavior, whether written on paper or spoken aloud.  Verbal behavior gets you a gold star or a failing grade.  Part of unlearning this bad habit is becoming consciously aware of the difference between an explanation and a password.</p
><p
>Does this seem too harsh?  When you're faced by a confusing metal plate, can't &quot;Heat conduction?&quot; be a first step toward finding the answer?  Maybe, but only if you don't fall into the trap of thinking that you are looking for a password.  What if there is no teacher to tell you that you failed?  Then you may think that &quot;Light is wakalixes&quot; is a good explanation, that &quot;wakalixes&quot; is the correct password.  It happened to me when I was nine years old - not because I was stupid, but because this is what happens <em
  >by default.</em
  >* <em
  >This is how human beings think, unless they are trained</em
  >not* to fall into the trap.  Humanity stayed stuck in holes like this for thousands of years.</p
><p
>Maybe, if we drill students that <em
  >words don't count, only anticipation-controllers,</em
  > the student will <em
  >not</em
  > get stuck on &quot;Heat conduction? No?  Maybe heat convection?  That's not it either?&quot;  Maybe <em
  >then,</em
  > thinking the phrase &quot;Heat conduction&quot; will lead onto a genuinely helpful path, like:</p
><ul
><li
  >&quot;Heat conduction?&quot;</li
  ><li
  >But that's only a phrase - what does it mean?</li
  ><li
  >The diffusion equation?</li
  ><li
  >But those are only symbols - how do I apply them?</li
  ><li
  >What does applying the diffusion equation lead me to anticipate?</li
  ><li
  >It sure doesn't lead me to anticipate that the side of a metal plate farther away from a radiator would feel warmer.</li
  ><li
  >I <a href="/lw/if/your_strength_as_a_rationalist/"
    >notice</a
    > that I am <a href="/lw/im/hindsight_devalues_science/"
    >confused</a
    >.  Maybe the near side just <em
    >feels</em
    > cooler, because it's made of more insulative material and transfers less heat to my hand?  I'll try measuring the temperature...</li
  ><li
  >Okay, that wasn't it.  Can I try to verify whether the diffusion equation holds true of this metal plate, at all?  Is heat <em
    >flowing</em
    > the way it usually does, or is something else going on?</li
  ><li
  >I could hold a match to the plate and try to measure how heat spreads over time...</li
  ></ul
><p
>If we are <em
  >not</em
  > strict about &quot;Eh, maybe because of heat conduction?&quot; being a fake explanation, the student will very probably get stuck on some wakalixes-password.  <em
  >This happens by default, it happened to the whole human species for thousands of years.</em
  ></p
><p
><em
  >(This post is part of the sequence <a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions"
    >Mysterious Answers to Mysterious Questions</a
    >.)<br
     /></em
  ></p
><h1 id="science-as-attire"
>Science as Attire</h1
><p
>The preview for the <em
  >X-Men</em
  > movie has a voice-over saying:  &quot;In every human being... there is the genetic code... for mutation.&quot;  Apparently you can acquire all sorts of neat abilities by mutation.  The mutant Storm, for example, has the ability to throw lightning bolts. </p
><p
>I beg you, dear reader, to consider the biological machinery necessary to generate electricity; the biological adaptations necessary to avoid being harmed by electricity; and the cognitive circuitry required for finely tuned control of lightning bolts.  If we actually observed any organism acquiring these abilities <em
  >in one generation,</em
  > as the result of <em
  >mutation,</em
  > it would outright falsify the neo-Darwinian model of natural selection.  It would be worse than finding rabbit fossils in the pre-Cambrian.  If evolutionary theory could <em
  >actually</em
  > stretch to cover Storm, it would <a href="/lw/if/your_strength_as_a_rationalist/"
  >be able to explain anything</a
  >, and we all know what that would imply.</p
><p
>The <em
  >X-Men</em
  > comics use terms like &quot;evolution&quot;, &quot;mutation&quot;, and &quot;genetic code&quot;, purely to place themselves in what they conceive to be the <em
  >literary genre</em
  > of science.  The part that scares me is wondering how many people, especially in the media, understand science <em
  >only</em
  > as a literary genre.</p
><p
>I encounter people who very definitely <a href="/lw/i6/professing_and_cheering/"
  >believe in</a
  > evolution, who sneer at the folly of creationists.  And yet they have no idea of what the theory of evolutionary biology permits and prohibits.  They'll talk about &quot;the next step in the evolution of humanity&quot;, as if natural selection got here by following a plan.  Or even worse, they'll talk about something completely outside the domain of evolutionary biology, like an improved design for computer chips, or corporations splitting, or humans uploading themselves into computers, and they'll call <em
  >that</em
  > &quot;evolution&quot;.  If evolutionary biology could cover that, it could cover anything.</p
><p
>Probably an actual majority of the people who <em
  >believe in</em
  > evolution use the phrase &quot;<a href="/lw/ip/fake_explanations/"
  >because of evolution</a
  >&quot; because they want to be part of the scientific in-crowd - <a href="/lw/i7/belief_as_attire/"
  >belief as scientific attire</a
  >, like wearing a lab coat.  If the scientific in-crowd instead used the phrase &quot;because of intelligent design&quot;, they would just as cheerfully use that instead - it would make no difference to their anticipation-controllers.  Saying &quot;because of evolution&quot; instead of &quot;because of intelligent design&quot; does not, <em
  >for them,</em
  > prohibit Storm.  Its only purpose, for them, is to identify with a tribe.</p
><p
>I encounter people who are quite willing to entertain the notion of dumber-than-human Artificial Intelligence, or even mildly smarter-than-human Artificial Intelligence.  Introduce the notion of strongly superhuman Artificial Intelligence, and they'll suddenly decide it's &quot;<a href="/lw/io/is_molecular_nanotechnology_scientific/"
  >pseudoscience</a
  >&quot;. It's not that they think they have a theory of intelligence which lets them calculate a theoretical upper bound on the power of an optimization process.  Rather, they associate strongly superhuman AI to the <em
  >literary genre</em
  > of apocalyptic literature; whereas an AI running a small corporation associates to the literary genre of <em
  >Wired</em
  > magazine.  They aren't speaking from within a model of cognition.  They don't realize they <em
  >need</em
  > a model.  They don't realize that science is <em
  >about</em
  > models.  Their devastating critiques consist purely of <em
  >comparisons to apocalyptic literature</em
  >, rather than, say, known laws which prohibit such an outcome.  They understand science <em
  >only</em
  >as a literary genre, or in-group to belong to.  The <a href="/lw/i7/belief_as_attire/"
  >attire</a
  > doesn't look to them like a lab coat; this isn't the football team they're <a href="/lw/i6/professing_and_cheering/"
  >cheering</a
  > for.</p
><p
>Is there anything in science that you are <em
  >proud</em
  > of believing, and yet you do not use the belief professionally?  You had best ask yourself which future experiences your belief <em
  >prohibits</em
  > from happening to you.  That is the sum of what you have assimilated and made a true part of yourself.  Anything else is probably <a href="/lw/iq/guessing_the_teachers_password/"
  >passwords</a
  > or <a href="/lw/i7/belief_as_attire/"
  >attire</a
  >.</p
><h1 id="fake-causality"
>Fake Causality</h1
><p
>Phlogiston was the 18 century's answer to the Elemental Fire of the Greek alchemists.  Ignite wood, and let it burn.  What is the orangey-bright &quot;fire&quot; stuff?  Why does the wood transform into ash?  To both questions, the 18th-century chemists answered, &quot;phlogiston&quot;.</p
><p
>...and that was it, you see, that was their answer:  &quot;Phlogiston.&quot;</p
><p
>Phlogiston escaped from burning substances as visible fire.  As the phlogiston escaped, the burning substances lost phlogiston and so became ash, the &quot;true material&quot;.  Flames in enclosed containers went out because the air became saturated with phlogiston, and so could not hold any more.  Charcoal left little residue upon burning because it was nearly pure phlogiston.</p
><p
>Of course, one didn't use phlogiston theory to <em
  >predict</em
  > the outcome of a chemical transformation.  You looked at the result first, then you used phlogiston theory to <em
  >explain</em
  > it.  It's not that phlogiston theorists predicted a flame would extinguish in a closed container; rather they lit a flame in a container, watched it go out, and then said, &quot;The air must have become saturated with phlogiston.&quot;  You couldn't even use phlogiston theory to <a href="/lw/if/your_strength_as_a_rationalist/"
  >say what you ought <em
    >not</em
    > to see</a
  >; it could explain everything.</p
><p
>This was an earlier age of science.  For a long time, no one realized there was a problem.  <a href="/lw/ip/fake_explanations/"
  >Fake explanations</a
  > don't <em
  >feel</em
  > fake.  That's what makes them dangerous.</p
><p
>Modern research suggests that humans think about cause and effect using something like the directed acyclic graphs (DAGs) of Bayes nets.  Because it rained, the sidewalk is wet; because the sidewalk is wet, it is slippery:</p
><p
>[Rain] -&gt; [Sidewalk wet] -&gt; [Sidewalk slippery]</p
><p
>From this we can infer - or, in a Bayes net, rigorously calculate in probabilities - that when the sidewalk is slippery, it probably rained; but if we already know that the sidewalk is wet, learning that the sidewalk is slippery tells us nothing more about whether it rained.</p
><p
>Why is fire hot and bright when it burns?</p
><p
>[&quot;Phlogiston&quot;] -&gt; [Fire hot and bright]</p
><p
>It <em
  >feels</em
  > like an explanation.  It's <em
  >represented</em
  > using the same cognitive data format.  But the human mind does not automatically detect when a cause has an unconstraining arrow to its effect. Worse, thanks to <a href="/lw/il/hindsight_bias/"
  >hindsight bias</a
  >, it may feel like the cause <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/"
  >constrains</a
  > the effect, when it was merely* *<a href="/lw/ii/conservation_of_expected_evidence/"
  >fitted</a
  > to the effect.</p
><p
>Interestingly, <a href="http://books.google.com/books?id=k9VsqN24pNYC&amp;dq=&amp;pg=PP1&amp;ots=WR9UGWdOdd&amp;sig=w_Mrax-y4VVwZy5SQGySphNsKMc&amp;prev=http://www.google.com/search?hl=en&amp;safe=off&amp;q=pearl+intelligent+systems&amp;btnG=Search&amp;sa=X&amp;oi=print&amp;ct=title#PPA143,M1"
  >our modern understanding of probabilistic reasoning about causality</a
  > can describe precisely what the phlogiston theorists were doing wrong.  One of the primary inspirations for Bayesian networks was noticing the problem of double-counting evidence if inference resonates between an effect and a cause.  For example, let's say that I get a bit of unreliable information that the sidewalk is wet.  This should make me think it's more likely to be raining.  But, if it's more likely to be raining, doesn't that make it more likely that the sidewalk is wet?  And wouldn't <em
  >that</em
  > make it more likely that the sidewalk is slippery?  But if the sidewalk is slippery, it's probably wet; and then I should again raise my probability that it's raining...</p
><p
>Judea Pearl uses the metaphor of an algorithm for counting soldiers in a line.  Suppose you're in the line, and you see two soldiers next to you, one in front and one in back.  That's three soldiers.  So you ask the soldier next to you, &quot;How many soldiers do <em
  >you</em
  > see?&quot;  He looks around and says, &quot;Three&quot;.  So that's a total of six soldiers.  This, obviously, is <em
  >not</em
  > how to do it.</p
><p
>A smarter way is to ask the soldier in front of you, &quot;How many soldiers forward of you?&quot; and the soldier in back, &quot;How many soldiers backward of you?&quot;  The question &quot;How many soldiers forward?&quot; can be passed on as a message without confusion.  If I'm at the front of the line, I pass the message &quot;1 soldier forward&quot;, for myself.  The person directly in back of me gets the message &quot;1 soldier forward&quot;, and passes on the message &quot;2 soldiers forward&quot; to the soldier behind him.  At the same time, each soldier is also getting the message &quot;N soldiers backward&quot; from the soldier behind them, and passing it on as &quot;N+1 soldiers backward&quot; to the soldier in front of them.  How many soldiers in total?  Add the two numbers you receive, plus one for yourself: that is the total number of soldiers in line.</p
><p
>The key idea is that every soldier must <em
  >separately</em
  > track the two messages, the forward-message and backward-message, and add them together only at the end.  You never add any soldiers from the backward-message you receive to the forward-message you pass back.  Indeed, the total number of soldiers is never passed as a message - no one ever says it aloud.</p
><p
>An analogous principle operates in rigorous probabilistic reasoning about causality.  If you learn something about whether it's raining, from some source <em
  >other</em
  > than observing the sidewalk to be wet, this will send a forward-message from [rain] to [sidewalk wet] and raise our expectation of the sidewalk being wet.  If you observe the sidewalk to be wet, this sends a backward-message to our belief that it is raining, and this message propagates from [rain] to all neighboring nodes <em
  >except</em
  > the [sidewalk wet] node.  We count each piece of evidence exactly once; no update message ever &quot;bounces&quot; back and forth.  The exact algorithm may be found in Judea Pearl's classic &quot;<a href="http://books.google.com/books?id=k9VsqN24pNYC&amp;dq=&amp;pg=PP1&amp;ots=WR9UGWdOdd&amp;sig=w_Mrax-y4VVwZy5SQGySphNsKMc&amp;prev=http://www.google.com/search?hl=en&amp;safe=off&amp;q=pearl+intelligent+systems&amp;btnG=Search&amp;sa=X&amp;oi=print&amp;ct=title#PPA143,M1"
  >Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</a
  >&quot;.</p
><p
>So what went wrong in phlogiston theory?  When we observe that fire is hot, the [fire] node can send a backward-evidence to the [&quot;phlogiston&quot;] node, leading us to update our beliefs about phlogiston.  But if so, we can't count this as a successful forward-prediction of phlogiston theory.  The message should go in only one direction, and not bounce back.</p
><p
>Alas, human beings do not use a rigorous algorithm for updating belief networks.  We learn about parent nodes from observing children, and predict child nodes from beliefs about parents.  But we don't keep rigorously separate books for the backward-message and forward-message.  We just remember that phlogiston is hot, which <em
  >causes</em
  > fire to be hot.  So it seems like phlogiston theory predicts the hotness of fire.  Or, worse, it just feels like <em
  >phlogiston makes the fire hot.</em
  ></p
><p
>Until you notice that no <em
  >advance</em
  > predictions are being made, the non-constraining causal node is not labeled &quot;fake&quot;.  It's represented the same way as any other node in your belief network.  It feels like a fact, like all the other facts you know:  <em
  >Phlogiston makes the fire hot.</em
  ></p
><p
>A properly designed AI would notice the problem instantly.  This wouldn't even require special-purpose code, just correct bookkeeping of the belief network.  (Sadly, we humans can't rewrite our own code, the way a properly designed AI could.)</p
><p
>Speaking of &quot;<a href="/lw/im/hindsight_devalues_science/"
  >hindsight bias</a
  >&quot; is just the nontechnical way of saying that humans do not rigorously separate forward and backward messages, allowing forward messages to be contaminated by backward ones.</p
><p
>Those who long ago went down the path of phlogiston were not trying to be fools.  No scientist deliberately wants to get stuck in a blind alley.  Are there any fake explanations in <em
  >your</em
  > mind?   If there are, I guarantee they're not labeled &quot;fake explanation&quot;, so polling your thoughts for the &quot;fake&quot; keyword will not turn them up.</p
><p
>Thanks to <a href="/lw/im/hindsight_devalues_science/"
  >hindsight bias</a
  >, it's also not enough to check how well your theory &quot;predicts&quot; facts you already know.  You've got to predict for tomorrow, not yesterday.  It's the only way a messy human mind can be guaranteed of sending a pure forward message.</p
><h1 id="semantic-stopsigns"
>Semantic Stopsigns</h1
><p
><em
  >And the child asked:</em
  ></p
><p
>Q:  Where did this rock come from?<br
   />A:  I chipped it off the big boulder, at the center of the village.<br
   />Q:  Where did the boulder come from?<br
   />A:  It probably rolled off the huge mountain that towers over our village.<br
   />Q:  Where did the mountain come from?<br
   />A:  The same place as all stone: it is the bones of Ymir, the primordial giant.<br
   />Q:  Where did the primordial giant, Ymir, come from?<br
   />A:  From the great abyss, Ginnungagap.<br
   />Q:  Where did the great abyss, Ginnungagap, come from?<br
   />A:  Never ask that question.</p
><p
>Consider the seeming paradox of the First Cause.  Science has traced events back to the Big Bang, but why did the Big Bang happen?  It's all well and good to say that the zero of time begins at the Big Bang - that there is nothing before the Big Bang in the ordinary flow of minutes and hours.  But saying this presumes our physical law, which itself appears highly structured; it calls out for explanation.  Where did the physical laws come from?  You could say that we're all a computer simulation, but then the computer simulation is running on some other world's laws of physics - where did <em
  >those</em
  > laws of physics come from?</p
><p
>At this point, some people say, &quot;God!&quot;</p
><p
>What could possibly make anyone, even a highly religious person, think this even <em
  >helped</em
  > answer the paradox of the First Cause?  Why wouldn't you automatically ask, &quot;Where did God come from?&quot;  Saying &quot;God is uncaused&quot; or &quot;God created Himself&quot; leaves us in exactly the same position as &quot;Time began with the Big Bang.&quot;  We just ask why the whole metasystem exists in the first place, or why some events but not others are allowed to be uncaused.</p
><p
>My purpose here is not to discuss the seeming paradox of the First Cause, but to ask why anyone would think &quot;God!&quot; <em
  >could</em
  > resolve the paradox.  Saying &quot;God!&quot; is <a href="/lw/i6/professing_and_cheering/"
  >a way of belonging to a tribe</a
  >, which gives people a motive to say it as often as possible - some people even say it for questions like &quot;Why did this hurricane strike New Orleans?&quot;  Even so, you'd hope people would notice that on the <em
  >particular</em
  > puzzle of the First Cause, saying &quot;God!&quot; doesn't help.  It doesn't make the paradox seem any less paradoxical <em
  >even if true.</em
  >  How could anyone <em
  >not</em
  > notice this?</p
><p
>Jonathan Wallace suggested that &quot;God!&quot; functions as a <em
  >semantic stopsign</em
  > - that it isn't a propositional assertion, so much as a cognitive traffic signal: do not think past this point.  Saying &quot;God!&quot; doesn't so much resolve the paradox, as put up a cognitive traffic signal to halt the obvious continuation of the question-and-answer chain.</p
><p
>Of course <em
  >you'd</em
  > never do that, being a good and proper atheist, right?  But &quot;God!&quot; isn't the <em
  >only</em
  > semantic stopsign, just the obvious first example.</p
><p
>The transhuman technologies - molecular nanotechnology, advanced biotech, genetech, Artificial Intelligence, et cetera - pose tough policy questions.  What kind of role, if any, should a government take in supervising a parent's choice of genes for their child?  Could parents deliberately choose genes for schizophrenia?  If enhancing a child's intelligence is expensive, should governments help ensure access, to prevent the emergence of a cognitive elite?  You can propose various institutions to answer these policy questions - for example, that private charities should provide financial aid for intelligence enhancement - but the obvious next question is, &quot;Will this institution be effective?&quot;  If we rely on product liability lawsuits to prevent corporations from building harmful nanotech, will that really <em
  >work?</em
  ></p
><p
>I know someone whose answer to every one of these questions is &quot;Liberal democracy!&quot;  That's it.  That's his answer.  If you ask the obvious question of &quot;How well have liberal democracies performed, historically, on problems this tricky?&quot; or &quot;What if liberal democracy does something stupid?&quot; then you're an autocrat, or libertopian, or otherwise a very very bad person.  No one is allowed to question democracy.</p
><p
>I once called this kind of thinking &quot;the divine right of democracy&quot;.  But it is more precise to say that &quot;Democracy!&quot; functioned for him as a semantic stopsign.  If anyone had said to him &quot;Turn it over to the Coca-Cola corporation!&quot;, he would have asked the obvious next questions:  &quot;Why?  What will the Coca-Cola corporation do about it?  Why should we trust them?  Have they done well in the past on equally tricky problems?&quot;</p
><p
>Or suppose that someone says &quot;Mexican-Americans are plotting to remove all the oxygen in Earth's atmosphere.&quot;  You'd probably ask, &quot;Why would they do <em
  >that?</em
  >  Don't Mexican-Americans have to breathe too?  Do Mexican-Americans even function as a unified conspiracy?&quot;  If you don't ask these obvious next questions when someone says, &quot;Corporations are plotting to remove Earth's oxygen,&quot; then &quot;Corporations!&quot; functions for you as a semantic stopsign.</p
><p
>Be careful here not to create a new generic counterargument against things you don't like - &quot;Oh, it's just a stopsign!&quot;  No word is a stopsign of itself; the question is whether a word has that effect on a particular person.  Having <a href="/lw/hp/feeling_rational/"
  >strong emotions</a
  > about something doesn't qualify it as a stopsign.  I'm not exactly fond of terrorists or fearful of private property; that doesn't mean &quot;Terrorists!&quot; or &quot;Capitalism!&quot; are cognitive traffic signals unto me.  (The word &quot;intelligence&quot; did once have that effect on me, though no longer.)  What distinguishes a semantic stopsign is <em
  >failure to consider the obvious next question.</em
  ></p
><p
><em
  >(This post is part of the sequence <a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions"
    >Mysterious Answers to Mysterious Questions</a
    >.)</em
  ></p
><h1 id="mysterious-answers-to-mysterious-questions"
>Mysterious Answers to Mysterious Questions</h1
><p
>Imagine looking at your hand, and knowing nothing of cells, nothing of biochemistry, nothing of DNA. You've learned some anatomy from dissection, so you know your hand contains muscles; but you don't know why muscles move instead of lying there like clay. Your hand is just... stuff... and for some reason it moves under your direction. Is this not magic?</p
><blockquote
><p
  >&quot;The animal body does not act as a thermodynamic engine ... consciousness teaches every individual that they are, to some extent, subject to the direction of his will. It appears therefore that animated creatures have the power of immediately applying to certain moving particles of matter within their bodies, forces by which the motions of these particles are directed to produce derived mechanical effects... The influence of animal or vegetable life on matter is infinitely beyond the range of any scientific inquiry hitherto entered on. Its power of directing the motions of moving particles, in the demonstrated daily miracle of our human free-will, and in the growth of generation after generation of plants from a single seed, are infinitely different from any possible result of the fortuitous concurrence of atoms... Modern biologists were coming once more to the acceptance of something and that was a vital principle.&quot;<br
     />        -- Lord Kelvin</p
  ></blockquote
><p
>This was the theory of <em
  >vitalism</em
  >; that the mysterious difference between living matter and non-living matter was explained by an <em
  >elan vital</em
  > or <em
  >vis vitalis</em
  >.  Elan vital infused living matter and caused it to move as consciously directed. Elan vital participated in chemical transformations which no mere non-living particles could undergo - Wรถhler's later synthesis of urea, a component of urine, was a major blow to the vitalistic theory because it showed that mere<em
  >chemistry</em
  > could duplicate a product of biology.</p
><p
>Calling &quot;elan vital&quot; an explanation, even a <a href="http://www.overcomingbias.com/2007/08/fake-explanatio.html"
  >fake explanation</a
  > like <a href="http://www.overcomingbias.com/2007/08/fake-causality.html"
  >phlogiston</a
  >, is probably giving it too much credit.  It functioned primarily as a <a href="http://www.overcomingbias.com/2007/08/semantic-stopsi.html"
  >curiosity-stopper</a
  >.  You said &quot;Why?&quot; and the answer was &quot;Elan vital!&quot;</p
><p
>When you say &quot;Elan vital!&quot;, it <em
  >feels</em
  > like you know why your hand moves.  You have a little <a href="http://www.overcomingbias.com/2007/08/fake-causality.html"
  >causal diagram</a
  > in your head that says [&quot;Elan vital!&quot;] -&gt; [hand moves].  But actually you know nothing you didn't know before. You don't know, say, whether your hand will generate heat or absorb heat, unless you have observed the fact already; if not, you won't be able to predict it in advance.  Your curiosity feels sated, but it hasn't been fed.  Since you can say &quot;Why? Elan vital!&quot; to any possible observation, it is equally good at explaining all outcomes, a disguised hypothesis of maximum entropy, etcetera.</p
><p
>But the greater lesson lies in the vitalists' reverence for the elan vital, their eagerness to pronounce it a mystery beyond all science. Meeting the great dragon Unknown, the vitalists did not draw their swords to do battle, but bowed their necks in submission. They <a href="http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html"
  >took pride</a
  > in their ignorance, made biology into a <em
  >sacred</em
  > mystery, and thereby became loath to <a href="http://yudkowsky.net/virtues/"
  >relinquish their ignorance</a
  > when evidence came knocking.</p
><p
>The Secret of Life was <em
  >infinitely beyond the reach of science!</em
  > Not just a <em
  >little</em
  > beyond, mind you, but <em
  >infinitely</em
  > beyond! Lord Kelvin sure did get a tremendous emotional kick out of <em
  >not knowing something.</em
  ></p
><p
>But ignorance exists in the map, not in the territory.  If I am ignorant about a phenomenon, that is a fact about my own state of mind, not a fact about the phenomenon itself. A phenomenon can <em
  >seem</em
  > mysterious to some particular person.  There are no phenomena which are mysterious of themselves. To worship a phenomenon because it seems so wonderfully mysterious, is to worship your own ignorance.</p
><p
>Vitalism shared with phlogiston the error of <em
  >encapsulating the mystery as a substance.</em
  > Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called &quot;phlogiston&quot;. Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called &quot;elan vital&quot;. Neither answer helped <a href="http://www.overcomingbias.com/2007/08/focus-your-unce.html"
  >concentrate the model's probability density</a
  > - make some outcomes easier to explain than others. The &quot;explanation&quot; just wrapped up the question as a small, hard, opaque black ball.</p
><p
>In a comedy written by Moliere, a physician explains the power of a soporific by saying that it contains a &quot;dormitive potency&quot;.  Same principle.  It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.</p
><p
>But the deeper failure is supposing that an <em
  >answer</em
  > can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances.</p
><p
>This is the ultimate and fully general explanation for why, again and again in humanity's history, people are shocked to discover that an incredibly mysterious question has a non-mysterious answer.  Mystery is a property of questions, not answers.</p
><p
>Therefore I call theories such as vitalism <em
  >mysterious answers to mysterious questions</em
  >.</p
><p
>These are the signs of mysterious answers to mysterious questions:</p
><ul
><li
  >First, the explanation acts as a <a href="http://www.overcomingbias.com/2007/08/semantic-stopsi.html"
    >curiosity-stopper</a
    > rather than an <a href="http://www.overcomingbias.com/2007/07/making-beliefs-.html"
    >anticipation-controller</a
    >.</li
  ><li
  >Second, the hypothesis has no moving parts - the model is not a specific complex mechanism, but a blankly solid substance or force. The mysterious substance or mysterious force may be said to be here or there, to <a href="http://www.overcomingbias.com/2007/08/fake-causality.html"
    >cause</a
    >this or that; but the reason why the mysterious force behaves thus is wrapped in a blank unity.</li
  ><li
  >Third, those who proffer the explanation <a href="http://yudkowsky.net/virtues/"
    >cherish their ignorance</a
    >; they speak proudly of how the phenomenon defeats ordinary science or is unlike merely mundane phenomena.</li
  ><li
  >Fourth, <em
    >even after the answer is given, the phenomenon is still a mystery</em
    >and possesses the same quality of wonderful inexplicability that it had at the start.</li
  ></ul
><h1 id="the-futility-of-emergence"
>The Futility of Emergence</h1
><p
>The failures of <a href="/lw/is/fake_causality/"
  >phlogiston</a
  > and <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >vitalism</a
  > are <a href="/lw/im/hindsight_devalues_science/"
  >historical</a
  > <a href="/lw/il/hindsight_bias/"
  >hindsight</a
  >. Dare I step out on a limb, and name some <em
  >current</em
  > theory which I deem analogously flawed?</p
><p
>I name <em
  >emergence</em
  >or <em
  >emergent phenomena</em
  > - usually defined as the study of systems whose high-level behaviors arise or &quot;emerge&quot; from the interaction of many low-level elements.  (<a href="http://en.wikipedia.org/wiki/Emergence"
  >Wikipedia</a
  >:  &quot;The way complex systems and patterns arise out of a multiplicity of relatively simple interactions&quot;.)  Taken literally, that description fits every phenomenon in our universe above the level of individual quarks, which is part of the problem.  Imagine pointing to a market crash and saying &quot;It's not a quark!&quot;  Does that feel like an explanation?  No?  Then neither should saying &quot;It's an emergent phenomenon!&quot;</p
><p
>It's the noun &quot;emergence&quot; that I protest, rather than the verb &quot;emerges from&quot;.  There's nothing wrong with saying &quot;X emerges from Y&quot;, where Y is some specific, detailed model with internal moving parts.  &quot;Arises from&quot; is another legitimate phrase that means exactly the same thing:  Gravity arises from the curvature of spacetime, according to the specific mathematical model of General Relativity. Chemistry arises from interactions between atoms, according to the specific model of quantum electrodynamics.</p
><p
>Now suppose I should say that gravity is explained by &quot;arisence&quot; or that chemistry is an &quot;arising phenomenon&quot;, and claim that as my explanation.</p
><p
>The phrase &quot;emerges from&quot; is acceptable, just like &quot;arises from&quot; or &quot;is caused by&quot; are acceptable, if the phrase precedes some specific model to be judged on its own merits.</p
><p
>However, this is <em
  >not</em
  > the way &quot;emergence&quot; is commonly used. &quot;Emergence&quot; is commonly used as an explanation in its own right.</p
><p
>I have lost track of how many times I have heard people say, &quot;Intelligence is an emergent phenomenon!&quot; as if that explained intelligence. This usage fits all the checklist items for a <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >mysterious answer to a mysterious question</a
  >. What do you know, after you have said that intelligence is &quot;emergent&quot;?  You can make no new predictions.  You do not know anything about the behavior of real-world minds that you did not know before.  It feels like you believe a new fact, but you don't anticipate any different outcomes. Your curiosity feels sated, but it has not been fed.  The hypothesis has no moving parts - there's no detailed internal model to manipulate.  Those who proffer the hypothesis of &quot;emergence&quot; confess their ignorance of the internals, and take pride in it; they contrast the science of &quot;emergence&quot; to other sciences merely mundane.</p
><p
>And even after the answer of &quot;Why? Emergence!&quot; is given, <em
  >the phenomenon is still a mystery</em
  > and possesses the same sacred impenetrability it had at the start.</p
><p
>A fun exercise is to eliminate the adjective &quot;emergent&quot; from any sentence in which it appears, and see if the sentence says anything different:</p
><ul
><li
  ><em
    >Before:</em
    >  Human intelligence is an emergent product of neurons firing.</li
  ><li
  ><p
    ><em
      >After:</em
      >  Human intelligence is a product of neurons firing.</p
    ></li
  ><li
  ><em
    >Before:</em
    >  The behavior of the ant colony is the emergent outcome of the interactions of many individual ants.</li
  ><li
  ><em
    >After:</em
    >  The behavior of the ant colony is the outcome of the interactions of many individual ants.</li
  ><li
  ><p
    ><em
      >Even better:</em
      > A colony is made of ants. We can successfully predict some aspects of colony behavior using models that include only individual ants, without any global colony variables, showing that we understand how those colony behaviors arise from ant behaviors.</p
    ></li
  ></ul
><p
>Another fun exercise is to replace the word &quot;emergent&quot; with the old <a href="/lw/iq/guessing_the_teachers_password/"
  >word</a
  >, the <a href="/lw/ip/fake_explanations/"
  >explanation</a
  > that people had to use before emergence was invented:</p
><ul
><li
  ><em
    >Before:</em
    >  Life is an emergent phenomenon.</li
  ><li
  ><p
    ><em
      >After:</em
      >  Life is a magical phenomenon.</p
    ></li
  ><li
  ><em
    >Before:</em
    >  Human intelligence is an emergent product of neurons firing.</li
  ><li
  ><p
    ><em
      >After:</em
      >  Human intelligence is a magical product of neurons firing.</p
    ></li
  ></ul
><p
>Does not each statement convey exactly the same amount of knowledge about the phenomenon's behavior? Does not each hypothesis <a href="/lw/if/your_strength_as_a_rationalist/"
  >fit exactly the same set of outcomes</a
  >?</p
><p
>&quot;Emergence&quot; has become very popular, just as saying &quot;magic&quot; used to be very popular. &quot;Emergence&quot; has the same deep appeal to human psychology, for the same reason. &quot;Emergence&quot; is such a wonderfully easy explanation, and it feels good to say it; it gives you a <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >sacred mystery</a
  > to worship. Emergence is popular <em
  >because</em
  > it is the junk food of curiosity. You can explain anything using emergence, and so people do just that; for it feels so wonderful to explain things. Humans are still humans, even if they've taken a few science classes in college. Once they find a way to escape the <a href="http://yudkowsky.net/virtues/"
  >shackles</a
  > of settled science, they get up to the same shenanigans as their ancestors, <a href="/lw/i7/belief_as_attire/"
  >dressed up in the literary genre of &quot;science&quot;</a
  > but still the same species psychology.</p
><h1 id="say-not-complexity"
>Say Not &quot;Complexity&quot;</h1
><p
>Once upon a time...</p
><p
>This is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice.  I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn't know yet if he could learn to think about AI.</p
><p
>I had asked Marcello to say how he thought an AI might discover how to solve a Rubik's Cube.  Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them.  How would an AI <em
  >invent for itself</em
  > the concept of an &quot;operator&quot;, or &quot;macro&quot;, which is the key to solving the Rubik's Cube?</p
><p
>At some point in this discussion, Marcello said:  &quot;Well, I think the AI needs complexity to do X, and complexity to do Y -&quot;</p
><p
>And I said, &quot;Don't say '<em
  >complexity'.</em
  >&quot;</p
><p
>Marcello said, &quot;Why not?&quot;</p
><p
>I said, &quot;Complexity should never be a goal in itself.  You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder.&quot;  (I was thinking of all the people whom I had heard advocating that the Internet would &quot;wake up&quot; and become an AI when it became &quot;sufficiently complex&quot;.)</p
><p
>And Marcello said, &quot;But there's got to be <em
  >some</em
  > amount of complexity that does it.&quot;</p
><p
>I closed my eyes briefly, and tried to think of how to explain it all in words.  To me, saying 'complexity' simply <em
  >felt</em
  > like the wrong move in the AI dance.  No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion.  We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience...</p
><p
>I said, &quot;Did you read <a href="http://yudkowsky.net/bayes/technical.html"
  >A Technical Explanation of Technical Explanation</a
  >?&quot;</p
><p
>&quot;Yes,&quot; said Marcello.</p
><p
>&quot;Okay,&quot; I said, &quot;saying 'complexity' doesn't concentrate your probability mass.&quot;</p
><p
>&quot;Oh,&quot; Marcello said, &quot;like '<a href="/lw/iv/the_futility_of_emergence/"
  >emergence</a
  >'.  Huh.  So... now I've got to think about how X might actually happen...&quot;</p
><p
>That was when I thought to myself, &quot;<em
  >Maybe <strong
    >this</strong
    > one is teachable.</em
  >&quot;</p
><p
>Complexity is not a useless concept.  It has mathematical definitions attached to it, such as Kolmogorov complexity, and Vapnik-Chervonenkis complexity.  Even on an intuitive level, complexity is often worth thinking about - you have to judge the complexity of a hypothesis and decide if it's &quot;too complicated&quot; given the supporting evidence, or look at a design and try to make it simpler.</p
><p
>But concepts are not useful or useless of themselves.  Only <em
  >usages</em
  > are correct or incorrect.  In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing.  It is an extremely common misstep, at least in my field.  You can join a discussion on Artificial General Intelligence and watch people doing the same thing, left and right, over and over again - constantly skipping over things they don't understand, without realizing that's what they're doing.</p
><p
>In an eyeblink it happens: putting a <a href="/lw/is/fake_causality/"
  >non-controlling causal node</a
  > behind something mysterious, a causal node that <a href="/lw/ip/fake_explanations/"
  >feels like an explanation</a
  > but isn't.  The mistake takes place below the level of words.  It requires no special character flaw; it is how human beings think <a href="/lw/iq/guessing_the_teachers_password/"
  >by default</a
  >, since the ancient times.</p
><p
>What you must avoid is <em
  >skipping over the mysterious part;</em
  > you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts - &quot;complexity&quot;, for example.  But the essential mistake is that <em
  >skip-over,</em
  > regardless of what causal node goes behind it.  The skip-over is not a thought, but a microthought.  You have to pay close attention to catch yourself at it.  And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning.  You have to <em
  >feel</em
  > which parts of your map are still blank, and more importantly, pay attention to that feeling.</p
><p
>I suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness.  You'll get more kudos for a seemingly complete model that includes some &quot;<a href="/lw/iv/the_futility_of_emergence/"
  >emergent phenomena</a
  >&quot;, versus an explicitly incomplete map where the label says &quot;I got no clue how this part works&quot; or &quot;then a miracle occurs&quot;.  A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?  And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important.  That's the price you sometimes pay, for entering into terra incognita and trying to solve problems <em
  >incrementally.</em
  >  But that makes it even <em
  >more</em
  > important to <em
  >know</em
  > when you aren't finished yet.  Mostly, people don't dare to enter terra incognita at all, for the deadly fear of wasting their time.* *</p
><p
>And if you're working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to <a href="/lw/id/you_can_face_reality/"
  >admit to yourself</a
  > that you don't know how to build an AI yet, and your current life-plans will come crashing down in ruins around your ears.  But perhaps I am <a href="/lw/hz/correspondence_bias/"
  >over-explaining</a
  >, since skip-over happens <a href="/lw/iq/guessing_the_teachers_password/"
  >by default</a
  > in humans; if you're looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.</p
><p
>Marcello and I developed a convention in our AI work: when we ran into something we didn't understand, which was often, we would say &quot;magic&quot; - as in, &quot;X magically does Y&quot; - to remind ourselves that <em
  >here was an unsolved problem, a gap in our understanding.</em
  >  It is far better to say &quot;magic&quot;, than &quot;complexity&quot; or &quot;emergence&quot;; the latter <a href="/lw/iq/guessing_the_teachers_password/"
  >words</a
  > create an illusion of understanding.  Wiser to say &quot;magic&quot;, and leave yourself a placeholder, a reminder of work you will have to do later.</p
><h1 id="positive-bias-look-into-the-dark"
>Positive Bias: Look Into the Dark</h1
><p
>I am teaching a class, and I write upon the blackboard three numbers:  2-4-6.  &quot;I am thinking of a rule,&quot; I say, &quot;which governs sequences of three numbers.  The sequence 2-4-6, as it so happens, obeys this rule.  Each of you will find, on your desk, a pile of index cards.  Write down a sequence of three numbers on a card, and I'll mark it &quot;Yes&quot; for fits the rule, or &quot;No&quot; for not fitting the rule.  Then you can write down another set of three numbers and ask whether it fits again, and so on.  When you're confident that you know the rule, write down the rule on a card.  You can test as many triplets as you like.&quot;</p
><p
>Here's the record of one student's guesses:</p
><pre
><code
  >4, 6, 2            No  
4, 6, 8            Yes  
10, 12, 14         Yes
</code
  ></pre
><p
>At this point the student wrote down his guess at the rule.  What do <em
  >you</em
  > think the rule is?  Would you have wanted to test another triplet, and if so, what would it be?  Take a moment to think before continuing.</p
><p
>The challenge above is based on a classic experiment due to Peter Wason, the 2-4-6 task.  Although subjects given this task typically expressed high confidence in their guesses, only 21% of the subjects successfully guessed the experimenter's real rule, and replications since then have continued to show success rates of around 20%.</p
><p
>The study was called &quot;On the failure to eliminate hypotheses in a conceptual task&quot; (<em
  >Quarterly Journal of Experimental Psychology,</em
  > 12: 129-140, 1960).  Subjects who attempt the 2-4-6 task usually try to generate <em
  >positive</em
  > examples, rather than <em
  >negative</em
  > examples - they apply the hypothetical rule to generate a representative instance, and see if it is labeled &quot;Yes&quot;.</p
><p
>Thus, someone who forms the hypothesis &quot;numbers increasing by two&quot; will test the triplet 8-10-12, hear that it fits, and confidently announce the rule.  Someone who forms the hypothesis X-2X-3X will test the triplet 3-6-9, discover that it fits, and then announce that rule.</p
><p
>In every case the actual rule is the same: the three numbers must be in ascending order.</p
><p
>But to discover this, you would have to generate triplets that <em
  >shouldn't</em
  > fit, such as 20-23-26, and see if they are labeled &quot;No&quot;.  Which people tend not to do, in this experiment.  In some cases, subjects devise, &quot;test&quot;, and announce rules far more complicated than the actual answer.</p
><p
>This cognitive phenomenon is usually lumped in with &quot;confirmation bias&quot;.  However, it seems to me that the phenomenon of trying to test <em
  >positive</em
  > rather than <em
  >negative</em
  > examples, ought to be distinguished from the phenomenon of trying to preserve the belief you started with.  &quot;Positive bias&quot; is sometimes used as a synonym for &quot;confirmation bias&quot;, and fits this particular flaw much better.</p
><p
>It once seemed that <a href="/lw/is/fake_causality/"
  >phlogiston theory</a
  > could explain a flame going out in an enclosed box (the air became saturated with phlogiston and no more could be released), but phlogiston theory could just as well have explained the flame <em
  >not</em
  > going out.  To notice this, you have to search for negative examples instead of positive examples, look into zero instead of one; which goes against the grain of what experiment has shown to be human instinct.</p
><p
>For by instinct, we human beings only live in half the world.</p
><p
>One may be lectured on positive bias for days, and yet overlook it in-the-moment.  Positive bias is not something we do as a matter of logic, or even as a matter of emotional attachment.  The 2-4-6 task is &quot;cold&quot;, logical, not affectively &quot;hot&quot;.  And yet the mistake is sub-verbal, on the level of imagery, of instinctive reactions.  Because the problem doesn't arise from following a deliberate rule that says &quot;Only think about positive examples&quot;, it can't be solved just by knowing verbally that &quot;We ought to think about both positive and negative examples.&quot;  Which example automatically pops into your head?  You have to learn, wordlessly, to zag instead of zig.  You have to learn to flinch toward the zero, instead of away from it.</p
><p
>I have been writing for quite some time now on the notion that <a href="/lw/if/your_strength_as_a_rationalist/"
  >the strength of a hypothesis is what it <em
    >can't</em
    > explain, not what it <em
    >can</em
    ></a
  > - if you are equally good at explaining any outcome, you have zero knowledge.  So to spot an explanation that isn't helpful, it's not enough to think of what it does explain very well - you also have to search for results it <em
  >couldn't</em
  > explain, and this is the true strength of the theory.</p
><p
>So I said all this, and then yesterday, <a href="/lw/iv/the_futility_of_emergence/"
  >I challenged the usefulness of &quot;emergence&quot; as a concept</a
  >.  One commenter cited superconductivity and ferromagnetism as examples of emergence.  I replied that non-superconductivity and non-ferromagnetism were also examples of emergence, which was the problem.  But be it far from me to criticize the commenter!  Despite having read extensively on &quot;confirmation bias&quot;, I didn't spot the &quot;gotcha&quot; in the 2-4-6 task the first time I read about it.  It's a subverbal blink-reaction that has to be retrained.  I'm still working on it myself.</p
><p
>So much of a rationalist's skill is below the level of words.  It makes for challenging work in trying to convey the Art through blog posts.  People will agree with you, but then, in the next sentence, do something subdeliberative that goes in the opposite direction.  Not that I'm complaining!  A major reason I'm posting here is to observe what my words <em
  >haven't</em
  > conveyed.</p
><p
>Are you searching for positive examples of positive bias right now, or sparing a fraction of your search on what positive bias should lead you to <em
  >not</em
  > see?  Did you look toward light or darkness?</p
><h1 id="my-wild-and-reckless-youth"
>My Wild and Reckless Youth</h1
><p
>It is said that parents do all the things they tell their children not to do, which is how they know not to do them.</p
><p
>Long ago, in the unthinkably distant past, I was a devoted Traditional Rationalist, conceiving myself skilled according to that kind, yet I knew not the Way of Bayes.  When the young Eliezer was confronted with a mysterious-seeming question, the precepts of Traditional Rationality did not stop him from devising a <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >Mysterious Answer</a
  >.  It is, by far, the most embarrassing mistake I made in my life, and I still wince to think of it.</p
><p
>What was my mysterious answer to a mysterious question?  This I will not describe, for it would be a long tale and complicated.  I was young, and a mere Traditional Rationalist who knew not the teachings of Tversky and Kahneman.  I knew about Occam's Razor, but not the <a href="http://en.wikipedia.org/wiki/Conjunction_fallacy"
  >conjunction fallacy</a
  >.  I thought I could get away with thinking complicated thoughts myself, in the literary style of the complicated thoughts I read in science books, not realizing that correct complexity is only possible when every step is pinned down overwhelmingly.  Today, one of the chief pieces of advice I give to aspiring young rationalists is &quot;Do not attempt long chains of reasoning or complicated plans.&quot;</p
><p
>Nothing more than this need be said:  Even after I invented my &quot;answer&quot;, the phenomenon was <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >still a mystery</a
  > unto me, and possessed the same quality of wondrous impenetrability that it had at the start.</p
><p
>Make no <a href="/lw/hz/correspondence_bias/"
  >mistake</a
  >, that younger Eliezer was not stupid.  All the errors of which the young Eliezer was guilty, are still being made today by respected scientists in respected journals.  It would have taken a subtler skill to protect him, than ever he was taught as a Traditional Rationalist.</p
><p
>Indeed, the young Eliezer diligently and painstakingly followed the injunctions of Traditional Rationality in the course of going astray.</p
><p
>As a Traditional Rationalist, the young Eliezer was careful to ensure that his Mysterious Answer made a bold prediction of future experience.  Namely, I expected future neurologists to discover that neurons were exploiting quantum gravity, a la Sir Roger Penrose.  This required neurons to maintain a certain degree of quantum coherence, which was something you could look for, and find or not find.  Either you observe that or you don't, right?</p
><p
>But my hypothesis made no <em
  >retrospective</em
  > predictions.  According to Traditional Science, retrospective predictions don't count - so why bother making them?  To a Bayesian, on the other hand, if a hypothesis does not <em
  >today</em
  > have a favorable likelihood ratio over &quot;I don't know&quot;, it raises the question of why you <em
  >today</em
  > believe anything more complicated than &quot;I don't know&quot;.  But I knew not the Way of Bayes, so I was not thinking about likelihood ratios or focusing probability density.  I had Made a Falsifiable Prediction; was this not the Law?</p
><p
>As a Traditional Rationalist, the young Eliezer was careful not to believe in magic, mysticism, carbon chauvinism, or anything of that sort.  I proudly <a href="/lw/i6/professing_and_cheering/"
  >professed</a
  > of my Mysterious Answer, &quot;It is just physics like all the rest of physics!&quot;  As if you could save magic from being a cognitive isomorph of magic, by <a href="/lw/ir/science_as_attire/"
  >calling</a
  > it quantum gravity.  But I knew not the Way of Bayes, and did not see the <a href="/lw/ip/fake_explanations/"
  >level</a
  > on which my idea was isomorphic to magic.  I gave my <em
  >allegiance</em
  > to physics, but this did not save me; what does probability theory know of allegiances?  I avoided everything that Traditional Rationality told me was forbidden, but what was left was still magic.</p
><p
>Beyond a doubt, my allegiance to Traditional Rationality helped me get out of the hole I dug myself into.  If I hadn't been a Traditional Rationalist, I would have been <em
  >completely</em
  > screwed.  But Traditional Rationality still wasn't enough to get it <em
  >right.</em
  >  It just led me into different mistakes than the ones it had explicitly forbidden.</p
><p
>When I think about how my younger self very carefully followed the rules of Traditional Rationality in the course of getting the answer <em
  >wrong,</em
  > it sheds light on the question of why people who call themselves &quot;rationalists&quot; <a href="/lw/he/knowing_about_biases_can_hurt_people/"
  >do not rule the world</a
  >.  You need <em
  >one whole hell of a lot</em
  > of rationality before it does anything but lead you into new and interesting mistakes.* *</p
><p
>Traditional Rationality is taught as an art, rather than a science; you read the biography of famous physicists describing the lessons life taught them, and you try to do what they tell you to do.  But you haven't lived their lives, and half of what they're trying to describe is an instinct that has been trained into them.</p
><p
>The way Traditional Rationality is designed, it would have been acceptable for me to spend 30 years on my silly idea, so long as I succeeded in falsifying it eventually, and was honest with myself about what my theory predicted, and accepted the disproof when it arrived, et cetera.  This is enough to let the Ratchet of Science click forward, but it's a little harsh on the people who waste 30 years of their lives.  Traditional Rationality is a walk, not a dance.  It's designed to get you to the truth <em
  >eventually</em
  >, and gives you all too much time to smell the flowers along the way.</p
><p
>Traditional Rationalists can agree to disagree.  Traditional Rationality doesn't have the <em
  >ideal</em
  > that thinking is an exact art in which there is only one correct probability estimate given the evidence.  In Traditional Rationality, you're allowed to guess, and then test your guess.  But experience has taught me that if you don't <em
  >know,</em
  > and you guess, you'll end up being wrong.</p
><p
>The Way of Bayes is also an imprecise art, at least the way I'm holding forth upon it.  These blog posts are still fumbling attempts to put into words lessons that would be better taught by experience.  But at least there's <em
  >underlying</em
  > math, plus experimental evidence from cognitive psychology on how humans actually think.  Maybe that will be enough to cross the stratospherically high threshold required for a discipline that lets you actually get it right, instead of just constraining you into interesting new mistakes.</p
><h1 id="failing-to-learn-from-history"
>Failing to Learn from History</h1
><p
>Once upon a time, in <a href="/lw/iy/my_wild_and_reckless_youth/"
  >my wild and reckless youth</a
  >, when I knew not the Way of Bayes, I gave a <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >Mysterious Answer</a
  > to a mysterious-seeming question.  Many failures occurred in sequence, but one mistake stands out as most critical:  My younger self did not realize that <em
  >solving a mystery should make it feel less confusing.</em
  >  I was trying to explain a Mysterious Phenomenon - which to me meant providing a cause for it, fitting it into an integrated model of reality.  Why should this make the phenomenon less Mysterious, when that is its nature?  I was trying to <em
  >explain</em
  > the Mysterious Phenomenon, not render it (by some impossible alchemy) into a mundane phenomenon, a phenomenon that wouldn't even call out for an unusual explanation in the first place.<br
   />As a Traditional Rationalist, I knew the historical tales of astrologers and astronomy, of alchemists and chemistry, of vitalists and biology.  But the Mysterious Phenomenon was not like this.  It was something <em
  >new,</em
  >something stranger, something more difficult, something that ordinary science had failed to explain for centuries -</p
><ul
><li
  >as if stars and matter and life had not been mysteries for hundreds of years and thousands of years, from the dawn of human thought right up until science finally solved them -</li
  ></ul
><p
>We learn about astronomy and chemistry and biology in school, and it seems to us that these matters have <em
  >always been</em
  > the proper realm of science, that they have <em
  >never been</em
  > mysterious.  When science dares to challenge a new Great Puzzle, the children of that generation are skeptical, for they have never seen science explain something that <em
  >feels</em
  > mysterious to them.  Science is only good for explaining <em
  >scientific</em
  > subjects, like stars and matter and life.</p
><p
>I thought the lesson of history was that astrologers and alchemists and vitalists had an <a href="/lw/hz/correspondence_bias/"
  >innate character flaw</a
  >, a tendency toward mysterianism, which led them to come up with mysterious explanations for non-mysterious subjects.  But surely, if a phenomenon really <em
  >was</em
  > very weird, a weird explanation might be in order?</p
><p
>It was only afterward, when I began to see the mundane structure inside the mystery, that I realized whose shoes I was standing in.  Only then did I realize how reasonable vitalism had seemed<em
  >at the time</em
  >, how <em
  >surprising</em
  > and <em
  >embarrassing</em
  > had been the universe's reply of, &quot;Life is mundane, and does not need a weird explanation.&quot;</p
><p
>We read history but we don't <em
  >live</em
  > it, we don't <em
  >experience</em
  > it.  If only I had <em
  >personally</em
  > postulated astrological mysteries and then discovered Newtonian mechanics, postulated alchemical mysteries and then discovered chemistry, postulated vitalistic mysteries and then discovered biology.  I would have thought of my Mysterious Answer and said to myself:  <em
  >No way am I falling for that again.</em
  ></p
><h1 id="making-history-available"
>Making History Available</h1
><p
>There is a habit of thought which I call the<em
  >logical fallacy of generalization from fictional evidence,</em
  > which deserves a blog post in its own right, one of these days.  Journalists who, for example, talk about the <em
  >Terminator</em
  > movies in a report on AI, do not usually treat <em
  >Terminator</em
  > as a prophecy or fixed truth.  But the movie is recalled - is <a href="http://en.wikipedia.org/wiki/Availability_heuristic"
  >available</a
  > - as if it were an illustrative historical case.  As if the journalist had seen it happen on some other planet, so that it might well happen here.  More on this in Section 6 of <a href="http://singinst.org/Biases.pdf"
  >this paper</a
  >.</p
><p
>There is an inverse error to generalizing from fictional evidence: failing to be sufficiently moved by <em
  >historical</em
  > evidence.  The trouble with generalizing from fictional evidence is that it is fiction - it never actually happened.  It's not drawn from the same distribution as this, our real universe; <a href="http://www.overcomingbias.com/2007/07/tell-your-anti-.html"
  >fiction differs from reality in systematic ways</a
  >.  But history <em
  >has</em
  > happened, and <em
  >should</em
  > be available.</p
><p
>In our ancestral environment, there were no movies; what you saw with your own eyes was true.  Is it any wonder that fictions we see in lifelike moving pictures have too great an impact on us?  Conversely, things that <em
  >really happened,</em
  > we encounter as ink on paper; they happened, but we never <em
  >saw</em
  > them happen.  We don't remember them happening to us.</p
><p
>The inverse error is to treat history as mere story, process it with the same part of your mind that handles the novels you read.  You may say with your lips that it is &quot;truth&quot;, rather than &quot;fiction&quot;, but that doesn't mean you are being moved as much as you should be.  Many biases involve being insufficiently moved by <a href="/lw/hw/scope_insensitivity/"
  >dry, abstract information</a
  >.</p
><p
>Once upon a time, I gave a <a href="/lw/iu/mysterious_answers_to_mysterious_questions/"
  >Mysterious Answer</a
  > to a mysterious question, not realizing that I was making exactly the same mistake as astrologers devising mystical explanations for the stars, or alchemists devising magical properties of matter, or vitalists postulating an opaque &quot;elan vital&quot; to explain all of biology.</p
><p
>When <a href="/lw/iz/failing_to_learn_from_history/"
  >I finally realized whose shoes I was standing in</a
  >, there was a sudden shock of unexpected connection with the past.  I realized that the invention and destruction of vitalism - which I had only read about in books - had <em
  >actually happened to real people,</em
  > who experienced it much the same way I experienced the invention and destruction of my own mysterious answer.  And I also realized that if I had actually <em
  >experienced</em
  > the past - if I had lived through past scientific revolutions myself, rather than reading about them in history books - I probably would <em
  >not</em
  > have made the same mistake again.  I would not have come up with <em
  >another</em
  > mysterious answer; the first thousand lessons would have hammered home the moral.</p
><p
>So (I thought), to feel sufficiently the force of history, I should try to approximate the thoughts of an Eliezer who <em
  >had</em
  > lived through history - I should try to think as if everything I read about in history books, had actually happened to me.  (With appropriate reweighting for the availability bias of history books - I should remember being a thousand peasants for every ruler.)  I should immerse myself in history, imagine <em
  >living</em
  > through eras I only saw as ink on paper.</p
><p
>Why should I remember the Wright Brothers' first flight?  I was not there.  But as a rationalist, could I dare to <em
  >not</em
  > remember, when the event actually happened?  Is there so much difference between seeing an event through your eyes - which is actually a causal chain involving reflected photons, not a direct connection - and seeing an event through a history book?  Photons and history books both descend by causal chains from the event itself.</p
><p
>I had to overcome the false amnesia of being born at a particular time.  I had to recall - make <a href="http://en.wikipedia.org/wiki/Availability_heuristic"
  >available</a
  > - <em
  >all</em
  > the memories, not just the memories which, by mere coincidence, belonged to myself and my own era.</p
><p
>The Earth became older, of a sudden.</p
><p
>To my former memory, the United States had always existed - there was never a time when there was no United States.  I had not remembered, until that time, how the Roman Empire rose, and brought peace and order, and lasted through so many centuries, until I forgot that things had ever been otherwise; and yet the Empire fell, and barbarians overran my city, and the learning that I had possessed was lost.  The modern world became more fragile to my eyes; it was not the first modern world.</p
><p
>So many mistakes, made over and over and <em
  >over</em
  > again, because I did not remember making them, in every era I never lived...</p
><p
>And to think, people sometimes wonder if overcoming bias is important.</p
><p
>Don't you remember how many times your biases have killed you?  You don't?  I've noticed that sudden amnesia often follows a fatal mistake.  But take it from me, it happened.  I remember; I wasn't there.</p
><p
>So the next time you doubt the strangeness of the future, remember how you were born in a hunter-gatherer tribe ten thousand years ago, when no one knew of Science at all.  Remember how you were shocked, to the depths of your being, when Science explained the great and terrible sacred mysteries that you once revered so highly.  Remember how you once believed that you could fly by eating the right mushrooms, and then you accepted with disappointment that you would never fly, and then you flew.  Remember how you had always thought that slavery was right and proper, and then you changed your mind.  <a href="/lw/il/hindsight_bias/"
  >Don't imagine how you <em
    >could</em
    > have predicted the change</a
  >, for that is amnesia.  <em
  >Remember</em
  > that, in fact, you did not guess.  Remember how, century after century, the world changed in ways you did not guess.</p
><p
>Maybe then you will be less shocked by what happens next.</p
><h1 id="explainworshipignore"
>Explain/Worship/Ignore?</h1
><p
>As our tribe wanders through the grasslands, searching for fruit trees and prey, it happens every now and then that water pours down from the sky.</p
><p
>&quot;Why does water sometimes fall from the sky?&quot; I ask the bearded wise man of our tribe.</p
><p
>He thinks for a moment, this question having never occurred to him before, and then says, &quot;From time to time, the sky spirits battle, and when they do, their blood drips from the sky.&quot;</p
><p
>&quot;Where do the sky spirits come from?&quot; I ask.</p
><p
>His voice drops to a whisper.  &quot;From the before time.  From the long long ago.&quot;</p
><p
>When it rains, and you don't know why, you have several options.  First, you could simply not ask why - not follow up on the question, or never think of the question in the first place.  This is the Ignore command, which the bearded wise man originally selected.  Second, you could try to devise some sort of explanation, the Explain command, as the bearded man did in response to your first question.  Third, you could enjoy the sensation of mysteriousness - the Worship command.</p
><p
>Now, as you are bound to notice from this story, each time you select Explain, the best-case scenario is that you get an explanation, such as &quot;sky spirits&quot;.  But then this explanation itself is subject to the same dilemma - Explain, Worship, or Ignore?  Each time you hit Explain, science grinds for a while, returns an explanation, and then another dialog box pops up.  As good rationalists, we feel duty-bound to keep hitting Explain, but it seems like a road that has no end.</p
><p
>You hit Explain for life, and get chemistry; you hit Explain for chemistry, and get atoms; you hit Explain for atoms, and get electrons and nuclei; you hit Explain for nuclei, and get quantum chromodynamics and quarks; you hit Explain for how the quarks got there, and get back the Big Bang...</p
><p
>We can hit Explain for the Big Bang, and wait while science grinds through its process, and maybe someday it will return a perfectly good explanation.  But then that will just bring up another dialog box.  So, if we continue long enough, we must come to a <em
  >special</em
  > dialog box, a <em
  >new</em
  > option, an Explanation That Needs No Explanation, a place where the chain ends - and this, maybe, is the only explanation worth knowing.</p
><p
>There - I just hit Worship.</p
><p
>Never forget that there are many more ways to worship something than lighting candles around an altar.</p
><p
>If I'd said, &quot;Huh, that does seem paradoxical.  I wonder how the apparent paradox is resolved?&quot; then I would have hit Explain, which does sometimes take a while to produce an answer.</p
><p
>And if the whole issue seems to you unimportant, or irrelevant, or if you'd rather put off thinking about it until tomorrow, than you have hit Ignore.</p
><p
>Select your option wisely.</p
><h1 id="science-as-curiosity-stopper"
>&quot;Science&quot; as Curiosity-Stopper</h1
><p
>Imagine that I, in full view of live television cameras, raised my hands and chanted <em
  >abracadabra</em
  > and caused a brilliant light to be born, flaring in empty space beyond my outstretched hands.  Imagine that I committed this act of blatant, unmistakeable sorcery under the full supervision of James Randi and all skeptical armies. Most people, I think, would be <em
  >fairly curious</em
  > as to what was going on.</p
><p
>But now suppose instead that I don't go on television.  I do not wish to share the power, nor the truth behind it. I want to keep my sorcery secret.  And yet I also want to cast my spells whenever and wherever I please. I want to cast my brilliant flare of light so that I can read a book on the train - without anyone becoming curious.  Is there a spell that stops curiosity?</p
><p
>Yes indeed!  Whenever anyone asks &quot;How did you do that?&quot;, I just say &quot;Science!&quot;</p
><p
>It's <a href="/lw/ip/fake_explanations/"
  >not a real explanation</a
  >, so much as a <a href="/lw/it/semantic_stopsigns/"
  >curiosity-stopper</a
  >. It doesn't tell you whether the light will brighten or fade, change color in hue or saturation, and it certainly doesn't tell you how to make a similar light yourself. You don't actually <em
  >know</em
  > anything more than you knew before I said the <a href="/lw/iq/guessing_the_teachers_password/"
  >magic word</a
  >. But you turn away, satisfied that nothing unusual is going on.</p
><p
>Better yet, the same trick works with a standard light switch.</p
><p
>Flip a switch and a light bulb turns on.  Why?</p
><p
>In school, one is taught that the <a href="/lw/iq/guessing_the_teachers_password/"
  >password</a
  > to the light bulb is &quot;Electricity!&quot;  By now, I hope, you're wary of marking the light bulb &quot;understood&quot; on such a basis.  Does saying &quot;Electricity!&quot; let you do calculations that will control your anticipation of experience?  There is, at the least, a great deal more to learn.  (Physicists should ignore this paragraph and substitute a problem in <a href="/lw/kr/an_alien_god/"
  >evolutionary theory</a
  >, where the substance of the theory is again in calculations that few people know how to perform.)</p
><p
>If you thought the light bulb was <em
  >scientifically inexplicable,</em
  > it would seize the <em
  >entirety</em
  > of your attention.  You would drop whatever else you were doing, and focus on that light bulb.</p
><p
>But what does the phrase &quot;scientifically explicable&quot; mean?  It means that someone <em
  >else</em
  > knows how the light bulb works.  When you are told the light bulb is &quot;scientifically explicable&quot;, you don't know more than you knew earlier; you don't know whether the light bulb will brighten or fade.  But because someone <em
  >else</em
  > knows, it devalues the knowledge in your eyes.  You become less curious.</p
><p
>Since this is an econblog, someone out there is bound to say, &quot;If the light bulb were unknown to science, you could gain fame and fortune by investigating it.&quot;  But I'm not talking about greed.  I'm not talking about career ambition.  I'm talking about the raw emotion of curiosity - the feeling of being intrigued.  Why should <em
  >your</em
  > curiosity be diminished because someone <em
  >else,</em
  > not you, knows how the light bulb works?  Is this not spite?  It's not enough for <em
  >you</em
  > to know; other people must also be ignorant, or you won't be happy?</p
><p
>There are goods that knowledge may serve besides curiosity, such as the social utility of technology.  For these instrumental goods, it matters whether some other entity in local space already knows.  But for my own curiosity, why should it matter? </p
><p
>Besides, consider the consequences if you permit &quot;Someone else knows the answer&quot; to function as a curiosity-stopper.  One day you walk into your living room and see a giant green elephant, seemingly hovering in midair, surrounded by an aura of silver light.</p
><p
>&quot;What the heck?&quot; you say.</p
><p
>And a voice comes from above the elephant, saying, &quot;SOMEONE ELSE ALREADY KNOWS WHY THIS ELEPHANT IS HERE.&quot;</p
><p
>&quot;Oh,&quot; you say, &quot;in that case, never mind,&quot; and walk on to the kitchen.</p
><p
>I don't know the grand unified theory for this universe's laws of physics.  I also don't know much about human anatomy with the exception of the brain.  I couldn't point out on my body where my kidneys are, and I can't recall offhand what my liver does.  (I am not proud of this.  Alas, with all the math I need to study, I'm not likely to learn anatomy anytime soon.)</p
><p
>Should I, so far as <em
  >curiosity</em
  > is concerned, be more intrigued by my ignorance of the ultimate laws of physics, than the fact that I don't know much about what goes on inside my own body?</p
><p
>If I raised my hands and cast a light spell, you would be intrigued.  Should you be any <em
  >less</em
  > intrigued by the very fact that I raised my hands?  When you raise your arm and wave a hand around, this act of will is coordinated by (among other brain areas) your cerebellum.  I bet you don't know how the cerebellum works.  <em
  >I</em
  > know a little - though only the gross details, not enough to perform calculations... but so what?  What does that matter, if <em
  >you</em
  > don't know?  Why should there be a double standard of curiosity for sorcery and hand motions?</p
><p
>Look at yourself in the mirror.  Do you know what you're looking at?  Do you know what looks out from behind your eyes?  Do you know what you are?  Some of that answer, Science knows, and some of it Science does not.  But why should that distinction matter to your curiosity, if <em
  >you</em
  > don't know?</p
><p
>Do you know how your knees work?  Do you know how your shoes were made?  Do you know why your computer monitor glows?  Do you know why water is wet?</p
><p
>The world around you is full of puzzles.  Prioritize, if you must.  But do not complain that cruel Science has emptied the world of mystery.  With reasoning such as that, I could get you to overlook an elephant in your living room.</p
><h1 id="applause-lights"
>Applause Lights</h1
><p
>At the Singularity Summit 2007, one of the speakers called for democratic, multinational development of AI.  So I stepped up to the microphone and asked:</p
><blockquote
><p
  >Suppose that a group of democratic republics form a consortium to develop AI, and there's a lot of politicking during the process - some interest groups have unusually large influence, others get shafted - in other words, the result looks just like the products of modern democracies.  Alternatively, suppose a group of rebel nerds develops an AI in their basement, and instructs the AI to poll everyone in the world - dropping cellphones to anyone who doesn't have them - and do whatever the majority says.  Which of these do you think is more &quot;democratic&quot;, and would you feel safe with either?</p
  ></blockquote
><p
>I wanted to find out whether he believed in the pragmatic adequacy of the democratic political process, or if he believed in the moral rightness of voting.  But the speaker replied:</p
><blockquote
><p
  >The first scenario sounds like an editorial in Reason magazine, and the second sounds like a Hollywood movie plot.</p
  ></blockquote
><p
>Confused, I asked:</p
><blockquote
><p
  >Then what kind of democratic process <em
    >did</em
    > you have in mind?</p
  ></blockquote
><p
>The speaker replied:</p
><blockquote
><p
  >Something like the Human Genome Project - that was an internationally sponsored research project.</p
  ></blockquote
><p
>I asked:</p
><blockquote
><p
  >How would different interest groups resolve their conflicts in a structure like the Human Genome Project?</p
  ></blockquote
><p
>And the speaker said:</p
><blockquote
><p
  >I don't know.</p
  ></blockquote
><p
>This exchange puts me in mind of a <a href="http://www.time.com/time/magazine/article/0,9171,954853,00.html"
  >quote</a
  > ( which I failed to Google found by Jeff Grey and Miguel) from some dictator or other, who was asked if he had any intentions to move his pet state toward democracy: &gt; We believe we are already within a democratic system.  Some factors &gt; are still missing, like the expression of the people's will.</p
><p
>The substance of a democracy is the specific mechanism that resolves policy conflicts.  If all groups had the same preferred policies, there would be no need for democracy - we would automatically cooperate.  The resolution process can be a direct majority vote, or an elected legislature, or even a voter-sensitive behavior of an AI, but it has to be <em
  >something. </em
  >What does it <em
  >mean</em
  > to call for a &quot;democratic&quot; solution if you don't have a conflict-resolution mechanism in mind?</p
><p
>I think it means that you have said the word &quot;democracy&quot;, so the audience is supposed to cheer.  It's not so much a propositional* *statement, as the equivalent of the &quot;Applause&quot; light that tells a studio audience when to clap.</p
><p
>This case is remarkable only in that I mistook the applause light for a policy suggestion, with subsequent embarrassment for all.  Most applause lights are much more blatant, and can be detected by a simple reversal test.  For example, suppose someone says:</p
><blockquote
><p
  >We need to balance the risks and opportunities of AI.</p
  ></blockquote
><p
>If you reverse this statement, you get:</p
><blockquote
><p
  >We shouldn't balance the risks and opportunities of AI.</p
  ></blockquote
><p
>Since the reversal sounds <em
  >ab</em
  >normal, the unreversed statement is probably normal, implying it does not convey new information.  There are plenty of legitimate reasons for uttering a sentence that would be uninformative in isolation.  &quot;We need to balance the risks and opportunities of AI&quot; can introduce a discussion topic; it can emphasize the importance of a specific proposal for balancing; it can criticize an unbalanced proposal.  Linking to a normal assertion can convey new information to a bounded rationalist - the link itself may not be obvious.  But if <em
  >no</em
  > specifics follow, the sentence is probably an applause light.</p
><p
>I am tempted to give a talk sometime that consists of <em
  >nothing but</em
  > applause lights, and see how long it takes for the audience to start laughing:</p
><blockquote
><p
  >I am here to propose to you today that we need to balance the risks and opportunities of advanced Artificial Intelligence.  We should avoid the risks and, insofar as it is possible, realize the opportunities.  We should not needlessly confront entirely unnecessary dangers.  To achieve these goals, we must plan wisely and rationally.  We should not act in fear and panic, or give in to technophobia; but neither should we act in blind enthusiasm.  We should respect the interests of all parties with a stake in the Singularity.  We must try to ensure that the benefits of advanced technologies accrue to as many individuals as possible, rather than being restricted to a few.  We must try to avoid, as much as possible, violent conflicts using these technologies; and we must prevent massive destructive capability from falling into the hands of individuals.  We should think through these issues before, not after, it is too late to do anything about them...</p
  ></blockquote
><h1 id="chaotic-inversion"
>Chaotic Inversion</h1
><p
>I was recently having a conversation with some friends on the topic of hour-by-hour productivity and willpower maintenance - something I've struggled with my whole life.</p
><p
>I can <a href="/lw/un/on_doing_the_impossible/"
  >avoid running away from a hard problem the first time I see it</a
  > (perseverance on a timescale of seconds), and I can stick to the same problem for years; but to keep working on a timescale of <em
  >hours</em
  > is a constant battle for me.  It goes without saying that I've already read reams and reams of advice; and the most help I got from it was realizing that a sizable fraction other creative professionals had the same problem, and couldn't beat it either, no matter how reasonable* *all the advice sounds.</p
><p
>&quot;What do you do when you can't work?&quot; my friends asked me.  (Conversation probably not accurate, this is a very loose gist.)</p
><p
>And I replied that I usually browse random websites, or watch a short video.</p
><p
>&quot;Well,&quot; they said, &quot;if you know you can't work for a while, you should watch a movie or something.&quot;</p
><p
>&quot;Unfortunately,&quot; I replied, &quot;I have to do something whose time comes in short units, like browsing the Web or watching short videos, because I might become able to work again at any time, and I can't predict when -&quot;</p
><p
>And then I stopped, because I'd just had a revelation.</p
><p
>I'd always thought of my workcycle as something <em
  >chaotic,</em
  > something <em
  >unpredictable.</em
  >  I never used those words, but that was the way I <em
  >treated</em
  > it.</p
><p
>But here my friends seemed to be implying - what a strange thought - that <em
  >other</em
  > people could predict when they would become able to work again, and structure their time accordingly.</p
><p
>And it occurred to me for the first time that I might have been committing that damned old chestnut the <a href="/lw/oi/mind_projection_fallacy/"
  >Mind Projection Fallacy</a
  >, right out there in my ordinary everyday life instead of high abstraction.</p
><p
>Maybe it wasn't that my productivity was<em
  >unusually</em
  ><em
  >chaotic;</em
  > maybe I was just<em
  >unusually</em
  ><em
  >stupid</em
  > with respect to predicting it.</p
><p
>That's what inverted stupidity looks like - chaos.  Something hard to handle, hard to grasp, hard to guess, something you can't do anything with.  It's not just an idiom for high abstract things like Artificial Intelligence.  It can apply in ordinary life too.</p
><p
>And the reason we don't think of the alternative explanation &quot;I'm stupid&quot;, is <em
  >not</em
  > - I suspect - that we think so highly of ourselves.  It's just that we don't think of ourselves at all.  We just see <a href="/lw/oi/mind_projection_fallacy/"
  >a chaotic feature of the environment</a
  >.</p
><p
>So now it's occurred to me that my productivity problem may not be chaos, but my own stupidity.</p
><p
>And that may or may not help anything.  It certainly doesn't fix the problem right away.  Saying &quot;I'm ignorant&quot; doesn't make you knowledgeable.</p
><p
>But it is, at least, a different path than saying &quot;it's too chaotic&quot;.</p
>
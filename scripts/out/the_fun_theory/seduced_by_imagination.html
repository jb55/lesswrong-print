<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_xp" class="content clear"><div class="md">
        
  <div><p><em>(This post is part of the <a href="../lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a>.)</em><br> <strong>Previously in series</strong>:&#xA0; <a href="/lw/xo/justified_expectation_of_pleasant_surprises/">Justified Expectation of Pleasant Surprises</a></p>
<p>"Vagueness" usually has a <a href="/lw/ic/the_virtue_of_narrowness/">bad name</a> in rationality - connoting skipped steps in reasoning and attempts to <a href="/lw/i4/belief_in_belief/">avoid falsification</a>.&#xA0; But a rational view of the Future <em>should </em>be vague, because the information we have about the Future is <a href="/lw/vz/the_weak_inside_view/">weak</a>.&#xA0; Yesterday I argued that <a href="/lw/xo/justified_expectation_of_pleasant_surprises/">justified vague hopes</a> might also be better <em>hedonically</em> than specific foreknowledge - the power of pleasant surprises.</p>
<p>But there's also a more severe warning that I must deliver:&#xA0; It's not a good idea to dwell much <em>on</em> imagined pleasant futures, since you can't actually dwell <em>in </em>them.&#xA0; It can suck the emotional energy out of your actual, current, ongoing life.</p>
<p>Epistemically, we know the Past much more <em>specifically </em>than the Future.&#xA0; But also on <em>emotional</em> grounds, it's probably wiser to compare yourself to Earth's past, so you can see how far we've come, and how much better we're doing.&#xA0; Rather than comparing your life to an imagined future, and thinking about how awful you've got it Now.</p>
<p>Having set out to explain George Orwell's observation that <a href="http://www.orwell.ru/library/articles/socialists/english/e_fun">no one can seem to write about a Utopia where anyone would want to live</a> - having laid out the various Laws of Fun that I believe are being <em>violated </em>in these dreary Heavens - I am now explaining why you <em>shouldn't</em> apply this knowledge to invent an extremely seductive Utopia and write stories set there.&#xA0; That may suck out your soul like an emotional vacuum cleaner.</p>
<p><a id="more"></a></p>
<p>I briefly remarked on this phenomenon <a href="/lw/xl/eutopia_is_scary/">earlier</a>, and someone said, "Define 'suck out your soul'."&#xA0; Well, it's mainly a tactile thing: you can practically <em>feel </em>the pulling sensation, if your dreams wander too far into the Future.&#xA0; It's like something out of H. P. Lovecraft:&#xA0; <em>The Call of Eutopia.</em>&#xA0; A professional hazard of having to stare out into vistas that <em>humans were meant to gaze upon</em>, and knowing <em>a little too much</em> about the lighter side of existence.</p>
<p>But for the record, I will now lay out the components of "soul-sucking", that you may recognize the bright abyss and steer your thoughts away:</p>
<ul>
<li>Your emotional energy drains away into your imagination of Paradise:
<ul>
<li>You find yourself thinking of it more and more often.</li>
<li>The actual challenges of your current existence start to seem less interesting, less compelling; you think of them less and less.</li>
<li>Comparing everything to your imagined perfect world heightens your annoyances and diminishes your pleasures.</li>
</ul>
</li>
<li>You go into an <a href="/lw/lm/affective_death_spirals/">affective death spiral</a> around your imagined scenario; you're reluctant to admit anything bad could happen on your assumptions, and you find more and more nice things to say.</li>
<li>Your mind begins to forget the difference between fiction and real life:
<ul>
<li>You originally made many arbitrary or iffy choices in constructing your scenario.&#xA0; You forget that the Future is actually more unpredictable than this, and that you made your choices using limited foresight and merely human optimizing ability.</li>
<li>You forget that, in real life, at least <em>some </em>of your amazing good ideas are <em>guaranteed </em>not to work as well as they do in your imagination.</li>
<li>You start wanting the <em>exact specific</em> Paradise you imagined, and worrying about the disappointment if you don't get that <em>exact</em> thing.</li>
</ul>
</li>
</ul>
<p>Hope can be a dangerous thing.&#xA0; And when you've just been hit hard - at the moment when you most <em>need</em> hope to keep you going - that's also when the real world seems most painful, and the world of imagination becomes most seductive.</p>
<p>It's a balancing act, I think.&#xA0; One needs enough Fun Theory to truly and legitimately justify hope in the future.&#xA0; But not a detailed vision so seductive that it steals emotional energy from the real life and real challenge of creating that future.&#xA0; You need "a light at the end of the secular rationalist tunnel" as <a href="http://www.overcomingbias.com/2009/01/better-and-better.html#comment-144803018">Roko put it</a>, but you don't want people to drift away from their bodies into that light.</p>
<p>So <em>how much</em> light is that, exactly?&#xA0; Ah, now that's the issue.</p>
<p>I'll start with a simple and genuine question:&#xA0; Is what I've already said, enough?</p>
<p>Is knowing the abstract fun theory and being able to pinpoint the exact flaws in previous flawed Utopias, enough to make you look forward to tomorrow?&#xA0; Is it enough to inspire a stronger will to live?&#xA0; To dispel worries about a long dark tea-time of the soul?&#xA0; Does it now seem - on a gut level - that if we could really build an AI and really shape it, the resulting future would be very much worth staying alive to see?</p></div></div></div>
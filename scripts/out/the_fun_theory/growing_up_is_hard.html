<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_xd" class="content clear"><div class="md">
        
  <div><p><em>(This post is part of the <a href="/lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a>.)</em><a href="/lw/xb/free_to_optimize/"></a></p>
<p>Terrence Deacon's <em>The Symbolic Species</em> is the best book I've ever read on the evolution of intelligence.&#xA0; Deacon somewhat overreaches when he tries to theorize about what our X-factor <em>is;</em> but his exposition of its <em>evolution</em> is first-class.</p>
<p>Deacon makes an excellent case - he has quite persuaded me - that the increased <em>relative </em>size of our frontal cortex, compared to other hominids, is of overwhelming importance in understanding the evolutionary development of humanity.&#xA0; It's not just a question of increased computing capacity, like adding extra processors onto a cluster; it's a question of what kind of signals dominate, in the brain.</p>
<p>People with Williams Syndrome (caused by deletion of a certain region on chromosome 7) are hypersocial, ultra-gregarious; as children they fail to show a normal fear of adult strangers.&#xA0; WSers are cognitively impaired on most dimensions, but their verbal abilities are spared or even exaggerated; they often speak early, with complex sentences and large vocabulary, and excellent verbal recall, even if they can never learn to do basic arithmetic.</p>
<p>Deacon makes a case for some Williams Syndrome symptoms coming from a frontal cortex that is <em>relatively too large</em> for a human, with the result that prefrontal signals - including certain social emotions - dominate more than they should.</p>
<p><a id="more"></a></p>
<p>"Both postmortem analysis and MRI analysis have revealed brains with a reduction of the entire posterior cerebral cortex, but a sparing of the cerebellum and frontal lobes, and perhaps even an exaggeration of cerebellar size," says Deacon.</p>
<p>Williams Syndrome's deficits can be explained by the shrunken posterior cortex - they can't solve simple problems involving shapes, because the parietal cortex, which handles shape-processing, is diminished.&#xA0; But the frontal cortex is not actually <em>enlarged;</em> it is simply <em>spared.</em>&#xA0; So where do WSers' <em>augmented </em>verbal abilities come from?</p>
<p>Perhaps because the signals sent out by the frontal cortex, saying "pay attention to this verbal stuff!", <em>win out</em> over signals coming from the shrunken sections of the brain.&#xA0; So the verbal abilities get lots of exercise - and other abilities don't.</p>
<p>Similarly with the hyper-gregarious nature of WSers; the signal saying "Pay attention to this person!", originating in the frontal areas where social processing gets done, dominates the emotional landscape.</p>
<p>And Williams Syndrome is not frontal <em>enlargement,</em> remember; it's just frontal <em>sparing</em> in an otherwise shrunken brain, which increases the <em>relative</em> force of frontal signals...</p>
<p>...beyond the <em>narrow </em>parameters within which a human brain is adapted to work.</p>
<p>I mention this because you might look at the history of human evolution, and think to yourself, "Hm... to get from a chimpanzee to a human... you enlarge the frontal cortex... so if we enlarge it <em>even further...</em>"</p>
<p>The road to +Human is not that simple.</p>
<p>Hominid brains have been tested billions of times over through thousands of generations.&#xA0; But you shouldn't reason <a href="/lw/ti/qualitative_strategies_of_friendliness/">qualitatively</a>, "Testing creates 'robustness', so now the human brain must be 'extremely robust'."&#xA0; Sure, we can expect the human brain to be robust against <em>some</em> insults, like the loss of a single neuron.&#xA0; But testing in an evolutionary paradigm only creates robustness over the domain tested.&#xA0; Yes, <em>sometimes </em>you get robustness beyond that, because sometimes evolution finds simple solutions that prove to generalize -</p>
<p>But people do go crazy.&#xA0; Not colloquial crazy, actual crazy.&#xA0; Some ordinary young man in college suddenly decides that everyone around them is <em>staring </em>at them because they're part of the <em>conspiracy</em>.&#xA0; (I saw that happen once, and made a classic non-Bayesian mistake; I knew that this was archetypal schizophrenic behavior, but I didn't realize that similar symptoms can arise from many other causes.&#xA0; Psychosis, it turns out, is a general failure mode, "the fever of CNS illnesses"; it can also be caused by drugs, brain tumors, or just sleep deprivation.&#xA0; I saw the perfect fit to what I'd read of schizophrenia, and didn't ask "<a href="/lw/iw/positive_bias_look_into_the_dark/">What if other things fit just as perfectly?</a>"&#xA0; So my snap diagnosis of schizophrenia turned out to be wrong; but as I wasn't foolish enough to try to handle the case myself, things turned out all right in the end.)</p>
<p>Wikipedia says that the current main hypotheses being considered for psychosis are (a) too much dopamine in one place (b) not enough glutamate somewhere else.&#xA0; (I thought I remembered hearing about serotonin imbalances, but maybe that was something else.)</p>
<p>That's how <em>robust </em>the human brain is: a gentle little neurotransmitter imbalance - so subtle they're still having trouble tracking it down after who knows how many fMRI studies - can give you a full-blown case of stark raving mad.</p>
<p>I don't know how often psychosis happens to hunter-gatherers, so maybe it has something to do with a modern diet?&#xA0; We're not getting exactly the right ratio of Omega 6 to Omega 3 fats, or we're eating too much processed sugar, or something.&#xA0; And among the many other things that go haywire with the metabolism as a result, the brain moves into a more fragile state that breaks down more easily...</p>
<p>Or whatever.&#xA0; That's just a random hypothesis.&#xA0; By which I mean to say:&#xA0; The brain really <em>is</em> adapted to a very narrow range of operating parameters.&#xA0; It doesn't tolerate a little too much dopamine, just as your metabolism isn't very robust against non-ancestral ratios of Omega 6 to Omega 3.&#xA0; Yes, <em>sometimes </em>you get bonus robustness in a new domain, when evolution solves W, X, and Y using a compact adaptation that also extends to novel Z.&#xA0; Other times... quite often, really... Z just isn't covered.</p>
<p>Often, you step outside the box of the ancestral parameter ranges, and things just plain break.</p>
<p>Every part of your brain assumes that all the other surrounding parts work a certain way.&#xA0; The present brain is the Environment of Evolutionary Adaptedness for every individual piece of the present brain.</p>
<p>Start modifying the pieces in ways that seem like "good ideas" - making the frontal cortex larger, for example - and you start operating outside the ancestral box of parameter ranges.&#xA0; And then everything goes to hell.&#xA0; Why <em>shouldn't</em> it?&#xA0; Why would the brain be designed for easy upgradability?</p>
<p>Even if one change works - will the second?&#xA0; Will the third?&#xA0; Will all four changes work well together?&#xA0; Will the fifth change have all that greater a probability of breaking something, because you're already operating that much further outside the ancestral box?&#xA0; Will the sixth change prove that you exhausted all the brain's robustness in tolerating the changes you made already, and now there's no adaptivity left?</p>
<p>Poetry aside, a human being <em>isn't</em> the seed of a god.&#xA0; We don't have neat little dials that you can easily tweak to more "advanced" settings.&#xA0; We are <em>not</em> designed for our parts to be upgraded.&#xA0; Our parts are adapted to work exactly as they are, in their current context, every part tested in a regime of the other parts being the way they are.&#xA0; <a href="/lw/kt/evolutions_are_stupid_but_work_anyway/">Idiot evolution</a> does not look ahead, it does not design with the intent of different future uses.&#xA0; We are <em>not</em> designed to unfold into something bigger<em>.<br></em></p>
<p>Which is not to say that it could never, ever be done.</p>
<p>You could build a modular, cleanly designed AI that could make a billion sequential upgrades to itself using deterministic guarantees of correctness.&#xA0; A Friendly AI programmer could do even more arcane things to make sure the AI knew what you would-want if you understood the possibilities.&#xA0; And then the AI could apply superior intelligence to untangle the pattern of all those neurons (<a href="/lw/x4/nonperson_predicates/">without simulating you in such fine detail as to create a new person</a>), and to foresee the consequences of its acts, and to understand the meaning of those consequences under your values.&#xA0; And the AI could upgrade one thing while simultaneously tweaking the five things that depend on it and the twenty things that depend on them.&#xA0; Finding a gradual, incremental path to greater intelligence (so as not to effectively erase you and replace you with someone else) that didn't drive you psychotic or give you Williams Syndrome or a hundred other syndromes.</p>
<p>Or you could walk the path of unassisted human enhancement, trying to make changes to yourself <em>without</em> understanding them fully.&#xA0; Sometimes changing yourself the wrong way, and being murdered or suspended to disk, and replaced by an earlier backup.&#xA0; Racing against the clock, trying to raise your intelligence without breaking your brain or mutating your will.&#xA0; Hoping you became sufficiently super-smart that you could improve the skill with which you modified yourself.&#xA0; Before your hacked brain moved so far outside ancestral parameters and tolerated so many insults that its fragility reached a limit, and you fell to pieces with every new attempted modification beyond that.&#xA0; Death is far from the worst risk here.&#xA0; Not every form of madness will appear immediately when you branch yourself for testing - some insanities might incubate for a while before they became visible.&#xA0; And you might not notice if your goals shifted only a bit at a time, as your emotional balance altered with the strange new harmonies of your brain.</p>
<p>Each path has its little upsides and downsides.&#xA0; (E.g:&#xA0; AI requires supreme precise knowledge; human upgrading has a nonzero probability of success through trial and error.&#xA0; Malfunctioning AIs mostly kill you and tile the galaxy with smiley faces; human upgrading might produce insane gods to rule over you in Hell forever.&#xA0; Or so my current understanding would predict, anyway; it's not like I've observed any of this as a fact.)</p>
<p>And I'm sorry to dismiss such a gigantic dilemma with three paragraphs, but it wanders from the point of today's post:</p>
<p>The point of today's post is that growing up - or even deciding what you want to be when you grow up - is as around <a href="/lw/x7/cant_unbirth_a_child/">as hard as designing a new intelligent species</a>.&#xA0; Harder, since you're constrained to start from the base of an existing design.&#xA0; There is no <em>natural</em> path laid out to godhood, no Level attribute that you can neatly increment and watch everything else fall into place.&#xA0; It is an <a href="/lw/uk/beyond_the_reach_of_god/">adult problem</a>.</p>
<p>Being a transhumanist means <em>wanting</em> certain things - judging them to be good.&#xA0; It doesn't mean you think those goals are easy to achieve.</p>
<p>Just as there's a wide range of understanding among people who talk about, say, quantum mechanics, there's also a certain range of competence among transhumanists.&#xA0; There are transhumanists who fall into the trap of the <a href="/lw/lg/the_affect_heuristic/">affect heuristic</a>, who see the potential benefit of a technology, and therefore <em>feel really good</em> about that technology, so that it also seems that the technology (a) has readily managed downsides (b) is easy to implement well and (c) will arrive relatively soon.</p>
<p>But only the <em>most </em>formidable adherents of an idea are any sign of its strength.&#xA0; Ten thousand New Agers babbling nonsense, do not cast the least shadow on real quantum mechanics.&#xA0; And among the more formidable transhumanists, it is not at all rare to find someone who wants something <em>and </em>thinks it will not be easy to get.</p>
<p>One is much more likely to find, say, Nick Bostrom - that is, Dr. Nick Bostrom, Director of the Oxford Future of Humanity Institute and founding Chair of the World Transhumanist Assocation - arguing that <a href="http://www.nickbostrom.com/evolution.pdf">a possible test for whether a cognitive enhancement is</a> likely to have downsides, is the ease with which it <em>could</em> have occurred as a natural mutation - since if it had only upsides and could easily occur as a natural mutation, why hasn't the brain already adapted accordingly?&#xA0; This is one reason to be wary of, say, cholinergic memory enhancers: if they have no downsides, why doesn't the brain produce more acetylcholine already?&#xA0; Maybe you're using up a limited memory capacity, or forgetting something else...</p>
<p>And that may or may not turn out to be a good heuristic.&#xA0; But the point is that the serious, smart, technically minded transhumanists, do not always expect that the road to everything they want is easy.&#xA0; (Where you want to be wary of people who say, "But I dutifully acknowledge that there are obstacles!" but stay in basically the same mindset of <a href="/lw/ib/the_proper_use_of_doubt/">never truly doubting</a> the victory.)</p>
<p>So you'll forgive me if I am somewhat annoyed with people who run around saying, "I'd like to be a hundred times as smart!" as if it were as simple as scaling up a hundred times instead of requiring a whole new cognitive architecture; and as if a change of that magnitude in one shot wouldn't amount to erasure and replacement.&#xA0; Or asking, "Hey, <em>why not just</em> augment humans instead of building AI?" as if it wouldn't be a desperate race against madness.</p>
<p>I'm not against being smarter.&#xA0; I'm not against augmenting humans.&#xA0; I am still a transhumanist; I still judge that these are good goals.</p>
<p>But it's really not that <em>simple</em>, okay?</p></div></div></div>
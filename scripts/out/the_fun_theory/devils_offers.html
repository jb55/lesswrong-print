<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_x3" class="content clear"><div class="md">
        
  <div><p><em>(This post is part of the <a href="/lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a>.)</em><br> <strong>Previously in series</strong>:&#xA0; <a href="/lw/x2/harmful_options/">Harmful Options</a></p>
<p>An iota of <a href="/lw/k9/the_logical_fallacy_of_generalization_from/">fictional evidence</a> from <a href="http://books.google.com/books?id=CDjJ7K3bm28C&amp;pg=PA196&amp;lpg=PA196&amp;dq=%22Helion+had+leaned+and+said%22&amp;source=bl&amp;ots=QS-yd1jvWz&amp;sig=twyQUHiB9O-nPsw0BcEAbuAnkgw&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PPA196,M1"><em>The Golden Age</em></a> by John C. Wright:</p>
<p style="margin-left: 40px;">&#xA0;&#xA0;&#xA0; Helion had leaned and said, "Son, once you go in there, the full powers and total command structures of the Rhadamanth Sophotech will be at your command.&#xA0; You will be invested with godlike powers; but you will still have the passions and distempers of a merely human spirit.&#xA0; There are two temptations which will threaten you.&#xA0; First, you will be tempted to remove your human weaknesses by abrupt mental surgery.&#xA0; The Invariants do this, and to a lesser degree, so do the White Manorials, abandoning humanity to escape from pain.&#xA0; Second, you will be tempted to indulge your human weakness.&#xA0; The Cacophiles do this, and to a lesser degree, so do the Black Manorials.&#xA0; Our society will gladly feed every sin and vice and impulse you might have; and then stand by helplessly and watch as you destroy yourself; because the first law of the Golden Oecumene is that no peaceful activity is forbidden.&#xA0; Free men may freely harm themselves, provided only that it is only themselves that they harm."<br>&#xA0;&#xA0;&#xA0; Phaethon knew what his sire was intimating, but he did not let himself feel irritated.&#xA0; Not today.&#xA0; Today was the day of his majority, his emancipation; today, he could forgive even Helion's incessant, nagging fears.<br>&#xA0;&#xA0;&#xA0; Phaethon also knew that most Rhadamanthines were not permitted to face the Noetic tests until they were octogenerians; most did not pass on their first attempt, or even their second.&#xA0; Many folk were not trusted with the full powers of an adult until they reached their Centennial.&#xA0; Helion, despite criticism from the other Silver-Gray branches, was permitting Phaethon to face the tests five years early...</p>
<p><a id="more"></a></p>
<p style="margin-left: 40px;">&#xA0;&#xA0;&#xA0; Then Phaethon said, "It's a paradox, Father.&#xA0; I cannot be, at the same time and in the same sense, a child and an adult.&#xA0; And, if I am an adult, I cannot be, at the same time, free to make my own successes, but not free to make my own mistakes."<br>&#xA0;&#xA0;&#xA0; Helion looked sardonic.&#xA0; "'Mistake' is such a simple word.&#xA0; An adult who suffers a moment of foolishness or anger, one rash moment, has time enough to delete or destroy his own free will, memory, or judgment.&#xA0; No one is allowed to force a cure on him.&#xA0; No one can restore his sanity against his will.&#xA0; And so we all stand quietly by, with folded hands and cold eyes, and meekly watch good men annihilate themselves.&#xA0; It is somewhat... quaint... to call such a horrifying disaster a 'mistake.'"</p>
<p>Is this the best Future we could possibly get to - the Future where you must be absolutely stern and resistant throughout your entire life, because <em>one moment of weakness</em> is enough to betray you to <a href="/lw/h3/superstimuli_and_the_collapse_of_western/">overwhelming temptation</a>?</p>
<p>Such flawless perfection would be easy enough for a superintelligence, perhaps - for a <em>true </em>adult - but for a human, even a hundred-year-old human, it seems like a dangerous and inhospitable place to live.&#xA0; Even if you are strong enough to always choose correctly - maybe you don't want to <em>have </em>to be so strong, always at every moment.</p>
<p>This is the great flaw in Wright's otherwise shining Utopia - that the Sophotechs are <em>helpfully </em>offering up overwhelming temptations to people who would not be at <em>quite </em>so much risk from only <em>themselves</em>.&#xA0; (Though if not for this flaw in Wright's Utopia, he would have had no story...)</p>
<p>If I recall correctly, it was while reading <em>The Golden Age</em> that I generalized the principle "<a href="/lw/wz/living_by_your_own_strength/">Offering people powers beyond their own is not always helping them.</a>"</p>
<p>If you couldn't just ask a Sophotech to edit your neural networks - and you couldn't buy a standard package at the supermarket - but, rather, had to study neuroscience yourself until you could do it with your own hands - then that would act as something of a natural limiter.&#xA0; Sure, there are pleasure centers that would be relatively easy to stimulate; but we don't tell you where they are, so you have to do your own neuroscience.&#xA0; Or we don't sell you your own neurosurgery kit, so you have to build it yourself - metaphorically speaking, anyway -</p>
<p>But you see the idea: it is not so terrible a disrespect for free will, to live in a world in which people are free to shoot their feet off <em>through their own strength</em> - in the hope that by the time they're smart enough to do it <em>under their own power</em>, they're smart enough <em>not </em>to.</p>
<p>The more dangerous and destructive the act, the more you require people to do it without external help.&#xA0; If it's really dangerous, you don't just require them to do their own engineering, but to do their own science.&#xA0; A <a href="/lw/wc/singletons_rule_ok/">singleton</a> might be justified in <a href="/lw/p0/to_spread_science_keep_it_secret/">prohibiting standardized textbooks</a> in certain fields, so that people have to do their own science - make their own discoveries, learn to rule out their own stupid hypotheses, and fight their own overconfidence.&#xA0; Besides, everyone should experience <a href="/lw/os/joy_in_discovery/">the joy of major discovery</a> at least once in their lifetime, and to do this properly, you may have to prevent spoilers from entering the public discourse.&#xA0; So you're getting <a href="/lw/p0/to_spread_science_keep_it_secret/">three</a> social benefits at once, here.</p>
<p>But now I'm trailing off into plots for SF novels, instead of Fun Theory per se.&#xA0; (It can be fun to muse how I would create the world if I had to order it according to my own childish wisdom, but in real life one rather prefers to <a href="/lw/wp/what_i_think_if_not_why/">avoid that scenario</a>.)</p>
<p>As a matter of Fun Theory, though, you can imagine a <em>better </em>world than the Golden Oecumene depicted above - it is not the <em>best </em>world imaginable, fun-theoretically speaking.&#xA0; We would prefer (if attainable) a world in which people own their own mistakes and their own successes, and yet they are not given loaded handguns on a silver platter, nor do they perish through <a href="/lw/ld/the_hidden_complexity_of_wishes/">suicide by genie bottle</a>.</p>
<p>Once you imagine a world in which people can shoot off their own feet <em>through their own strength,</em> are you making that world incrementally better by offering incremental help along the way?</p>
<p>It's one matter to prohibit people from using dangerous powers that they have grown enough to acquire naturally - to literally <em>protect them from themselves.</em>&#xA0; One expects that if a mind kept getting smarter, at some eudaimonic rate of intelligence increase, then - if you took the most obvious course - the mind would eventually become able to edit its own source code, and bliss itself out <a href="/lw/rb/possibility_and_couldness/">if it chose to do so</a>.&#xA0; Unless the mind's growth were steered onto a non-obvious course, or monitors were mandated to prohibit that event...&#xA0; To protect people <em>from their own powers</em> might take some twisting.</p>
<p>To descend from above and <em>offer dangerous powers as an untimely gift</em>, is another matter entirely.&#xA0; That's why the title of this post is "Devil's Offers", not "Dangerous Choices".</p>
<p>And to allow dangerous powers to be sold in a marketplace - or alternatively to prohibit them from being transferred from one mind to another - that is somewhere in between.</p>
<p>John C. Wright's writing has a particular poignancy for me, for in my <a href="/lw/u2/the_sheer_folly_of_callow_youth/">foolish youth</a> I thought that something very much like this scenario was a good idea - that a benevolent superintelligence ought to go around offering people lots of options, and doing as it was asked.</p>
<p>In retrospect, this was a case of a pernicious distortion where you end up believing things that are easy to market to other people.</p>
<p>I know someone who drives across the country on long trips, rather than flying.&#xA0; Air travel scares him.&#xA0; Statistics, naturally, show that flying a given distance is much safer than driving it.&#xA0; But some people fear too much the <em>loss of control</em> that comes from not having their own hands on the steering wheel.&#xA0; It's a common complaint.</p>
<p>The future sounds less scary if you imagine yourself having lots of control over it.&#xA0; For every awful thing that you imagine happening to you, you can imagine, "But I won't choose that, so it will be all right."</p>
<p>And if it's not your own hands on the steering wheel, you think of scary things, and imagine, "What if this is chosen <em>for </em>me, and I can't say no?"</p>
<p>But in real life rather than imagination, human choice is a fragile thing.&#xA0; If the whole field of heuristics and biases teaches us anything, it surely teaches us that.&#xA0; Nor has it been the verdict of experiment, that humans correctly estimate the flaws of their own decision mechanisms.</p>
<p>I flinched away from that thought's implications, not so much because I feared superintelligent paternalism <em>myself</em>, but because I feared what other people would say of that position.&#xA0; If I believed it, I would have to defend it, so I managed not to believe it.&#xA0; Instead I told people not to worry, a superintelligence would surely respect their decisions (and even believed it myself).&#xA0; A very pernicious sort of self-deception.</p>
<p>Human governments are made up of humans who are foolish like ourselves, plus they have poor incentives.&#xA0; Less skin in the game, and <a href="/lw/uu/why_does_power_corrupt/">specific human brainware to be corrupted by wielding power</a>.&#xA0; So we've learned the historical lesson to be wary of ceding control to human bureaucrats and politicians.&#xA0; We may even be emotionally hardwired to resent the loss of anything we perceive as power.</p>
<p>Which is just to say that people are biased, by instinct, by <a href="/lw/so/humans_in_funny_suits/">anthropomorphism</a>, and by narrow experience, to <a href="/lw/uv/ends_dont_justify_means_among_humans/"><em>under</em>estimate how much they could potentially trust a superintelligence</a> which lacks a human's corruption circuits, doesn't easily make certain kinds of mistakes, and has strong overlap between its motives and your own interests.</p>
<p>Do you trust yourself?&#xA0; Do you trust yourself to know when to trust yourself?&#xA0; If you're dealing with a superintelligence kindly enough to care about you at all, rather than disassembling you for raw materials, are you wise to second-guess its choice of <em>who </em>it thinks should decide?&#xA0; Do you think you have a superior epistemic vantage point here, or what?</p>
<p>Obviously we should not trust all agents who claim to be trustworthy - especially if they are <em>weak </em>enough, relative to us, to <em>need </em>our goodwill.&#xA0; But I am quite ready to accept that a benevolent superintelligence may not offer certain choices.</p>
<p>If you <em>feel safer</em> driving than flying, because that way it's your own hands on the steering wheel, statistics be damned -</p>
<p>- then maybe it isn't <em>helping </em>you, for a superintelligence to offer you the option of driving.</p>
<p>Gravity doesn't ask you if you would like to float up out of the atmosphere into space and die.&#xA0; But you don't go around complaining that gravity is a tyrant, right?&#xA0; You can build a spaceship if you work hard and study hard.&#xA0; It would be a more dangerous world if your six-year-old son could do it in an hour using string and cardboard.</p></div></div></div>
<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_x4" class="content clear"><div class="md">
        
  <div><p><strong>Followup to</strong>:&#xA0; <a href="http://www.overcomingbias.com/2008/03/righting-a-wron.html">Righting a Wrong Question</a>, <a href="http://www.overcomingbias.com/2008/04/zombies.html">Zombies! Zombies?</a>, <a href="http://www.overcomingbias.com/2008/05/an-ai-new-timer.html">A Premature Word on AI</a>, <a href="http://www.overcomingbias.com/2008/10/try-persevere.html">On Doing the Impossible</a></p>
<p>There is a subproblem of <a href="http://yudkowsky.net/singularity/ai-risk">Friendly AI</a> which is so scary that I usually don't talk about it, because only a longtime reader of <em>Overcoming Bias</em> would react to it appropriately - that is, by saying, "Wow, that does sound like an <em>interesting </em>problem", instead of finding one of many subtle ways to scream and run away.</p>
<p>This is the problem that if you create an AI and tell it to model the world around it, it may form models of people that are people themselves.&#xA0; Not necessarily the <em>same</em> person, but people nonetheless.</p>
<p>If you look up at the night sky, and see the tiny dots of light that move over days and weeks - <span lang="grc-Latn" style="white-space: normal; text-decoration: none;" xml:lang="grc-Latn" class="Unicode" title="grc transliteration"><em>plan&#x113;toi</em></span>, the Greeks called them, "wanderers" - and you try to predict the movements of those planet-dots as best you can...</p>
<p>Historically, humans went through a journey as long and as wandering as the planets themselves, to find an accurate model.&#xA0; In the beginning, the models were things of cycles and epicycles, not much resembling the true Solar System.</p>
<p>But eventually we found laws of gravity, and finally built models - even if they were just on paper - that were <em>extremely</em> accurate so that Neptune could be deduced by looking at the unexplained perturbation of Uranus from its expected orbit.&#xA0; This required moment-by-moment modeling of where a simplified version of Uranus would be, and the other known planets.&#xA0; Simulation, not just abstraction.&#xA0; Prediction through simplified-yet-still-detailed pointwise similarity.</p>
<p>Suppose you have an AI that is around human beings.&#xA0; And like any Bayesian trying to explain its enivornment, the AI goes in quest of <em>highly accurate models </em>that predict what it sees of humans.</p>
<p>Models that predict/explain why people do the things they do, say the things they say, want the things they want, think the things they think, and even why people talk about "the <a href="http://www.overcomingbias.com/2007/08/mysterious-answ.html">mystery</a> of <a href="http://www.overcomingbias.com/2008/04/zombies.html">subjective experience</a>".</p>
<p>The model that most precisely predicts these facts, may well be a 'simulation' detailed enough to <em>be </em>a person in its own right.</p>
<p><a id="more"></a></p>
<p>A highly detailed model <em>of </em>me, may not <em>be </em>me.&#xA0; But it will, at least, be a model which (for purposes of prediction via similarity) thinks <em>itself </em>to be Eliezer Yudkowsky.&#xA0; It will be a model that, when cranked to find my behavior if asked "Who are you and are you conscious?", says "I am Eliezer Yudkowsky and I seem have subjective experiences" <a href="http://www.overcomingbias.com/2008/04/zombies.html">for much the same reason I do</a>.</p>
<p>If that doesn't worry you, (re)read <a href="http://www.overcomingbias.com/2008/04/zombies.html">Zombies! Zombies?</a>.</p>
<p>It seems likely (though not certain) that this happens <em>automatically</em>, whenever a mind of sufficient power to find the right answer, and not <em>otherwise</em> disinclined to create a sentient being trapped within itself, tries to model a human as accurately as possible.</p>
<p>Now you could wave your hands and say, "Oh, by the time the AI is smart enough to do that, it will be smart enough not to".&#xA0; (This is, in general, a phrase useful in running away from Friendly AI problems.)&#xA0; But do you know this for a fact?</p>
<p>When dealing with things that confuse you, it is wise to widen your confidence intervals.&#xA0; Is a human mind the simplest possible mind that can be sentient?&#xA0; What if, in the course of trying to model its own programmers, a relatively younger AI manages to create a sentient simulation trapped within itself?&#xA0; How soon do you have to start worrying?&#xA0; Ask yourself that fundamental question, "What do I think I know, and how do I think I know it?"</p>
<p>You could wave your hands and say, "Oh, it's more important to get the job done quickly, then to worry about such relatively minor problems; the end justifies the means.&#xA0; Why, look at all these problems the Earth has right now..."&#xA0; (This is also a general way of running from Friendly AI problems.)</p>
<p>But we may consider and discard many hypotheses in the course of finding the truth, and we are but slow humans.&#xA0; What if an AI creates millions, billions, trillions of alternative hypotheses, models that are actually people, who die when they are disproven?</p>
<p>If you accidentally kill a few trillion people, or permit them to be killed - you could say that the weight of the Future outweighs this evil, perhaps.&#xA0; But the absolute weight of the sin would not be light.&#xA0; If you would balk at killing a million people with a nuclear weapon, you should balk at this.</p>
<p>You could wave your hands and say, "The model will contain abstractions over various uncertainties within it, and this will prevent it from being conscious even though it produces well-calibrated probability distributions over what you will say when you are asked to talk about consciousness."&#xA0; To which I can only reply, "That would be very convenient if it were true, but how the hell do you <em>know </em>that?"&#xA0; An element of a model marked 'abstract' is still there as a computational token, and the interacting causal system may still be sentient.</p>
<p>For these purposes, we do not, in principle, need to crack the entire Hard Problem of Consciousness - the <a href="http://www.overcomingbias.com/2008/03/dissolving-the.html">confusion</a> that we name "subjective experience".&#xA0; We only need to understand enough of it to know when a process is <em>not</em> conscious, <em>not </em>a person, <em>not </em>something deserving of the rights of citizenship.&#xA0; In practice, I suspect you can't <em>halfway </em>stop being confused - but in theory, half would be enough.</p>
<p>We need a <em>nonperson predicate</em> - a predicate that returns 1 for anything that is a person, and can return 0 or 1 for anything that is not a person.&#xA0; This is a "nonperson predicate" because <em>if </em>it returns 0, <em>then </em>you know that something is definitely not a person.</p>
<p>You can have more than one such predicate, and if <em>any </em>of them returns 0, you're ok.&#xA0; It just had better never return 0 on anything that <em>is</em> a person, however many nonpeople it returns 1 on.</p>
<p>We can even hope that the vast majority of models the AI needs, will be swiftly and trivially excluded by a predicate that quickly answers 0.&#xA0; And that the AI would only need to resort to more specific predicates in case of modeling actual people.</p>
<p>With a good toolbox of nonperson predicates in hand, we could exclude all "model citizens" - all beliefs that are themselves people - from the set of hypotheses our Bayesian AI may invent to try to model its person-containing environment.</p>
<p>Does that sound odd?&#xA0; Well, one has to handle the problem somehow.&#xA0; I am open to better ideas, though I will be a bit skeptical about any suggestions for how to proceed that let us <a href="http://www.overcomingbias.com/2008/12/artificial-myst.html">cleverly avoid solving the damn mystery</a>.</p>
<p>So do I <em>have</em> a nonperson predicate?&#xA0; No.&#xA0; At least, no nontrivial ones.</p>
<p>This is a challenge that I have not even tried to talk about, with <a href="http://www.overcomingbias.com/2008/09/above-average-s.html">those folk who think themselves ready to challenge the problem of true AI</a>.&#xA0; For they seem to have the standard reflex of running away from difficult problems, and are challenging AI only because <a href="http://www.overcomingbias.com/2008/08/dreams-of-ai-de.html">they think their amazing insight has already solved it</a>.&#xA0; Just mentioning the problem of Friendly AI by itself, or of precision-grade AI design, is enough to send them fleeing into the night, screaming "It's too hard!&#xA0; It can't be done!"&#xA0; If I tried to explain that their job duties might impinge upon the sacred, mysterious, holy Problem of Subjective Experience -</p>
<p>- I'd actually expect to get blank stares, mostly, followed by some <em>instantaneous </em>dismissal which requires no further effort on their part.&#xA0; I'm not sure of what the exact dismissal would be - maybe, "Oh, none of the hypotheses my AI considers, could <em>possibly </em>be a person?"&#xA0; I don't know; I haven't bothered trying.</p>
<p>But it has to be a dismissal which rules out all possibility of their having to <a href="http://www.overcomingbias.com/2008/12/artificial-myst.html">actually solve the damn problem</a>, because most of them would think that they are smart enough to build an AI - indeed, smart enough to have already solved the key part of the problem - but not smart enough to solve the Mystery of Consciousness, which still <em>looks</em> scary to them.</p>
<p>Even if they thought of trying to solve it, they would be afraid of <em>admitting </em>they were trying to solve it.&#xA0; Most of these people cling to the shreds of their <a href="http://www.overcomingbias.com/2006/12/the_proper_use_.html">modesty</a>, trying at one and the same time to have solved the AI problem while still being humble ordinary blokes.&#xA0; (There's a grain of truth to that, but at the same time: who the hell do they think they're kidding?)&#xA0; They know without words that their audience sees the Mystery of Consciousness as a <em>sacred untouchable problem</em>, <a href="http://www.overcomingbias.com/2008/05/einsteins-super.html">reserved for some future superbeing</a>.&#xA0; They don't want people to think that they're claiming <a href="http://www.overcomingbias.com/2008/05/einsteins-super.html">an Einsteinian aura of destiny</a> by trying to solve the problem.&#xA0; So it is easier to dismiss the problem, and not believe a proposition that would be uncomfortable to explain.</p>
<p>Build an AI?&#xA0; Sure!&#xA0; Make it Friendly?&#xA0; Now that you point it out, sure!&#xA0; But trying to come up with a "nonperson predicate"?&#xA0; That's just way above the difficulty level they signed up to handle.</p>
<p>But a longtime <em>Overcoming Bias</em> reader will be aware that <a href="http://www.overcomingbias.com/2007/08/mysterious-answ.html">a blank map does not correspond to a blank territory</a>.&#xA0; That <a href="http://www.overcomingbias.com/2008/03/wrong-questions.html">impossible confusing questions correspond to places where your own thoughts are tangled</a>, not to places where the environment itself contains magic.&#xA0; That <a href="http://www.overcomingbias.com/2008/05/einsteins-super.html">even difficult problems do not require an aura of destiny to solve</a>.&#xA0; And that the first step to solving one is <a href="http://www.overcomingbias.com/2008/10/try-persevere.html">not running away from the problem like a frightened rabbit</a>, but instead sticking long enough to learn something.</p>
<p>So I am not running away from this problem myself.&#xA0; <a href="http://www.overcomingbias.com/2008/03/wrong-questions.html">I doubt it is even difficult in any absolute sense, just a place where my brain is tangled.</a>&#xA0; I suspect, based on some prior experience with similar challenges, that you can't <em>really </em>be good enough to build a Friendly AI, and still be tangled up in your own brain like that.&#xA0; So it is not necessarily any <em>new </em>effort - over and above that required <em>generally </em>to build a mind while knowing exactly what you are about.</p>
<p>But in any case, I am not screaming and running away from the problem.&#xA0; And I hope that you, dear longtime <em>Overcoming Bias</em> reader, will not faint at the audacity of my trying to solve it.</p></div></div></div>
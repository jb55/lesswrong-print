<!-- .meta --><!--<div><i><h0>Part 1 of 13 in the sequence &nbsp;<a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions">Mysterious Answers to Mysterious Questions</a></h0></i><br/><br/></div>--><div id="entry_t3_x2" class="content clear"><div class="md">
        
  <div><p><em>(This post is part of the <a href="/lw/xy/the_fun_theory_sequence/">Fun Theory Sequence</a>.)</em><br> <strong>Previously in series</strong>:&#xA0; <a href="/lw/wz/living_by_your_own_strength/">Living By Your Own Strength</a></p>
<p>Barry Schwartz's <a href="http://www.swarthmore.edu/SocSci/bschwar1/Sci.Amer.pdf">The Paradox of Choice</a> - which I haven't read, though I've read some of the research behind it - talks about how offering people <em>more choices</em> can make them <em>less happy</em>.</p>
<p>A simple intuition says this shouldn't ought to happen to rational agents:&#xA0; If your current choice is X, and you're offered an alternative Y that's worse than X, and you know it, you can always just go on doing X.&#xA0; So a rational agent shouldn't do worse by having more options.&#xA0; The more available actions you have, the more powerful you become - that's how it should ought to work.</p>
<p>For example, if an ideal rational agent is initially <em>forced</em> to <a href="/lw/nc/newcombs_problem_and_regret_of_rationality/">take only box B in Newcomb's Problem</a>, and is then offered the <em>additional</em> choice of taking both boxes A and B, the rational agent shouldn't <em>regret having more options</em>.&#xA0; Such regret indicates that you're "fighting your own ritual of cognition" which helplessly selects the worse choice once it's offered you.</p>
<p>But this intuition only governs <em>extremely</em> idealized rationalists, or rationalists in extremely idealized situations.&#xA0; Bounded rationalists can easily do worse with strictly more options, because they burn computing operations to evaluate them.&#xA0; You could write an invincible chess program in one line of Python if its only legal move were the winning one.</p>
<p>Of course Schwartz and co. are not talking about anything so pure and innocent as the <em>computing cost</em> of having more choices.</p>
<p>If you're dealing, not with an ideal rationalist, not with a bounded rationalist, but with a <em>human being -</em></p>
<p>Say, would you like to finish reading this post, or <a href="http://www.youtube.com/watch?v=Yu_moia-oVI">watch this surprising video</a> instead?</p>
<p><a id="more"></a></p>
<p>Schwartz, I believe, talks primarily about the decrease in <em>happiness</em> and <em>satisfaction</em> that results from having more mutually exclusive options.&#xA0; Before this research was done, it was already known that people are more sensitive to losses than to gains, generally by a factor of between 2 and 2.5 (in various different experimental scenarios).&#xA0; That is, the pain of losing something is between 2 and 2.5 times as worse as the joy of gaining it.&#xA0; (This is an interesting constant in its own right, and may have something to do with compensating for our systematic overconfidence.)</p>
<p>So - if you can only choose one dessert, you're likely to be happier choosing from a menu of two than a menu of fourteen.&#xA0; In the first case, you eat one dessert and pass up one dessert; in the latter case, you eat one dessert and pass up thirteen desserts.&#xA0; And we are more sensitive to loss than to gain.</p>
<p>(If I order dessert on a menu at all, I will order quickly and then close the menu and put it away, so as not to look at the other items.)</p>
<p>Not only that, but if the options have incommensurable attributes, then whatever option we select is likely to <em>look worse</em> because of the comparison.&#xA0; A luxury car that would have looked great by comparison to a Crown Victoria, instead becomes slower than the Ferrari, more expensive than the 9-5, with worse mileage than the Prius, and not looking quite as good as the Mustang.&#xA0; So we lose on satisfaction with the road we <em>did</em> take.</p>
<p>And then there are more direct forms of harm done by painful choices.&#xA0; IIRC, an experiment showed that people who <em>refused</em> to eat a cookie - who were offered the cookie, and chose <em>not</em> to take it - did worse on subsequent tests of mental performance than either those who ate the cookie or those who were not offered any cookie.&#xA0; You pay a price in mental energy for resisting temptation.</p>
<p>Or consider the various "trolley problems" of ethical philosophy - a trolley is bearing down on 5 people, but there's one person who's very fat and can be pushed onto the tracks to stop the trolley, that sort of thing.&#xA0; If you're forced to choose between two unacceptable evils, you'll pay a price either way.&#xA0; Vide <em>Sophie's Choice</em>.</p>
<p>An option need not be taken, or even be strongly considered, in order to wreak harm.&#xA0; Recall the point from "<a href="/lw/ww/high_challenge/">High Challenge</a>", about how offering to do someone's work for them is not always helping them - how the ultimate computer game is not the one that just says "YOU WIN", forever.</p>
<p>Suppose your computer games, in addition to the <em>long difficult</em> path to your level's goal, also had little side-paths that you could use - directly in the game, as corridors - that would bypass all the enemies and take you straight to the goal, offering along the way all the items and experience that you could have gotten the hard way.&#xA0; And this corridor is always visible, out of the corner of your eye.</p>
<p><em>Even </em><em>if you resolutely refused</em> to take the easy path through the game, knowing that it would cheat you of the very experience that you paid money in order to buy - wouldn't that always-visible corridor, make the game that much less fun?&#xA0; Knowing, for every alien you shot, and every decision you made, that there was always an easier path?</p>
<p>I don't know if this story has ever been written, but you can imagine a Devil who follows someone around, making their life miserable, <em>solely by offering them options which are never actually taken</em> - a "deal with the Devil" story that only requires the Devil to have the <em>capacity</em> to grant wishes, rather than ever granting a single one.</p>
<p>And what if the worse option is actually taken?&#xA0; I'm not suggesting that it is always a good idea <a href="/lw/gz/policy_debates_should_not_appear_onesided/">for human governments to go around Prohibiting temptations</a>.&#xA0; But the literature of heuristics and biases is replete with examples of reproducible stupid choices; and there is also such a thing as akrasia (weakness of will).</p>
<p>If you're an agent operating from a <em>much </em>higher vantage point - high enough to see humans as flawed algorithms, so that it's not a matter of second-guessing but second-<em>knowing</em> - then is it <em>benevolence</em> to offer choices that will assuredly be made wrongly?&#xA0; Clearly, removing all choices from someone and reducing their life to <a href="http://www.progressquest.com/">Progress Quest</a>, is not helping them.&#xA0; But are we wise enough to <em>know when we should choose?</em>&#xA0; And in some cases, even offering that much of a choice, <em>even if the choice is made correctly,</em> may already do the harm...</p></div></div></div>

# Belief in the Implied Invisible

One generalized lesson *not* to learn from the Anti-Zombie Argument
is, "Anything you can't see doesn't exist."

It's tempting to conclude the general rule.  It would make the
Anti-Zombie Argument much simpler, on future occasions, if we could
take this as a premise.  But unfortunately that's just not
Bayesian.

Suppose I transmit a photon out toward infinity, not aimed at any
stars, or any galaxies, pointing it toward one of the great voids
between superclusters.  Based on standard physics, in other words,
I don't expect this photon to intercept anything on its way out. 
The photon is moving at light speed, so I can't chase after it and
capture it again.

If the expansion of the universe is accelerating, as current
cosmology holds, there will come a future point where I don't
expect to be able to interact with the photon even in principle - a
future time beyond which I don't expect the photon's future light
cone to intercept my world-line.  Even if an alien species captured
the photon and rushed back to tell us, they couldn't travel fast
enough to make up for the accelerating expansion of the universe.

Should I believe that, in the moment where I can no longer interact
with it even in principle, the photon disappears?

No.

It would violate Conservation of Energy.  And the second law of
thermodynamics.  And just about every other law of physics.  And
probably the Three Laws of Robotics.  It would imply the photon
knows I care about it and knows exactly when to disappear.

It's a *silly idea*.

But if you can believe in the continued existence of photons that
have become experimentally undetectable to you, why doesn't this
imply a general license to believe in the invisible?

(If you want to think about this question on your own, do so before
the jump...)

Though I failed to Google a source, I remember reading that when it
was first proposed that the Milky Way was our *galaxy*- that the
hazy river of light in the night sky was made up of millions (or
even billions) of stars - that Occam's Razor was invoked against
the new hypothesis.  Because, you see, the hypothesis vastly
multiplied the number of "entities" in the believed universe.  Or
maybe it was the suggestion that "nebulae" - those hazy patches
seen through a telescope - might be galaxies full of stars, that
got the invocation of Occam's Razor.

*Lex parsimoniae:  Entia non sunt multiplicanda praeter necessitatem.*

That was Occam's original formulation, the law of parsimony: 
Entities should not be multiplied beyond necessity.

If you postulate billions of stars that no one has ever believed in
before, you're multiplying entities, aren't you?

No.  There are
[two Bayesian formalizations of Occam's Razor](/lw/jp/occams_razor/): 
Solomonoff Induction, and Minimum Message Length.  Neither
penalizes galaxies for being big.

Which they had better not do!  One of the lessons of history is
that what-we-call-reality keeps turning out to be bigger and bigger
and huger yet.  Remember when the Earth was at the center of the
universe?  Remember when no one had invented Avogadro's number?  If
Occam's Razor was weighing against the multiplication of entities
every time, we'd have to start doubting Occam's Razor, because it
would have consistently turned out to be wrong.

In Solomonoff induction, the complexity of your model is the amount
of *code* in the computer program you have to write to simulate
your model.  The amount of *code,* not the amount of RAM it uses,
or the number of cycles it takes to compute.  A model of the
universe that contains billions of galaxies containing billions of
stars, each star made of a billion trillion decillion quarks, will
take a lot of RAM to run - but the *code* only has to describe the
behavior of the quarks, and the stars and galaxies can be left to
run themselves.  I am speaking semi-metaphorically here - there are
things in the universe besides quarks - but the point is,
postulating an extra billion galaxies doesn't count against the
size of your code, if you've already described one galaxy.  It just
takes a bit more RAM, and Occam's Razor doesn't care about RAM.

Why not?  The Minimum Message Length formalism, which is nearly
equivalent to Solomonoff Induction, may make the principle
clearer:  If you have to tell someone how your model of the
universe works, you don't have to individually specify the location
of each quark in each star in each galaxy.  You just have to write
down some equations.  The amount of "stuff" that obeys the equation
doesn't affect how long it takes to write the equation down.  If
you encode the equation into a file, and the file is 100 bits long,
then there are 2^100^ other models that would be around the same
file size, and you'll need roughly 100 bits of supporting
evidence.  You've got a limited amount of probability mass; and a
priori, you've got to divide that mass up among all the messages
you could send; and so postulating a model from within a model
space of 2^100^ alternatives, means you've got to accept a 2^-100^
prior probability penalty - but having more galaxies doesn't add to
this.

Postulating billions of stars in billions of galaxies doesn't
affect the length of your message describing the overall behavior
of all those galaxies.  So you don't take a probability hit from
having the *same*equations describing more things.  (So long as
your model's predictive successes aren't sensitive to the exact
initial conditions.  If you've got to specify the exact positions
of all the quarks for your model to predict as well as it does, the
extra quarks do count as a hit.)

If you suppose that the photon disappears when you are no longer
looking at it, this is an *additional law* in your model of the
universe.  It's the laws that are "entities", costly under the laws
of parsimony.  Extra quarks are free.

So does it boil down to, "I believe the photon goes on existing as
it wings off to nowhere, because my priors say it's simpler for it
to go on existing than to disappear"?

This is what I thought at first, but on reflection, it's not quite
right.  (And not just because it opens the door to obvious
abuses.)

I would boil it down to a distinction between belief in the
*implied invisible,* and belief in the *additional invisible.*

When you believe that the photon goes on existing as it wings out
to infinity, you're not believing that as an *additional* fact.

What you believe (assign probability to) is a set of simple
equations; you believe these equations describe the universe.  You
believe these equations because they are the simplest equations you
could find that describe the evidence.  These equations are
*highly*experimentally testable; they explain huge mounds of
evidence visible in the past, and predict the results of many
observations in the future.

You believe these equations, and it is a *logical implication* of
these equations that the photon goes on existing as it wings off to
nowhere, so you believe that as well.

Your priors, or even your probabilities, don't *directly*talk about
the photon.  What you assign probability to is not the photon, but
the general laws.  When you assign probability to the laws of
physics as we know them, you *automatically* contribute that same
probability to the photon continuing to exist on its way to nowhere
- if you believe the logical implications of what you believe.

It's not that you believe in the invisible *as such,* from
reasoning about invisible things.  Rather the experimental evidence
supports certain laws, and belief in those laws logically implies
the existence of certain entities that you can't interact with. 
This is belief in the *implied invisible.*

On the other hand, if you believe that the photon is eaten out of
existence by the Flying Spaghetti Monster - maybe on this just one
occasion - or even if you believed without reason that the photon
hit a dust speck on its way out - then you would be believing in a
specific extra invisible event, on its own.  If you thought that
this sort of thing happened in general, you would believe in a
specific extra invisible law.  This is belief in the
*additional invisible.*

The whole matter would be a lot simpler, admittedly, if we could
just rule out the existence of entities we can't interact with,
once and for all - have the universe stop existing at the edge of
our telescopes.  But this requires us to be very silly.

Saying that you shouldn't ever need a separate and additional
belief about invisible things - that you only believe invisibles
that are *logical implications* of general laws which are
themselves testable, and even then, don't have any further beliefs
about them that are not logical implications of visibly testable
general rules - actually does seem to rule out all abuses of belief
in the invisible, when applied correctly.

Perhaps I should say, "you should assign unaltered prior
probability to additional invisibles", rather than saying, "do not
believe in them."  But if you think of a *belief* as something
evidentially additional, something you bother to track, something
where you bother to count up support for or against, then it's
questionable whether we should ever have additional beliefs about
additional invisibles.

There are exotic cases that break this in theory.  (E.g:  The
epiphenomenal demons are watching you, and will torture
[3\^\^\^3](/lw/kd/pascals_mugging_tiny_probabilities_of_vast/)
victims for a year, somewhere you can't ever verify the event, if
you ever say the word "Niblick".)  But I can't think of a case
where the principle fails in human practice.

**Added:**  To make it clear why you would sometimes want to think
about implied invisibles, suppose you're going to launch a
spaceship, at nearly the speed of light, toward a faraway
supercluster.  By the time the spaceship gets there and sets up a
colony, the universe's expansion will have accelerated too much for
them to ever send a message back.  Do you deem it worth the purely
altruistic effort to set up this colony, for the sake of all the
people who will live there and be happy?  Or do you think the
spaceship blips out of existence before it gets there?  This could
be a very real question at some point.

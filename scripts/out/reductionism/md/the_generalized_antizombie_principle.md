
# The Generalized Anti-Zombie Principle

> "Each problem that I solved became a rule which served afterwards
> to solve other problems."  
>         -- Rene Descartes, *Discours de la Methode*

"[Zombies](/lw/p7/zombies_zombies/)" are putatively beings that are
atom-by-atom identical to us, governed by all the same
third-party-visible physical laws, except that they are not
conscious.

Though the philosophy is complicated, the
[core argument against zombies](/lw/p7/zombies_zombies/) is
simple:  When you focus your inward awareness on your inward
awareness, soon after your internal narrative (the little voice
inside your head that speaks your thoughts) says "I am aware of
being aware", and then you say it out loud, and then you type it
into a computer keyboard, and create a third-party visible blog
post.

Consciousness, whatever it may be - a substance, a process, a name
for a confusion - is not epiphenomenal; your mind can catch the
inner listener in the act of listening, and say so out loud. 
*The fact that I have typed this paragraph* would at least *seem*
to refute the idea that consciousness has no experimentally
detectable consequences.

I hate to say "So now let's accept this and move on," over such a
philosophically controversial question, but it seems like a
considerable majority of Overcoming Bias commenters do accept
this.  And there are other conclusions you can only get to after
you accept that you cannot subtract consciousness and leave the
universe looking exactly the same.  So now let's accept this and
move on.

The form of the Anti-Zombie Argument seems like it should
generalize, becoming an Anti-Zombie Principle.  But what is the
proper generalization?

Let's say, for example, that someone says:  "I have a switch in my
hand, which does not affect your brain in any way; and iff this
switch is flipped, you will cease to be conscious."  Does the
Anti-Zombie Principle rule this out as well, with the same
structure of argument?

It appears to me that in the case above, the answer is yes.  In
particular, you can say:  "Even after your switch is flipped, I
will still talk about consciousness *for exactly the same reasons*
I did before.  If I am conscious right now, I will still be
conscious after you flip the switch."

Philosophers may object, "But now you're equating consciousness
with talking about consciousness!  What about the Zombie Master,
the chatbot that regurgitates a remixed corpus of amateur human
discourse on consciousness?"

But I did *not* equate "consciousness" with verbal behavior.  The
core premise is that, *among other things,* the
[true referent](/lw/p6/reductive_reference/) of "consciousness" is
*also* the *cause in humans* of talking about inner listeners.

As I argued (at some length) in the
[sequence on words](/lw/od/37_ways_that_words_can_be_wrong/), what
you want in defining a word is not always a perfect
[Aristotelian](/lw/nf/the_parable_of_hemlock/)
necessary-and-sufficient definition; sometimes you just want a
[treasure map](/lw/nh/extensions_and_intensions/) that leads you to
the extensional referent.  So "that which *does in fact* make me
talk about an unspeakable awareness" is not a
necessary-and-sufficient definition.  But if what does *in fact*
cause me to discourse about an unspeakable awareness, is not
"consciousness", then...

...then the discourse gets pretty futile.  That is not a knockdown
argument against zombies - an
[empirical](/lw/ne/the_parable_of_the_dagger/) question can't be
settled by mere difficulties of discourse.  But if you try to defy
the Anti-Zombie Principle, you will have problems with the
*meaning* of your discourse, not just its plausibility.

Could we *define* the word "consciousness" to mean "whatever
actually makes humans talk about 'consciousness'"?  This would have
the powerful advantage of guaranteeing that there is at least one
real fact named by the word "consciousness".  Even if our belief in
consciousness is a confusion, "consciousness" would name the
cognitive architecture that generated the confusion.  But to
establish a definition is only to promise to use a word
consistently; it doesn't settle any empirical questions, such as
whether our inner awareness makes us talk about our inner
awareness.

Let's return to the Off-Switch.

If we allow that the Anti-Zombie Argument applies against the
Off-Switch, then the Generalized Anti-Zombie Principle does *not*
say only, "Any change that is not in-principle experimentally
detectable (IPED) cannot remove your consciousness."  The switch's
flipping is experimentally detectable, but it still seems
*highly*unlikely to remove your consciousness.

Perhaps the Anti-Zombie Principle says, "Any change that does not
affect you in any IPED way cannot remove your consciousness"?

But is it a reasonable stipulation to say that flipping the switch
does not affect you in *any* IPED way?  All the particles in the
switch are interacting with the particles composing your body and
brain.  There are gravitational effects - tiny, but real and IPED. 
The gravitational pull from a one-gram switch ten meters away is
[around](http://www.google.com/search?hl=en&safe=off&q=G+*+1+gram+/+(10+meters)%5E2&btnG=Search)
6 \* 10^-16^ m/s^2^.  That's around half a neutron diameter per
second per second, far below thermal noise, but way above the
Planck level.

We could flip the switch light-years away, in which case the flip
would have no immediate causal effect on you (whatever "immediate"
means in this case) (if the Standard Model of physics is correct).

But it doesn't seem like we *should* have to alter the thought
experiment in this fashion.  It seems that, if a disconnected
switch is flipped on the other side of a room, you should not
expect your inner listener to go out like a light, because the
switch "obviously doesn't change" that which is the true cause of
your talking about an inner listener.  Whatever you really are, you
don't expect the switch to mess with it.

This is a *large* step.

If you deny that it is a reasonable step, you had better never go
near a switch again.  But still, it's a large step.

The key idea of [reductionism](/lw/on/reductionism/) is that our
maps of the universe are multi-level to save on computing power,
but physics seems to be strictly single-level.  All our discourse
about the universe takes place using
[references far above](/lw/p6/reductive_reference/) the level of
fundamental particles.

The switch's flip *does* change the fundamental particles of your
body and brain.  It nudges them by whole neutron diameters away
from where they would have otherwise been.

In ordinary life, we gloss a change this small by saying that the
switch "doesn't affect you".  But it *does* affect you.  It changes
everything by whole neutron diameters!  What could possibly be
remaining the same?  Only the *description* that you would give of
the higher levels of organization - the cells, the proteins, the
spikes traveling along a neural axon.  As the map is far less
detailed than the territory, it must map
[many different states to the same description](/lw/nw/fallacies_of_compression/).

Any reasonable sort of humanish *description* of the brain that
talks about neurons and activity patterns (or even the
conformations of individual microtubules making up axons and
dendrites) won't change when you flip a switch on the other side of
the room.  Nuclei are larger than neutrons, atoms are larger than
nuclei, and by the time you get up to talking about the *molecular*
level, that tiny little gravitational force has vanished from the
list of things you bother to *track*.

But if you add up enough tiny little gravitational pulls, they will
eventually yank you across the room and tear you apart by tidal
forces, so clearly a small effect is *not* "no effect at all".

Maybe the tidal force from that tiny little pull, by an *amazing*
coincidence, pulls a single extra calcium ion just a tiny bit
closer to an ion channel, causing it to be pulled in just a tiny
bit sooner, making a single neuron fire infinitesimally sooner than
it would otherwise have done, a difference which amplifies
chaotically, finally making a whole neural spike occur that
otherwise wouldn't have occurred, sending you off on a different
train of thought, that triggers an epileptic fit, that kills you,
causing you to cease to be conscious...

If you add up a lot of tiny quantitative effects, you get a big
quantitative effect - big enough to mess with anything you care to
name.  And so claiming that the switch has literally *zero* effect
on the things you care about, is taking it too far.

But with just one switch, the force exerted is vastly less than
thermal uncertainties, never mind quantum uncertainties.  If you
don't expect your consciousness to flicker in and out of existence
as the result of thermal jiggling, then you
[certainly](/lw/mn/absolute_authority/) shouldn't expect to go out
like a light when someone sneezes a kilometer away.

The alert Bayesian will note that I have just made an argument
about *expectations,* states of *knowledge,* justified *beliefs*
about what can and can't switch off your consciousness.

This doesn't necessarily destroy the Anti-Zombie Argument. 
[Probabilities are not certainties, but the *laws of* probability are theorems](/lw/o6/perpetual_motion_beliefs/);
if rationality says you can't believe something on your current
information, then that is a law, not a suggestion.

Still, this version of the Anti-Zombie Argument is weaker.  It
doesn't have the nice, clean, absolutely clear-cut status of, "You
can't possibly eliminate consciousness while leaving all the atoms
in *exactly* the same place."  (Or for "all the atoms" substitute
"all causes with in-principle experimentally detectable effects",
and "same wavefunction" for "same place", etc.)

But the new version of the Anti-Zombie Argument still carries.  You
can say, "I don't know what consciousness really is, and I suspect
I may be fundamentally confused about the question.  But if the
word refers to anything at all, it refers to something that is,
among other things, the cause of my talking about consciousness. 
Now, I don't know why I talk about consciousness.  But it happens
inside my skull, and I expect it has something to do with neurons
firing.  Or maybe, if I really understood consciousness, I would
have to talk about an even more fundamental level than that, like
microtubules, or neurotransmitters diffusing across a synaptic
channel.  But still, that switch you just flipped has an effect on
my neurotransmitters and microtubules that's much, much less than
thermal noise at 310 Kelvin.  So whatever the true cause of my
talking about consciousness may be, I don't expect it to be hugely
affected by the gravitational pull from that switch.  Maybe it's
just a tiny little infinitesimal bit affected?  But it's certainly
not going to go out like a light.  I expect to go on talking about
consciousness in *almost exactly* the same way afterward, for
*almost exactly* the same reasons."

This application of the Anti-Zombie Principle is weaker.  But it's
also much more general.  And, in terms of sheer common sense,
correct.

The reductionist and the substance dualist actually have two
different versions of the above statement.  The reductionist
furthermore says, "Whatever makes me talk about consciousness, it
seems likely that the important parts take place on a much higher
functional level than atomic nuclei.  Someone who understood
consciousness could abstract away from individual neurons firing,
and talk about high-level cognitive architectures, and still
describe how my mind produces thoughts like 'I think therefore I
am'.  So nudging things around by the diameter of a nucleon,
shouldn't affect my consciousness (except maybe with very small
probability, or by a very tiny amount, or not until after a
significant delay)."

The substance dualist furthermore says, "Whatever makes me talk
about consciousness, it's got to be something beyond the
computational physics we know, which means that it might very well
involve quantum effects.  But still, my consciousness doesn't
flicker on and off whenever someone sneezes a kilometer away.  If
it did, I would *notice*.  It would be like skipping a few seconds,
or coming out of a general anesthetic, or sometimes saying, "I
don't think therefore I'm not."  So since it's a physical fact that
thermal vibrations don't disturb the stuff of my awareness, I don't
expect flipping the switch to disturb it either."

Either way, you *shouldn't* expect your sense of awareness to
vanish when someone says the word "Abracadabra", even if that does
have some infinitesimal physical effect on your brain -

But hold on!  If you *hear* someone say the word "Abracadabra",
that has a very noticeable effect on your brain - so large, even
your brain can notice it.  It may alter your internal narrative;
you may think, "Why did that person just say 'Abracadabra'?"

Well, but *still* you expect to go on talking about consciousness
in almost exactly the same way afterward, for almost exactly the
same reasons.

And again, it's not that "consciousness" is being *equated*to "that
which makes you talk about consciousness".  It's just that
consciousness, *among other things,* makes you talk about
consciousness.  So anything that makes your consciousness go out
like a light, should make you stop talking about consciousness.

If we do something to you, where you don't see how it could
*possibly* change your internal narrative - the little voice in
your head that sometimes says things like "I think therefore I am",
whose words you can choose to say aloud - then it shouldn't make
you cease to be conscious.

And this is true even if the internal narrative is just "pretty
much the same", and the causes of it are also pretty much the same;
among the causes that are pretty much the same, is whatever you
mean by "consciousness".

If you're wondering where all this is going, and why it's important
to go to such tremendous lengths to ponder such an obvious-seeming
Generalized Anti-Zombie Principle, then consider the following
debate:

Albert:  "Suppose I replaced all the neurons in your head with tiny
robotic artificial neurons that had the same connections, the same
local input-output behavior, and analogous internal state and
learning rules."

Bernice:  "That's killing me!  There wouldn't be a conscious being
there anymore."

Charles:  "Well, there'd still be a conscious being there, but it
wouldn't be *me.*"

Sir Roger Penrose:  "The thought experiment you propose is
impossible.  You *can't* duplicate the behavior of neurons without
tapping into quantum gravity.  That said, there's not much point in
me taking further part in this conversation."  *(Wanders away.)*

Albert:  "Suppose that the replacement is carried out one neuron at
a time, and the swap occurs so fast that it doesn't make any
difference to global processing."

Bernice:  "How could that possibly be the case?"

Albert:  "The little robot swims up to the neuron, surrounds it,
scans it, learns to duplicate it, and then suddenly takes over the
behavior, between one spike and the next.  In fact, the imitation
is *so* good, that your outward behavior is just the same as it
would be if the brain were left undisturbed.  Maybe not
*exactly*the same, but the causal impact is much less than thermal
noise at 310 Kelvin."

Charles:  "So what?"

Albert:  "So don't your beliefs violate the Generalized Anti-Zombie
Principle?  Whatever just happened, it didn't change your internal
narrative!  You'll go around talking about consciousness for
exactly the same reason as before."

Bernice:  "Those little robots are a Zombie Master.  They'll make
me talk about consciousness even though I'm not conscious.  The
Zombie World is possible if you allow there to be an added, extra,
experimentally detectable Zombie Master - which those robots
*are*."

Charles:  "Oh, that's not right, Bernice.  The little robots aren't
plotting how to fake consciousness, or processing a corpus of text
from human amateurs.  They're doing the same thing neurons do, just
in silicon instead of carbon."

Albert:  "Wait, didn't you just agree with me?"

Charles:  "I never said the new person wouldn't be conscious.  I
said it wouldn't be *me.*"

Albert:  "Well, obviously the Anti-Zombie Principle generalizes to
say that this operation hasn't disturbed the true cause of your
talking about this *me* thing."

Charles:  "Uh-uh!  Your operation certainly did disturb the true
cause of my talking about consciousness.  It substituted a
*different* cause in its place, the robots.  Now, just because that
new cause *also* happens to be conscious - talks about
consciousness for the same *generalized* reason - doesn't mean it's
the *same* cause that was originally there."

Albert:  "But I wouldn't even have to *tell* you about the robot
operation.  You wouldn't *notice.*  If you think, going on
introspective evidence, that you are in an important sense "the
same person" that you were five minutes ago, and I do something to
you that doesn't change the introspective evidence available to
you, then your conclusion that you are the same person that you
were five minutes ago should be equally justified.  Doesn't the
Generalized Anti-Zombie Principle say that if I do something to you
that alters your consciousness, let alone makes you a completely
different person, then you ought to *notice* somehow?"

Bernice:  "Not if you replace me with a Zombie Master.  Then
there's no one there *to* notice."

Charles:  "Introspection isn't perfect.  Lots of stuff goes on
inside my brain that I don't notice."

Albert:  "You're postulating epiphenomenal facts about
consciousness and identity!"

Bernice:  "No I'm not!  I can experimentally detect the difference
between neurons and robots."

Charles:  "No I'm not!  I can experimentally detect the moment when
the old me is replaced by a new person."

Albert:  "Yeah, and I can detect the switch flipping!  You're
detecting something that doesn't *make a noticeable difference* to
the*true cause* of your talk about consciousness and personal
identity.  And the proof is, you'll talk just the same way
afterward."

Bernice:  "That's because of your robotic Zombie Master!"

Charles:  "Just because two people talk about 'personal identity'
for similar reasons doesn't make them the same person."

I think the Generalized Anti-Zombie Principle supports Albert's
position, but the reasons shall have to wait for future posts.  I
need other prerequisites, and besides, this post is already too
long.

But you see the importance of the question, "How far can you
generalize the [Anti-Zombie Argument](/lw/p7/zombies_zombies/) and
have it still be valid?"

The makeup of future galactic civilizations may be determined by
the answer...


# What's a Bias Again?

A *bias* is a certain kind of obstacle to our goal of obtaining
truth - its character as an "obstacle" stems from this goal of
truth - but there are many obstacles that are not "biases".

If we start right out by asking "What is bias?", it comes at the
question in the wrong order.  As the proverb goes, "There are forty
kinds of lunacy but only one kind of common sense."  The truth is a
narrow target, a small region of configuration space to hit.  "She
loves me, she loves me not" may be a binary question, but E=MC\^2
is a tiny dot in the space of all equations, like a winning lottery
ticket in the space of all lottery tickets.  Error is not an
exceptional condition; it is success which is *a priori* so
improbable that it requires an explanation.

We don't start out with a moral duty to "reduce bias", because
biases are bad and evil and Just Not Done.  This is the sort of
thinking someone might end up with if they acquired a deontological
duty of "rationality" by social osmosis, which leads to people
trying to execute techniques without appreciating the reason for
them.  (Which is bad and evil and Just Not Done, according to
*Surely You're Joking, Mr. Feynman*, which I read as a kid.)

Rather, we want to get to the truth, for whatever reason, and we
find various obstacles getting in the way of our goal.  These
obstacles are not wholly dissimilar to each other - for example,
there are obstacles that have to do with not having enough
computing power available, or information being expensive.  It so
happens that a large group of obstacles seem to have a certain
character in common - to cluster in a region of obstacle-to-truth
space - and this cluster has been labeled "biases".

What is a bias?  Can we look at the empirical cluster and find a
compact test for membership?  Perhaps we will find that we can't
really give any explanation better than pointing to a few
extensional examples, and hoping the listener understands.  If you
are a scientist just beginning to investigate fire, it might be a
lot wiser to point to a campfire and say "Fire is that
orangey-bright hot stuff over there," rather than saying "I define
fire as an alchemical transmutation of substances which releases
phlogiston."  As I said in
[The Simple Truth](http://sl4.org/wiki/TheSimpleTruth), you should
not ignore something just because you can't define it.  I can't
quote the equations of General Relativity from memory, but
nonetheless if I walk off a cliff, I'll fall.  And we can say the
same of biases - they won't hit any less hard if it turns out we
can't define compactly what a "bias" is.  So we might point to
conjunction fallacies, to overconfidence, to the availability and
representativeness heuristics, to base rate neglect, and say: 
"Stuff like that."

With all that said, we seem to label as "biases" those obstacles to
truth which are produced, not by the cost of information, nor by
limited computing power, but by the shape of our own mental
machinery.  For example, the machinery is evolutionarily optimized
to purposes that actively oppose epistemic accuracy; for example,
the machinery to win arguments in adaptive political contexts.  Or
the selection pressure ran skew to epistemic accuracy; for example,
believing what others believe, to get along socially.  Or, in the
classic heuristic-and-bias, the machinery operates by an
identifiable algorithm that does some useful work but also produces
systematic errors: the availability heuristic is not itself a bias,
but it gives rise to identifiable, compactly describable biases. 
Our brains are doing something wrong, and after a lot of
experimentation and/or heavy thinking, someone identifies the
problem in a fashion that System 2 can comprehend; then we call it
a "bias".  Even if we can do no better for knowing, it is still a
failure that arises, in an identifiable fashion, from a particular
kind of cognitive machinery - not from having too little machinery,
but from the shape of the machinery itself.

"Biases" are distinguished from errors that arise from cognitive
content, such as adopted beliefs, or adopted moral duties.  These
we call "mistakes", rather than "biases", and they are much easier
to correct, once we've noticed them for ourselves.  (Though the
source of the mistake, or the source of the source of the mistake,
may ultimately be some bias.)

"Biases" are distinguished from errors that arise from damage to an
individual human brain, or from absorbed cultural mores; biases
arise from machinery that is humanly universal.

Plato wasn't "biased" because he was ignorant of General Relativity
- he had no way to gather that information, his ignorance did not
arise from the shape of his mental machinery.  But if Plato
believed that philosophers would make better kings because he
himself was a philosopher - and this belief, in turn, arose because
of a universal adaptive political instinct for self-promotion, and
not because Plato's daddy told him that everyone has a moral duty
to promote their own profession to governorship, or because Plato
sniffed too much glue as a kid - then that was a bias, whether
Plato was ever warned of it or not.

Biases may not be cheap to correct.  They may not even be
correctable.  But where we look upon our own mental machinery and
see a causal account of an identifiable class of errors; and when
the problem seems to come from the evolved shape of the machinery,
rather from there being too little machinery, or bad specific
content; then we call that a bias.

Personally, I see our quest in terms of acquiring personal skills
of rationality, in improving truthfinding technique.  The challenge
is to attain the positive goal of truth, not to avoid the negative
goal of failure.  Failurespace is wide, infinite errors in infinite
variety.  It is difficult to describe so huge a space:  "What is
true of one apple may not be true of another apple; thus more can
be said about a single apple than about all the apples in the
world."  Success-space is narrower, and therefore more can be said
about it.

While I am not averse (as you can see) to discussing definitions,
we should remember that is not our primary goal.  We are here to
pursue the great human quest for truth: for we have desperate need
of the knowledge, and besides, we're curious.  To this end let us
strive to overcome whatever obstacles lie in our way, whether we
call them "biases" or not.


# Why Truth? And...

Some of the comments in this blog have touched on the question of
why we ought to seek truth.  (Thankfully not many have questioned
[what truth is](http://sl4.org/wiki/TheSimpleTruth).)  Our shaping
motivation for configuring our thoughts to rationality, which
determines whether a given configuration is "good" or "bad", comes
from whyever we wanted to find truth in the first place.

It is written:  "The first virtue is curiosity."  Curiosity is one
reason to seek truth, and it may not be the only one, but it has a
special and admirable purity.  If your motive is curiosity, you
will assign priority to questions according to how the questions,
themselves, tickle your personal aesthetic sense.  A trickier
challenge, with a greater probability of failure, may be worth more
effort than a simpler one, just because it is more fun.



Some people, I suspect, may object that curiosity is an emotion and
is therefore "not rational". I label an emotion as "not rational"
if it rests on mistaken beliefs, or rather, on irrational epistemic
conduct: "If the iron approaches your face, and you believe it is
hot, and it is cool, the Way opposes your fear. If the iron
approaches your face, and you believe it is cool, and it is hot,
the Way opposes your calm." Conversely, then, an emotion which is
evoked by correct beliefs or epistemically rational thinking is a
"rational emotion"; and this has the advantage of letting us regard
calm as an emotional state, rather than a privileged default. When
people think of "emotion" and "rationality" as opposed, I suspect
that they are really thinking of System 1 and System 2 - fast
perceptual judgments versus slow deliberative judgments.
Deliberative judgments aren't always true, and perceptual judgments
aren't always false; so it is very important to distinguish that
dichotomy from "rationality". Both systems can serve the goal of
truth, or defeat it, according to how they are used.

Besides sheer emotional curiosity, what other motives are there for
desiring truth? Well, you might want to accomplish some specific
real-world goal, like building an airplane, and therefore you need
to know some specific truth about aerodynamics. Or more mundanely,
you want chocolate milk, and therefore you want to know whether the
local grocery has chocolate milk, so you can choose whether to walk
there or somewhere else. If this is the reason you want truth, then
the priority you assign to your questions will reflect the expected
utility of their information - how much the possible answers
influence your choices, how much your choices matter, and how much
you expect to find an answer that changes your choice from its
default.

To seek truth merely for its instrumental value may seem impure -
should we not desire the truth for its own sake? - but such
investigations are extremely important because they create an
outside criterion of verification: if your airplane drops out of
the sky, or if you get to the store and find no chocolate milk,
it's a hint that you did something wrong.  You get back feedback on
which modes of thinking work, and which don't.  Pure curiosity is a
wonderful thing, but it may not linger too long on verifying its
answers, once the attractive mystery is gone.  Curiosity, as a
human emotion, has been around since long before the ancient
Greeks.  But what set humanity firmly on the path of Science was
noticing that certain modes of thinking uncovered beliefs that let
us *manipulate the world.*  As far as sheer curiosity goes,
spinning campfire tales of gods and heroes satisfied that desire
just as well, and no one realized that anything was wrong with
that.

Are there motives for seeking truth besides curiosity and
pragmatism?  The third reason that I can think of is morality:  You
believe that to seek the truth is noble and important and
worthwhile.  Though such an ideal also attaches an intrinsic value
to truth, it's a very different state of mind from curiosity. 
Being curious about what's behind the curtain doesn't feel the same
as believing that you have a moral duty to look there.  In the
latter state of mind, you are a lot more likely to believe that
someone *else* should look behind the curtain, too, or castigate
them if they deliberately close their eyes.  For this reason, I
would also label as "morality" the belief that truthseeking is
pragmatically important *to society*, and therefore is incumbent as
a duty upon all.  Your priorities, under this motivation, will be
determined by your ideals about which truths are most important
(not most useful or most intriguing); or your moral ideals about
when, under what circumstances, the duty to seek truth is at its
strongest.

I tend to be suspicious of morality as a motivation for
rationality, *not* because I reject the moral ideal, but because it
invites certain kinds of trouble.  It is too easy to acquire, as
learned moral duties, modes of thinking that are dreadful missteps
in the dance.  Consider Mr. Spock of *Star Trek*, a naive archetype
of rationality.  Spock's emotional state is always set to "calm",
even when wildly inappropriate.  He often gives many significant
digits for probabilities that are grossly uncalibrated.  (E.g: 
"Captain, if you steer the Enterprise directly into that black
hole, our probability of surviving is only 2.234%"  Yet nine times
out of ten the Enterprise is not destroyed.  What kind of tragic
fool gives four significant digits for a figure that is off by two
orders of magnitude?)  Yet this popular image is how many people
conceive of the duty to be "rational" - small wonder that they do
not embrace it wholeheartedly.  To make rationality into a moral
duty is to give it all the dreadful degrees of freedom of an
arbitrary tribal custom.  People arrive at the wrong answer, and
then indignantly protest that they acted with propriety, rather
than learning from their mistake.

And yet if we're going to *improve* our skills of rationality, go
beyond the standards of performance set by hunter-gatherers, we'll
need deliberate beliefs about how to think with propriety.  When we
write new mental programs for ourselves, they start out in System
2, the deliberate system, and are only slowly - if ever - trained
into the neural circuitry that underlies System 1.  So if there are
certain kinds of thinking that we find we want to *avoid* - like,
say, biases - it will end up represented, within System 2, as an
injunction not to think that way; a professed duty of avoidance.

If we want the truth, we can most effectively obtain it by thinking
in certain ways, rather than others; and these are the techniques
of rationality.  Some of the techniques of rationality involve
overcoming a certain class of obstacles, the biases...

(Continued in next post:  "What's a bias, again?")

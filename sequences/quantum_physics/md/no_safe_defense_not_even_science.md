
# No Safe Defense, Not Even Science

I don't ask my friends about their childhoods - I lack social
curiosity - and so I don't know how much of a trend this really
is:

Of the people I know who are reaching upward as rationalists, who
volunteer information about their childhoods, there is a surprising
tendency to hear things like:  "My family joined a cult and I had
to break out," or "One of my parents was clinically insane and I
had to learn to filter out reality from their madness."

My own experience with growing up in an Orthodox Jewish family
seems tame by comparison... but it accomplished the same outcome: 
It broke my core emotional trust in the sanity of the people around
me.

Until this core emotional trust is broken, you don't start growing
as a rationalist.  I have trouble putting into words why this is
so.  Maybe any *unusual* skills you acquire - anything that makes
you *unusually*rational - requires you to zig when other people
zag.  Maybe that's just too scary, if the world still seems like a
sane place unto you.

Or maybe you don't bother putting in the hard work to be extra
bonus sane, if normality doesn't scare the hell out of you.

I know that many aspiring rationalists seem to run into roadblocks
around things like cryonics or many-worlds.  Not that they don't
see the logic; they see the logic and wonder, "Can this really be
true, when it seems so obvious now, and yet none of the people
around me believe it?"

Yes.  Welcome to the Earth where ethanol is made from corn and
environmentalists oppose nuclear power.  I'm sorry.

> (See also: 
> [Cultish Countercultishness](/lw/md/cultish_countercultishness/). 
> If you end up in the frame of mind of
> *nervously seeking reassurance*, this is never a good thing - even
> if it's because you're about to believe something that sounds
> logical but could cause other people to look at you funny.)

People who've had their trust broken in the sanity of the people
around them, seem to be able to evaluate strange ideas on their
merits, without feeling *nervous*about their strangeness.  The glue
that binds them to their current place has dissolved, and they can
walk in some direction, hopefully forward.

[Lonely dissent](/lw/mb/lonely_dissent/), I called it.  True
dissent doesn't feel like going to school wearing black; it feels
like going to school wearing a clown suit.

That's what it takes to be the lone voice who says, "If you
*really* think *you* know who's going to win the election, why
aren't you picking up the
[free money](/lw/ni/buy_now_or_forever_hold_your_peace/) on the
Intrade prediction market?" while all the people around you are
thinking, "It is good to be an individual and form your own
opinions, the shoe commercials told me so."

Maybe in some other world, some alternate Everett branch with a
saner human population, things would be different... but in *this*
world, I've never seen anyone begin to grow as a rationalist until
they make a deep emotional break with the
[wisdom of their pack](/lw/m9/aschs_conformity_experiment/).

Maybe in another world, things would be different.  And maybe not. 
I'm not sure that human beings realistically *can* trust and think
at the same time.

Once upon a time, there was something I trusted.

Eliezer~18~ trusted Science.

Eliezer~18~ dutifully acknowledged that the social process of
science was flawed.  Eliezer~18~ dutifully acknowledged that
academia was slow, and misallocated resources, and played
favorites, and mistreated its precious heretics.

That's the convenient thing about acknowledging flaws in *people*
who failed to live up to your ideal; you don't have to question the
*ideal itself*.

But who could possibly be foolish enough to question, "The
experimental method shall decide which hypothesis wins"?

Part of what fooled Eliezer~18~ was a (major!) general problem he
had, with an aversion to ideas that *resembled* things idiots had
said.  (See: 
[Reversed stupidity is not intelligence](/lw/lw/reversed_stupidity_is_not_intelligence/).)

Eliezer~18~ had seen *plenty* of people questioning the ideals of
Science Itself, and they were *all* on the Dark Side.  It's not
like these people were saying, "Okay, here's how I think an
excessive focus on definitive experiments
[misled physicists](/lw/q8/many_worlds_one_best_guess/) on the
decoherence interpretation of quantum mechanics, and here's how you
can [do better](/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/)
using probability theory..."  People who questioned the *ideal*of
Science were invariably trying to sell you snake oil, or trying to
safeguard their favorite form of stupidity from criticism, or
trying to disguise their personal resignation as a Deeply Wise
acceptance of futility.

And Eliezer~18~ flinched away from being like these people; so he
never questioned the ideal of the experimental method.

This is one reason that in these blog posts I am confronting,
head-on, Science Itself.  (Sure, someone may come back later and
use these posts to paint me as a lunatic, but frankly, anyone who
wishes to paint me as a lunatic already has *more than enough*
material to misquote.)  The natural assumption that anyone who
seriously challenges Science is on the Dark Side, meant that this
was one of *very few things* that Eliezer~18~ didn't think about
much.  Of course those "few things" turned out to contain the evil
black box surprises from hell.

If there'd been any other ideal that was a few centuries old, the
young Eliezer would have looked at it and said, "I wonder if this
is really right, and whether there's a way to
[do better](/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/)." 
But not the ideal of Science.  Science was the *master* idea, the
idea that let you change ideas.  You could *question* it, but you
were meant to
[question it and then accept it](/lw/ib/the_proper_use_of_doubt/),
not actually say, "Wait!  This is wrong!"

Thus, when once upon a time I came up with a stupid idea, I thought
I was behaving virtuously if I made sure there was a novel
prediction, and professed that I wished to test my idea
experimentally.  I thought I had done everything I was obliged to
do.

So I thought I was *safe*- not safe from any particular external
threat, but safe on some deeper level, like a child who trusts
their parent and has obeyed all the parent's rules.

I'd long since been broken of trust in the sanity of my family or
my teachers at school.  And the other children weren't intelligent
enough to compete with the conversations I could have with books. 
But I trusted the books, you see.  I trusted that if I did what
Richard Feynman told me to do, I would be safe.  I never thought
those words aloud, but it was how I felt.

When Eliezer~23~ realized exactly *how* stupid the stupid theory
had been - and that Traditional Rationality had not saved him from
it - and that Science would have been perfectly okay with his
wasting ten years testing the stupid idea, so long as afterward he
admitted it was wrong...

...well, I'm not going to say it was a huge emotional convulsion. 
I don't really go in for that kind of drama.  It simply became
obvious that I'd been stupid.

That's the trust I'm trying to break in you.  You are not safe. 
Ever.

Not even Science can save you.  The ideals of Science were born
centuries ago, in a time when no one knew anything about
probability theory or cognitive biases.  Science demands
*too little* of you, it blesses your good intentions too easily,
[it is not strict *enough*](/lw/qd/science_isnt_strict_enough/), it
only makes those injunctions that an
[average scientist](/lw/qe/do_scientists_already_know_this_stuff/)
can follow, it accepts
[slowness](/lw/q9/the_failures_of_eld_science/) as a fact of life.

So don't think that if you only follow the rules of Science, that
makes your reasoning defensible.

There is no known procedure you can follow that makes your
reasoning defensible.

There is no known set of injunctions which you can satisfy, and
know that you will not have been a fool.

There is no known morality-of-reasoning that you can do your best
to obey, and know that you are thereby shielded from criticism.

No, not even if you turn to Bayescraft.  It's much harder to use
and you'll never be sure that you're doing it right.

The discipline of Bayescraft is younger by far than the discipline
of Science.  You will find no textbooks, no elderly mentors, no
histories written of success and failure, no hard-and-fast rules
laid down.  You will have to study cognitive biases, and
probability theory, and evolutionary psychology, and social
psychology, and other cognitive sciences, and Artificial
Intelligence - and think through for *yourself* how to apply all
this knowledge to the case of correcting yourself, since it isn't
yet in the textbooks.

You don't know what your own mind is really doing. They find a new
cognitive bias every week and you're never sure if you've corrected
for it, or overcorrected.

The formal math is impossible to apply.  It doesn't break down as
easily as John Q. Unbeliever thinks, but you're never really sure
where the foundations come from.  You don't know why the universe
is simple enough to understand, or why *any* prior works for it. 
You don't know what your own priors *are,* let alone if they're any
good.

One of the problems with Science is that it's too vague to really
*scare*you.  "Ideas should be tested by experiment."  How can you
go wrong with that?

On the other hand, if you have some math of probability theory laid
out in front of you, and worse,
*you know you can't actually use it,* then it becomes clear that
you are trying to do something *difficult,* and that you might well
be doing it *wrong.*

So you cannot trust.

And all this that I have said, *will not be sufficient* to break
your trust.  That won't happen until you get into your first real
disaster from *following*The Rules, not from breaking them.

Eliezer~18~ already had the notion that you were *allowed*to
question Science.  Why, of course the scientific method was not
itself immune to questioning!  For are we not all good
rationalists?  Are we not allowed to question everything?

It was the notion that you could *actually in real life* follow
Science and fail miserably, that Eliezer~18~  didn't
*really, emotionally* believe was possible.

Oh, of course he *said* it was possible.  Eliezer~18~  dutifully
acknowledged the possibility of error, saying, "I could be wrong,
but..."

But he didn't think failure could happen in, you know,
*real life.*  You were supposed to *look* for flaws, not
[actually find them](/lw/ib/the_proper_use_of_doubt/).

And this emotional difference is a terribly difficult thing to
accomplish in words, and I fear there's no way I can really warn
you.

Your trust will not break, until you apply all that you have
learned here and from other books, and take it as far as you can
go, and find that *this too* fails you - that you have still been a
fool, and no one warned you against it - that all the most
important parts were left out of the guidance you received - that
some of the most precious ideals you followed, steered you in the
wrong direction -

- and if you still have
[something to protect](/lw/nb/something_to_protect/), so that you
*must*keep going, and *cannot* resign and wisely acknowledge the
limitations of rationality -

*- then* you will be ready to start your journey as a rationalist. 
To take sole responsibility, to live without any trustworthy
defenses, and to forge a higher Art than the one you were once
taught.

No one begins to truly search for the Way until their parents have
failed them, their gods are dead, and their tools have shattered in
their hand.


* * * * *

**Post Scriptum:**  On reviewing a draft of this essay, I
discovered a fairly inexcusable flaw in reasoning, which actually
affects one of the conclusions drawn.  I am
[leaving it in](http://www.overcomingbias.com/2008/02/my-favorite-lia.html). 
Just in case you thought that taking my advice made you safe; or
that you were supposed to *look*for flaws, but not *find*any.

And of course, if you look *too hard* for a flaw, and find a flaw
that is not a real flaw, and cling to it to reassure yourself of
how critical you are, you will only be worse off than before...

It is living with uncertainty - knowing on a gut level that
*there are flaws*, *they are serious* and *you have not found them*
- that is the difficult thing.


# Justified Expectation of Pleasant Surprises

I recently tried playing a computer game that made a major
fun-theoretic error.  (At least I strongly suspect it's an error,
though they are game designers and I am not.)

The game showed me - right from the start of play - what abilities
I could purchase as I increased in level.  Worse, there were
[many different choices](/lw/x2/harmful_options/); still worse, you
had to pay a cost in fungible points to acquire them, making you
feel like you were losing a resource...  But today, I'd just like
to focus on the problem of telling me,
*right at the start of the game,* about all the nice things that
might happen to me later.

I can't think of a good experimental result that backs this up; but
I'd expect that a pleasant *surprise* would have a greater hedonic
impact, than being told about the same gift in advance.  Sure, the
moment you were first *told*about the gift would be *good news*, a
moment of pleasure in the moment of being told.  But you wouldn't
have the gift in hand at that moment, which limits the pleasure. 
And then you have to wait.  And then when you finally get the gift
- it's pleasant to go from not having it to having it, *if* you
didn't wait too long; but a surprise would have a larger momentary
impact, I would think.

This particular game had a status screen that showed *all* my
future class abilities *at the start of the game* - inactive and
dark but with full information still displayed.  From a hedonic
standpoint this seems like *miserable*fun theory.  All the "good
news" is lumped into a gigantic package; the items of news would
have much greater impact if encountered separately.  And then I
have to wait a long time to actually acquire the abilities, so I
get an extended period of comparing my current weak game-self to
all the wonderful abilities I *could* have but don't.

Imagine living in two possible worlds.  Both worlds are otherwise
rich in [challenge](/lw/ww/high_challenge/),
[novelty](/lw/wx/complex_novelty/), and other aspects of Fun.  In
both worlds, you get smarter with age and
[acquire more abilities over time](/lw/wz/living_by_your_own_strength/),
so that your life is
[always getting better](/lw/xk/continuous_improvement/).

But in *one*world, the abilities that come with seniority are
openly discussed, hence widely known; you know what you have to
look forward to.

In the *other*world, anyone older than you will *refuse to talk*
about certain aspects of growing up; you'll just have to wait and
find out.



I ask you to contemplate - not just which world you might prefer to
live in - but how *much* you might want to live in the second
world, rather than the first.  I would even say that the second
world seems more *alive;* when I imagine living there, my imagined
*will to live* feels stronger.  I've got to stay alive to find out
what happens next, right?

The idea that *hope* is important to a happy life, is hardly
original with me - though I think it might not be emphasized quite
*enough*, on the lists of things people are told they need.

I
[don't agree with buying lottery tickets](/lw/hl/lotteries_a_waste_of_hope/),
but I do think I understand why people do it.  I remember the times
in my life when I had more or less belief that things would improve
- that they were heading up in the near-term or mid-term, close
enough to anticipate.  I'm having trouble describing how much of a
difference it makes.  Maybe I don't *need* to describe that
difference, unless some of my readers have never had any light at
the end of their tunnels, or some of my readers have never looked
forward and seen darkness.

If [existential angst](/lw/sc/existential_angst_factory/) comes
from having at least one deep problem in your life that you aren't
thinking about explicitly, so that the pain which comes from it
seems like a natural permanent feature - then the very first
question I'd ask, to identify a possible source of that problem,
would be, "Do you expect your life to improve in the near or
mid-term future?"

Sometimes I meet people who've been run over by life, in much the
same way as being run over by a truck.  Grand catastrophe isn't
necessary to destroy a will to live.  The extended absence of hope
leaves the same sort of wreckage.

People need hope.  I'm not the first to say it.

But I think that the importance of *vague hope* is
underemphasized.

"Vague" is usually not a compliment among rationalists.  Hear
"vague hopes" and you immediately think of, say, an alternative
medicine herbal profusion whose touted benefits are so conveniently
unobservable (not to mention experimentally unverified) that people
will buy it for anything and then refuse to admit it didn't work. 
You think of poorly worked-out plans with missing steps, or
supernatural prophecies made carefully unfalsifiable, or fantasies
of unearned riches, or...

But you know, generally speaking, our beliefs about the future
*should* be vaguer than our beliefs about the past.  We just know
less about tomorrow than we do about yesterday.

There are plenty of *bad* reasons to be vague, all sorts of
*suspicious*reasons to offer nonspecific predictions, but
[reversed stupidity is not intelligence](/lw/lw/reversed_stupidity_is_not_intelligence/): 
When you've eliminated all the ulterior motives for vagueness, your
beliefs about the future should *still*be vague.

We don't know much about the future; let's hope *that*doesn't
change for
[as long as human emotions stay what they are](/lw/xg/emotional_involvement/). 
Of all the poisoned gifts a big mind could give a small one,
[a walkthrough for the game](/lw/x3/devils_offers/) has to be near
the top of the list.

What we need to maintain our interest in life, is a
*justified expectation of pleasant surprises.*  (And yes, you can
[expect a surprise](/lw/v7/expected_creative_surprises/) if you're
not logically omniscient.)  This excludes the herbal profusions,
the poorly worked-out plans, and the
[supernatural](/lw/tv/excluding_the_supernatural/).  The best
reason for this justified expectation is *experience*, that is,
being pleasantly surprised on a frequent yet irregular basis.  (If
this isn't happening to you, please file a bug report with the
appropriate authorities.)

*Vague justifications* for believing in a pleasant
*specific outcome* would be the opposite.

There's also other dangers of having pleasant hopes that are
*too specific* - even if *justified*, though more often they aren't
- and I plan to talk about that in the next post.

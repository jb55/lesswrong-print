
# Hindsight Devalues Science

This
[excerpt](http://csml.som.ohio-state.edu/Music829C/hindsight.bias.html)
from Meyers's *Exploring Social Psychology* is worth reading in
entirety.  Cullen Murphy, editor of *The Atlantic,* said that the
social sciences turn up "no ideas or conclusions that can't be
found in [any] encyclopedia of quotations... Day after day social
scientists go out into the world.  Day after day they discover that
people's behavior is pretty much what you'd expect."

Of course, the "expectation" is all
[hindsight](/lw/il/hindsight_bias/).  (Hindsight bias:  Subjects
who know the actual answer to a question assign much higher
probabilities they "would have" guessed for that answer, compared
to subjects who must guess without knowing the answer.)

The historian Arthur Schlesinger, Jr. dismissed scientific studies
of WWII soldiers' experiences as "ponderous demonstrations" of
common sense.  For example:

1.  Better educated soldiers suffered more adjustment problems than
    less educated soldiers. (Intellectuals were less prepared for
    battle stresses than street-smart people.)  
2.  Southern soldiers coped better with the hot South Sea Island
    climate than Northern soldiers. (Southerners are more accustomed to
    hot weather.)  
3.  White privates were more eager to be promoted to
    noncommissioned officers than Black privates. (Years of oppression
    take a toll on achievement motivation.)  
4.  Southern Blacks preferred Southern to Northern White officers
    (because Southern officers were more experienced and skilled in
    interacting with Blacks).  
5.  As long as the fighting continued, soldiers were more eager to
    return home than after the war ended. (During the fighting,
    soldiers knew they were in mortal danger.)  

How many of these findings do you think you *could have* predicted
in advance?   3 out of 5?  4 out of 5?  Are there any cases where
you would have predicted the opposite - where your model
[takes a hit](/lw/ii/conservation_of_expected_evidence/)?  Take a
moment to think before continuing...

In this demonstration (from Paul Lazarsfeld by way of Meyers), all
of the findings above are the *opposite* of what was actually
found.  How many times did you think your model took a hit?  How
many times did you admit you would have been wrong?  That's how
good your model really was.  The measure of
[your strength as a rationalist](/lw/if/your_strength_as_a_rationalist/)
is your ability to be more confused by fiction than by reality.

Unless, of course, I reversed the results again.  What do you
think?

Do your thought processes at this point, where you *really don't*
know the answer, feel different from the thought processes you used
to rationalize either side of the "known" answer?

Daphna Baratz exposed college students to pairs of supposed
findings, one true ("In prosperous times people spend a larger
portion of their income than during a recession") and one the
truth's opposite.  In both sides of the pair, students rated the
supposed finding as what they "would have predicted".  Perfectly
standard hindsight bias.

Which leads people to think they have no need for science, because
they "could have predicted" that.

(Just as you would expect, right?)

Hindsight will lead us to systematically undervalue the
surprisingness of scientific findings, especially the discoveries
we *understand* - the ones that seem real to us, the ones we can
retrofit into our models of the world.  If you understand neurology
or physics and read news in that topic, then you probably
underestimate the surprisingness of findings in those fields too. 
This unfairly devalues the contribution of the researchers; and
worse, will prevent you from noticing when you are seeing evidence
that[doesn't fit](/lw/ii/conservation_of_expected_evidence/) what
you *really* would have expected.

We need to make a conscious effort to be shocked *enough.*
